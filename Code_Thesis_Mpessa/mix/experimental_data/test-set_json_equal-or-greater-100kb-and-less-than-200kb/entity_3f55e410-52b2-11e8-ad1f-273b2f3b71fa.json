{
  "datasourceIdentifier" : "awesome wiki export",
  "backlink" : "http://en.wikipedia.org/?curid=19996",
  "eid" : "3f55e410-52b2-11e8-ad1f-273b2f3b71fa",
  "loadTime" : 1525778615505,
  "textBody" : "MIDI (; short for Musical Instrument Digital Interface) is a technical standard that describes a communications protocol, digital interface, and electrical connectors and allows a wide variety of electronic musical instruments, computers and other related music and audio devices to connect and communicate with one another. A single MIDI link can carry up to sixteen channels of information, each of which can be routed to a separate device.\n\nMIDI carries event messages that specify notation, pitch and velocity (loudness or softness), control signals for parameters such as volume, vibrato, audio panning from left to right, cues in theatre, and clock signals that set and synchronize tempo between multiple devices. These messages are sent via a MIDI cable to other devices where they control sound generation and other features. A simple example of a MIDI setup is the use of a MIDI controller such as an electronic musical keyboard to trigger sounds created by a sound module, which is in turn plugged into a keyboard amplifier. This MIDI data can also be recorded into a hardware or software device called a sequencer, which can be used to edit the data and to play it back at a later time. \n\nA file format to store and exchange the data has also been defined. Advantages of MIDI include small file size, ease of modification and manipulation and a wide choice of electronic instruments and synthesizer or digitally-sampled sounds. Prior to the development of MIDI, electronic musical instruments from different manufacturers could generally not communicate with each other. With MIDI, any MIDI-compatible keyboard (or other controller device) can be connected to any other MIDI-compatible sequencer, sound module, drum machine, synthesizer, or computer, even if they are made by different manufacturers.\n\nMIDI technology was standardized in 1983 by a panel of music industry representatives, and is maintained by the MIDI Manufacturers Association (MMA). All official MIDI standards are jointly developed and published by the MMA in Los Angeles, and the MIDI Committee of the Association of Musical Electronics Industry (AMEI) in Tokyo. In 2016, the MMA established The MIDI Association (TMA) to support a global community of people who work, play, or create with MIDI.\n\nHistory\n\nDevelopment\n\nIn 1980, Roland introduced the DIN sync interface to synchronize different electronic musical instruments. It was introduced with the Roland TR-808 in 1980, followed by other Roland equipment in 1981. In 1981 Roland introduced Digital Control Bus (DCB). DCB was the precursor to MIDI, which adopted most of its features from the DCB protocol and used the same type of connectors as DIN sync.\n\nIn June 1981, Roland founder Ikutaro Kakehashi proposed the idea of standardization to Oberheim Electronics founder Tom Oberheim, who then discussed it with Sequential Circuits president Dave Smith. In October 1981, Kakehashi, Oberheim and Smith discussed the idea with representatives from Yamaha, Korg and Kawai. They then discussed how to develop a common standard, working with Roland's pre-existing DCB as a basis. Sequential Circuits engineers and synthesizer designers, Dave Smith and Chet Wood, devised a universal synthesizer interface, which would allow direct communication between equipment from different manufacturers. Smith proposed this standard at the Audio Engineering Society show in November 1981. The standard was then discussed and modified by representatives of Roland, Yamaha, Korg, Kawai, and Sequential Circuits,Holmes, Thom. Electronic and Experimental Music: Pioneers in Technology and Composition. New York: Routledge, 2003 and was named Musical Instrument Digital Interface. MIDI's development was announced to the public by Robert Moog, in the October 1982 edition of Keyboard magazine.\n\nBy the time of the January 1983 Winter NAMM Show, Smith was able to demonstrate a MIDI connection between his Prophet 600 analog synthesizer with a Roland JP-6 synthesizer. The MIDI Specification was published in August 1983. The MIDI standard was unveiled by Ikutaro Kakehashi and Dave Smith, who both later received Technical Grammy Awards in 2013 for their key roles in the development of MIDI.\n\nThe first MIDI synthesizers were the Roland Jupiter-6 and the Prophet 600, both released in 1982. The first MIDI drum machine was the Roland TR-909, released in 1983.Butler, Mark Jonathan. \"Unlocking the Groove: Rhythm, Meter, and Musical Design in Electronic Dance Music\". Indiana University Press, 2006. . p. 64 The first MIDI music sequencer was the Roland MSQ-700, released in 1983. The first computers to support MIDI were the NEC PC-88 and PC-98 in 1982, and the MSX (Yamaha CX5M)Helen Casabona, David Frederick, [https://books.google.co.uk/books?id\n6K5Tpl_zBoEC&pgPA15 Advanced MIDI Applications, page 15] , Alfred Music released in 1983.Martin Russ, [https://books.google.co.uk/books?id\nX9h5AgAAQBAJ&pg=PA85 Sound Synthesis and Sampling, page 85] , CRC Press\n\nImpact on the music industry\n\nMIDI's appeal was originally limited to professional musicians and record producers who wanted to use electronic instruments in the production of popular music. The standard allowed different instruments to \"speak\" with each other and with computers, and this spurred a rapid expansion of the sales and production of electronic instruments and music software. This interoperability allowed one device to be controlled from another, which reduced the amount of hardware musicians needed to own. MIDI's introduction coincided with the dawn of the personal computer era and the introductions of samplers and digital synthesizers.Macan, Edward. Rocking the Classics: English Progressive Rock and the Counterculture. New York: Oxford University Press, 1997. p.191 The creative possibilities brought about by MIDI technology have been credited as having helped to revive the music industry in the 1980s.Shuker, Roy. Understanding Popular Music. London: Routledge, 1994. p.286\n\nMIDI introduced many capabilities which transformed the way musicians work. MIDI sequencing made it possible for a user with no notation skills to build complex arrangements.Demorest, Steven M. Building Choral Excellence: Teaching Sight-Singing in the Choral Rehearsal. New York: Oxford University Press, 2003. p. 17 A musical act with as few as one or two members, each operating multiple MIDI-enabled devices, could deliver a performance which sounds similar to that of a much larger group of musicians.Pertout, Andrian. [http://www.pertout.com/Midi.htm Mixdown Monthly] , #26. 26 June 1996. Web. 22 August 2012 The expense of hiring outside musicians for a project could be reduced or eliminated, and complex productions could be realized on a system as small as a synthesizer with integrated keyboard and sequencer. MIDI helped establish home recording. By performing preproduction in a home environment, an artist can reduce recording costs by arriving at a recording studio with a song that is already partially completed and worked out. Educational technology enabled by MIDI has transformed music education.Crawford, Renee. An Australian Perspective: Technology in Secondary School Music. Journal of Historical Research in Music Education. Vol. 30, No. 2. Apr 2009. Print.\n\nComparison with digital audio \n\nThose new to the subject of MIDI might confuse it with digital audio. While it may appear that MIDI and digital audio equipment do the same task, recording of multiple channels of music using digital equipment, this is done differently by MIDI and digital audio systems. MIDI symbolically represents a note. When the synth player presses a key on a keyboard, MIDI records which key was pressed, with which velocity and which duration, whereas digital audio represents the sound produced by the instrument.\n\nApplications\n\nInstrument control\n\nMIDI was invented so that electronic or digital musical instruments could communicate with each other and so that one instrument can control another. For example, a MIDI-compatible sequencer can trigger beats produced by a drum sound module. Analog synthesizers that have no digital component and were built prior to MIDI's development can be retrofit with kits that convert MIDI messages into analog control voltages. When a note is played on a MIDI instrument, it generates a digital signal that can be used to trigger a note on another instrument. The capability for remote control allows full-sized instruments to be replaced with smaller sound modules, and allows musicians to combine instruments to achieve a fuller sound, or to create combinations of synthesized instrument sounds, such as acoustic piano and strings.Lau, Paul. \"[http://www.highbeam.com/doc/1P3-1610624011.html Why Still MIDI?].\"  Canadian Musician. Norris-Whitney Communications Inc. 2008. HighBeam Research. 4 September 2012 MIDI also enables other instrument parameters (volume, effects, etc.) to be controlled remotely.\n\nSynthesizers and samplers contain various tools for shaping an electronic or digital sound. Filters adjust timbre (bass and treble), and envelopes automate the way a sound evolves over time after a note is triggered.Sasso, Len. \"[http://www.emusician.com/news/0766/sound-programming-101/145154 Sound Programming 101] \". Electronic Musician. NewBay Media. 1 October 2002. Web. 4 September 2012. The frequency of a filter and the envelope attack, or the time it takes for a sound to reach its maximum level, are examples of synthesizer parameters, and can be controlled remotely through MIDI. Effects devices have different parameters, such as delay feedback or reverb time. When a MIDI continuous controller number (CCN) is assigned to one of these parameters, the device will respond to any messages it receives that are identified by that number. Controls such as knobs, switches, and pedals can be used to send these messages. A set of adjusted parameters can be saved to a device's internal memory as a \"patch\", and these patches can be remotely selected by MIDI program changes. The MIDI standard allows selection of 128 different programs, but devices can provide more by arranging their patches into banks of 128 programs each, and combining a program change message with a bank select message.Anderton, Craig. \"[http://www.soundonsound.com/sos/1995_articles/may95/midiforguitarists.html MIDI For Guitarists: A Crash Course In MIDI Effects Control] \". Sound On Sound. SOS Publications. May 1995.\n\nComposition\n\nMIDI events can be sequenced with computer software, or in specialized hardware music workstations. Many digital audio workstations (DAWs) are specifically designed to work with MIDI as an integral component. MIDI piano rolls have been developed in many DAWs so that the recorded MIDI messages can be extensively modified. These tools allow composers to audition and edit their work much more quickly and efficiently than did older solutions, such as multitrack recording.\n\nBecause MIDI is a set of commands that create sound, MIDI sequences can be manipulated in ways that prerecorded audio cannot. It is possible to change the key, instrumentation or tempo of a MIDI arrangement, and to reorder its individual sections.Campbell, Drew. \"\"Click, Click. Audio\" Stage Directions. Vol. 16, No. 3. Mar 2003. The ability to compose ideas and quickly hear them played back enables composers to experiment.McCutchan, Ann. The Muse That Sings: Composers Speak about the Creative Process. New York: Oxford University Press, 1999. p. 67-68,72 Algorithmic composition programs provide computer-generated performances that can be used as song ideas or accompaniment.\n\nSome composers may take advantage of MIDI 1.0 and General MIDI (GM) technology to allow musical data files to be shared among various electronic instruments by using a standard, portable set of commands and parameters. The data composed via the sequenced MIDI recordings can be saved as a Standard MIDI File (SMF), digitally distributed, and reproduced by any computer or electronic instrument that also adheres to the same MIDI, GM, and SMF standards. MIDI data files are much smaller than recorded audio files.\n\nUse with computers\n\nAt the time of MIDI's introduction, the computing industry was mainly devoted to mainframe computers, and personal computers were not commonly owned. The personal computer market stabilized at the same time that MIDI appeared, and computers became a viable option for music production. It was not until the advent of MIDI in 1983 that general-purpose computers started to play a role in mainstream music production.\n\nIn the years immediately after the 1983 ratification of the MIDI specification, MIDI features were adapted to several early computer platforms. NEC's PC-88 and PC-98 began supporting MIDI as early as 1982. Yamaha modules introduced MIDI support and sequencing to the MSX in 1983.\n\nThe spread of MIDI on personal computers was largely facilitated by Roland Corporation's MPU-401, released in 1984, as the first MIDI-equipped PC sound card, capable of MIDI sound processing and sequencing.[http://www.piclist.com/techref/io/serial/midi/mpu.html Programming the MPU-401 in UART mode] [ftp://ftp.oldskool.org/pub/drivers/Roland/MPU-401%20technical%20reference%20manual.pdf MIDI PROCESSING UNIT MPU-401 TECHNICAL REFERENCE MANUAL], Roland Corporation After Roland sold MPU sound chips to other sound card manufacturers,[http://www.textfiles.com/music/midi-em.txt MIDI INTERFACES FOR THE IBM PC] , Electronic Musician, September 1990 it established a universal standard MIDI-to-PC interface.Peter Manning (2013), [https://books.google.co.uk/books?idryet1i-8OlYC Electronic and Computer Music] , page 319, Oxford University Press The widespread adoption of MIDI led to computer-based MIDI software being developed. Soon after, a number of platforms began supporting MIDI, including the Plus and IIe, Apple Macintosh, Commodore 64 and Amiga, Atari ST, Acorn Archimedes, and PC DOS. The Macintosh was a favorite among US musicians, as it was marketed at a competitive price, and it took several years for PC systems to catch up with its efficiency and graphical interface.\n\nThe Atari ST was favored in Europe, where Macintoshes were more expensive. The Apple IIGS used a digital sound chip designed for the Ensoniq Mirage synthesizer, and later models used a custom sound system and upgraded processors, which drove other companies to improve their own offerings. The Atari ST was favored for its MIDI ports that were built directly into the computer. Most music software in MIDI's first decade was published for either the Apple or the Atari. By the time of Windows 3.0's 1990 release, PCs had caught up in processing power and had acquired a graphical interface, and software titles began to see release on multiple platforms.\n\nStandard files\n\nThe Standard MIDI File (SMF) is a file format that provides a standardized way for music sequences to be saved, transported, and opened in other systems. The compact size of these files led to their widespread use in computers, mobile phone ringtones, webpage authoring and musical greeting cards. These files are intended for universal use, and include such information as note values, timing and track names. Lyrics may be included as metadata, and can be displayed by karaoke machines.Hass, Jeffrey. \"[http://www.indiana.edu/%7Eemusic/etext/MIDI/chapter3_MIDI10.shtml Chapter Three: How MIDI works 10] \". Indiana University Jacobs School of Music. 2010. Web 13 August 2012 The SMF specification was developed and is maintained by the MMA.\n\nSMFs are created as an export format of software sequencers or hardware workstations. They organize MIDI messages into one or more parallel tracks, and timestamp the events so that they can be played back in sequence. A header contains the arrangement's track count, tempo and which of three SMF formats the file is in.  A type 0 file contains the entire performance, merged onto a single track, while type 1 files may contain any number of tracks that are performed in synchrony. Type 2 files are rarely used\"[http://www.midi.org/aboutmidi/tut_midifiles.php MIDI Files] \". midi.org Music Manufacturers Association. n.d. Web. 27 August 2012 and store multiple arrangements, with each arrangement having its own track and intended to be played in sequence.\nMicrosoft Windows bundles SMFs together with Downloadable Sounds (DLS) in a Resource Interchange File Format (RIFF) wrapper, as RMID files with a .rmi extension. RIFF-RMID has been deprecated in favor of Extensible Music Files (XMF).\"[http://www.digitalpreservation.gov/formats/fdd/fdd000120.shtml RIFF-based MIDI File Format] \". digitalpreservation.gov. Library of Congress. 26 March 2012. Web. 18 August 2012\n\nFile sharing\n\nA MIDI file is not a recording of actual audio. Rather, it is a set of instructions (e.g., for pitch, rhythm and other elements), and can use a thousand times less disk space than the equivalent recorded audio.Crawford, Walt. \"MIDI and Wave: Coping with the Language\". Online. Vol. 20, No. 1. Jan/Feb 1996 This made MIDI file arrangements an attractive way to share music, before the advent of broadband internet access and multi-gigabyte hard drives. Licensed MIDI files on floppy disks were commonly available in stores in Europe and Japan during the 1990s.\"MIDI Assoc. pushes for new licensing agreement. (MIDI Manufacturers Association).\" Music Trades. Music Trades Corp. 1996. HighBeam Research. 4 September 2012  The major drawback to this is the wide variation in quality of users' audio cards, and in the actual audio contained as samples or synthesized sound in the card that the MIDI data only refers to symbolically. There is no standardization of how symbols are expressed. Even a sound card that contains high-quality sampled sounds can have inconsistent quality from one sampled instrument to another, while different model cards have no guarantee of consistent sound of the same instrument. Early budget-priced cards, such as the AdLib and the Sound Blaster and its compatibles, used a stripped-down version of Yamaha's frequency modulation synthesis (FM synthesis) technologyWiffen, Paul. \"[http://www.soundonsound.com/sos/1997_articles/sep97/synthschool3.html Synth School, Part 3: Digital Synthesis (FM, PD & VPM)] \". Sound on Sound Sep 1997. Print. played back through low-quality digital-to-analog converters. The low-fidelity reproduction of these ubiquitous cards was often assumed to somehow be a property of MIDI itself. This created a perception of MIDI as low-quality audio, while in reality MIDI itself contains no sound, and the quality of its playback depends entirely on the quality of the sound-producing device (and of samples in the device).\n\nSoftware\n\nThe main advantage of the personal computer in a MIDI system is that it can serve a number of different purposes, depending on the software that is loaded. Multitasking allows simultaneous operation of programs that may be able to share data with each other.\n\nSequencers\n\nSequencing software provides a number of benefits to a composer or arranger. It allows recorded MIDI to be manipulated using standard computer editing features such as cut, copy and paste and drag and drop. Keyboard shortcuts can be used to streamline workflow, and editing functions are often selectable via MIDI commands. The sequencer allows each channel to be set to play a different sound, and gives a graphical overview of the arrangement. A variety of editing tools are made available, including a notation display that can be used to create printed parts for musicians. Tools such as looping, quantization, randomization, and transposition simplify the arranging process.\n\nBeat creation is simplified, and groove templates can be used to duplicate another track's rhythmic feel. Realistic expression can be added through the manipulation of real-time controllers. Mixing can be performed, and MIDI can be synchronized with recorded audio and video tracks. Work can be saved, and transported between different computers or studios.Gellerman, Elizabeth. \"Audio Editing SW Is Music to Multimedia Developers' Ears\". Technical Horizons in Education  Journal. Vol. 22, No. 2. Sep 1994Desmond, Peter. \"ICT in the Secondary Music Curriculum\". Aspects of Teaching Secondary Music: Perspectives on Practice. ed. Gary Spruce. New York: RoutledgeFalmer, 2002\n\nSequencers may take alternate forms, such as drum pattern editors that allow users to create beats by clicking on pattern grids, and loop sequencers such as ACID Pro, which allow MIDI to be combined with prerecorded audio loops whose tempos and keys are matched to each other. Cue list sequencing is used to trigger dialogue, sound effect, and music cues in stage and broadcast production.\n\nNotation/scoring software\n\nWith MIDI, notes played on a keyboard can automatically be transcribed to sheet music.Holmes, Thom. Electronic and Experimental Music: Pioneers in Technology and Composition. New York: Routledge, 2003 Scorewriting software typically lacks advanced sequencing tools, and is optimized for the creation of a neat, professional printout designed for live instrumentalists. These programs provide support for dynamics and expression markings, chord and lyric display, and complex score styles. Software is available that can print scores in braille.Solomon, Karen. \"[https://www.wired.com/culture/lifestyle/news/2000/02/34495 You Gotta Feel the Music] \". wired.com. Condé Nast. 27 February 2000. Web. 13 August 2012.\n\nSmartScore software can produce MIDI files from scanned sheet music.Cook, Janet Harniman. \"[http://www.soundonsound.com/sos/dec98/articles/midiscan.265.htm Musitek Midiscan v2.51] \". Sound On Sound. SOS Publications. Dec 1998. Print. Other notation programs include Finale, Encore, Sibelius and MuseScore.\n\nEditor/librarians\n\nPatch editors allow users to program their equipment through the computer interface. These became essential with the appearance of complex synthesizers such as the Yamaha FS1R,Johnson, Derek. \"[http://www.soundonsound.com/sos/mar99/articles/yamahafs1r.htm Yamaha FS1R Editor Software] \". Sound on Sound. Mar 1999. which contained several thousand programmable parameters, but had an interface that consisted of fifteen tiny buttons, four knobs and a small LCD.Johnson, Derek, and Debbie Poyser. \"[http://www.soundonsound.com/sos/dec98/articles/yamfs1r.549.htm Yamaha FS1R] \". Sound on Sound. Dec 1998. Digital instruments typically discourage users from experimentation, due to their lack of the feedback and direct control that switches and knobs would provide, but patch editors give owners of hardware instruments and effects devices the same editing functionality that is available to users of software synthesizers.\"[http://www.squest.com/Products/MidiQuest11/index.html Sound Quest MIDI Quest 11 Universal Editor] \". squest.com. n.p. n.d. Web. 21 August 2012 Some editors are designed for a specific instrument or effects device, while other, \"universal\" editors support a variety of equipment, and ideally can control the parameters of every device in a setup through the use of System Exclusive commands.\n\nPatch librarians have the specialized function of organizing the sounds in a collection of equipment, and allow transmission of entire banks of sounds between an instrument and a computer. This allows the user to augment the device's limited patch storage with a computer's much greater disk capacity, and to share custom patches with other owners of the same instrument.\"[http://www.cakewalk.com/support/kb/reader.aspx/2007013074 Desktop Music Handbook – MIDI] \". cakewalk.com. Cakewalk, Inc. 26 November 2010. Web. Retrieved 7 August 2012. Universal editor/librarians that combine the two functions were once common, and included Opcode Systems' Galaxy and eMagic's SoundDiver. These programs have been largely abandoned with the trend toward computer-based synthesis, although Mark of the Unicorn's (MOTU)'s Unisyn and Sound Quest's Midi Quest remain available. Native Instruments' Kore was an effort to bring the editor/librarian concept into the age of software instruments.\n\nAuto-accompaniment programs\n\nPrograms that can dynamically generate accompaniment tracks are called \"auto-accompaniment\" programs. These create a full band arrangement in a style that the user selects, and send the result to a MIDI sound generating device for playback. The generated tracks can be used as educational or practice tools, as accompaniment for live performances, or as a songwriting aid.\n\nSynthesis and sampling\n\nComputers can use software to generate sounds, which are then passed through a digital-to-analog converter (DAC) to a power amplifier and loudspeaker system. The number of sounds that can be played simultaneously (the polyphony) is dependent on the power of the computer's CPU, as are the sample rate and bit depth of playback, which directly affect the quality of the sound.Lehrman, Paul D. \"[http://www.soundonsound.com/sos/1995_articles/oct95/softwaresynthesis.html Software Synthesis: The Wave Of The Future?] \" Sound On Sound. SOS Publications. Oct 1995. Print. Synthesizers implemented in software are subject to timing issues that are not present with hardware instruments, whose dedicated operating systems are not subject to interruption from background tasks as desktop operating systems are. These timing issues can cause synchronization problems, and clicks and pops when sample playback is interrupted. Software synthesizers also exhibit a noticeable delay known as latency in their sound generation, because computers use an audio buffer that delays playback and disrupts MIDI timing.Walker, Martin. \"[http://www.soundonsound.com/sos/mar01/articles/pcmusician.asp Identifying & Solving PC MIDI & Audio Timing Problems] \". Sound On Sound. SOS Publications. Mar 2001. Print.\n\nSoftware synthesis' roots go back as far as the 1950s, when Max Mathews of Bell Labs wrote the MUSIC-N programming language, which was capable of non-real-time sound generation.Miller, Dennis. \"[http://www.soundonsound.com/sos/1997_articles/may97/softwaresynth2.html Sound Synthesis On A Computer, Part 2] \". Sound On Sound. SOS Publications. May 1997. Print. The first synthesizer to run directly on a host computer's CPU\"[http://www.keyboardmag.com/article/Midi-Ancestors-and-Milestones/2171 MIDI Ancestors and Milestones] \". keyboardmag.com. New Bay Media. n.d. Web. 6 August 2012. was Reality, by Dave Smith's Seer Systems, which achieved a low latency through tight driver integration, and therefore could run only on Creative Labs soundcards.Walker, Martin. \"[http://www.soundonsound.com/sos/1997_articles/nov97/seerreality.html Reality PC] \". Sound On Sound. SOS Publications. Nov 1997. Print. Some systems use dedicated hardware to reduce the load on the host CPU, as with Symbolic Sound Corporation's Kyma System, and the Creamware/Sonic Core Pulsar/SCOPE systems,Wherry, Mark. \"[http://www.soundonsound.com/sos/jun03/articles/creamwarescope.asp Creamware SCOPE] \". Sound On Sound. SOS Publications. Jun 2003. Print. which power an entire recording studio's worth of instruments, effect units, and mixers.Anderton, Craig. \"[http://www.keyboardmag.com/article/sonic-core-scope-xite-1/147874 Sonic Core SCOPE Xite-1] \". keyboardmag.com. New Bay Media, LLC. n.d. Web.\n\nThe ability to construct full MIDI arrangements entirely in computer software allows a composer to render a finalized result directly as an audio file.\n\nGame music\n\nEarly PC games were distributed on floppy disks, and the small size of MIDI files made them a viable means of providing soundtracks. Games of the DOS and early Windows eras typically required compatibility with either Ad Lib or Sound Blaster audio cards. These cards used FM synthesis, which generates sound through modulation of sine waves. John Chowning, the technique's pioneer, theorized that the technology would be capable of accurate recreation of any sound if enough sine waves were used, but budget computer audio cards performed FM synthesis with only two sine waves. Combined with the cards' 8-bit audio, this resulted in a sound described as \"artificial\"David Nicholson. \"[http://www.highbeam.com/doc/1P2-946733.html HARDWARE].\"  The Washington Post. Washingtonpost Newsweek Interactive. 1993. HighBeam Research. 4 September 2012 and \"primitive\".Levy, David S. \"[http://www.highbeam.com/doc/1G1-14803399.html Aztech's WavePower daughtercard improves FM reception. (Aztech Labs Inc.'s wavetable synthesis add-on card for Sound Blaster 16 or Sound Galaxy Pro 16 sound cards) (Hardware Review) (Evaluation).] \" Computer Shopper. SX2 Media Labs LLC. 1994. HighBeam Research. 4 September 2012\n\nWavetable daughterboards that were later available provided audio samples that could be used in place of the FM sound. These were expensive, but often used the sounds from respected MIDI instruments such as the E-mu Proteus. The computer industry moved in the mid-1990s toward wavetable-based soundcards with 16-bit playback, but standardized on a 2MB ROM, a space too small in which to fit good-quality samples of 128 instruments plus drum kits. Some manufacturers used 12-bit samples, and padded those to 16 bits.Labriola, Don. \"[http://www.highbeam.com/doc/1G1-16232686.html MIDI masters: wavetable synthesis brings sonic realism to inexpensive sound cards. (review of eight Musical Instrument Digital Interface sound cards) (includes related articles about testing methodology, pitfalls of wavetable technology, future wavetable developments) (Hardware Review) (Evaluation).]\"  Computer Shopper. SX2 Media Labs LLC. 1994. HighBeam Research. 4 September 2012\n\nOther applications\n\nMIDI has been adopted as a control protocol in a number of non-musical applications. MIDI Show Control uses MIDI commands to direct stage lighting systems and to trigger cued events in theatrical productions. VJs and turntablists use it to cue clips, and to synchronize equipment, and recording systems use it for synchronization and automation. Apple Motion allows control of animation parameters through MIDI. The 1987 first-person shooter game MIDI Maze and the 1990 Atari ST computer puzzle game Oxyd used MIDI to network computers together, and kits are available that allow MIDI control over home lighting and appliances.\"[http://midikits.net23.net/midi_10_out/interface_circuits.htm Interface Circuits] \". MIDI Kits. n.p. 30 August 2012. Web. 30 August 2012.\n\nDespite its association with music devices, MIDI can control any electronic or digital device that can read and process a MIDI command. It is therefore possible to send a spacecraft from Earth to another destination in space, control home lighting, heating and air conditioning and even sequence traffic light signals all through MIDI commands. The receiving device or object would require a General MIDI processor, however in this instance, the program changes would trigger a function on that device rather than notes from a MIDI instrument's controller.  Each function can be set to a timer (also controlled by MIDI) or other condition or trigger determined by the device's creator.\n\nDevices\n\nConnectors\n\nThe cables terminate in a 180° five-pin DIN connector. Standard applications use only three of the five conductors: a ground wire, and a balanced pair of conductors that carry a +5 volt signal.Bozeman, William C. Educational Technology: Best Practices from America's Schools. Larchmont: Eye on Education, 1999.  This connector configuration can only carry messages in one direction, so a second cable is necessary for two-way communication. Some proprietary applications, such as phantom-powered footswitch controllers, use the spare pins for direct current (DC) power transmission.Lockwood, Dave. \"[http://www.soundonsound.com/sos/dec01/articles/tcgmajor.asp TC Electronic G Major] \". Sound On Sound. SOS Publications. Dec 2001. Print.\n\nOpto-isolators keep MIDI devices electrically separated from their connectors, which prevents the occurrence of ground loopsMornington-West, Allen. \"Digital Theory\". Sound Recording Practice. 4th Ed. Ed. John Borwick. Oxford: Oxford University Press, 1996. and protects equipment from voltage spikes. There is no error detection capability in MIDI, so the maximum cable length is set at 15 meters (50 feet) in order to limit interference.\"[http://www.richmondsounddesign.com/faq.html#midilen Richmond Sound Design – Frequently Asked Questions] \". richmondsounddesign.com. Web. 5 August 2012.\n\nMost devices do not copy messages from their input to their output port. A third type of port, the \"thru\" port, emits a copy of everything received at the input port, allowing data to be forwarded to another instrument in a \"daisy chain\" arrangement.Hass, Jeffrey. \"[http://www.indiana.edu/%7Eemusic/etext/MIDI/chapter3_MIDI2.shtml Chapter Three: How MIDI works 2] \". Indiana University Jacobs School of Music. 2010. Web. 13 August 2012. Not all devices contain thru ports, and devices that lack the ability to generate MIDI data, such as effects units and sound modules, may not include out ports.Gibbs, Jonathan (Rev. by Peter Howell) \"Electronic Music\". Sound Recording Practice, 4th Ed. Ed. John Borwick. Oxford: Oxford University Press, 1996\n\nManagement devices\n\nEach device in a daisy chain adds delay to the system. This is avoided with a MIDI thru box, which contains several outputs that provide an exact copy of the box's input signal. A MIDI merger is able to combine the input from multiple devices into a single stream, and allows multiple controllers to be connected to a single device. A MIDI switcher allows switching between multiple devices, and eliminates the need to physically repatch cables. MIDI patch bays combine all of these functions. They contain multiple inputs and outputs, and allow any combination of input channels to be routed to any combination of output channels. Routing setups can be created using computer software, stored in memory, and selected by MIDI program change commands. This enables the devices to function as standalone MIDI routers in situations where no computer is present. MIDI patch bays also clean up any skewing of MIDI data bits that occurs at the input stage.\n\nMIDI data processors are used for utility tasks and special effects. These include MIDI filters, which remove unwanted MIDI data from the stream, and MIDI delays, effects which send a repeated copy of the input data at a set time.\n\nInterfaces\n\nA computer MIDI interface's main function is to match clock speeds between the MIDI device and the computer. Some computer sound cards include a standard MIDI connector, whereas others connect by any of various means that include the D-subminiature DA-15 game port, USB, FireWire, Ethernet or a proprietary connection. The increasing use of USB connectors in the 2000s has led to the availability of MIDI-to-USB data interfaces that can transfer MIDI channels to USB-equipped computers. Some MIDI keyboard controllers are equipped with USB jacks, and can be plugged into computers that run music software.\n\nMIDI's serial transmission leads to timing problems. Experienced musicians can detect time differences of as small as 1/3 of a millisecond (ms) (which is how long it takes sound to travel 4 inches), and a three-byte MIDI message requires nearly 1ms for transmission.Robinson, Herbie. \"[http://lists.apple.com/archives/coreaudio-api/2005/Jul/msg00120.html Re: core midi time stamping] \". Apple Coreaudio-api Mailing List. Apple, Inc. 18 July 2005. 8 August 2012. Because MIDI is serial, it can only send one event at a time. If an event is sent on two channels at once, the event on the higher-numbered channel cannot transmit until the first one is finished, and so is delayed by 1ms. If an event is sent on all channels at the same time, the highest-numbered channel's transmission will be delayed by as much as 16ms. This contributed to the rise of MIDI interfaces with multiple in- and out-ports, because timing improves when events are spread between multiple ports as opposed to multiple channels on the same port. The term \"MIDI slop\" refers to audible timing errors that result when MIDI transmission is delayed.Shirak, Rob. \"[http://www.emusician.com/news/0766/mark-of-the-unicorn/140335 Mark of the Unicorn] \". emusician.com. New Bay Media. 1 October 2000. Web. Retrieved 8 August 2012.\n\nControllers\n\nThere are two types of MIDI controllers: performance controllers that generate notes and are used to perform music,\"[http://www.rolandmusiced.com/spotlight/article.php?ArticleId=1040 MIDI Performance Instruments] \". Instruments of Change. Vol. 3, No. 1 (Winter 1999). Roland Corporation, U.S. and controllers which may not send notes, but transmit other types of real-time events. Many devices are some combination of the two types.\n\nKeyboards are by far the most common type of MIDI controller. MIDI was designed with keyboards in mind, and any controller that is not a keyboard is considered an \"alternative\" controller.\"[http://www.midi.org/aboutmidi/products.php MIDI Products] \". midi.org. MIDI Manufacturers Association. n.d. 1 August 1012 This was seen as a limitation by composers who were not interested in keyboard-based music, but the standard proved flexible, and MIDI compatibility was introduced to other types of controllers, including guitars, stringed and wind instruments, drums and specialized and experimental controllers. Other controllers include drum controllers and wind controllers, which can emulate the playing of drum kit and wind instruments, respectively.\n\nSoftware synthesizers offer great power and versatility, but some players feel that division of attention between a MIDI keyboard and a computer keyboard and mouse robs some of the immediacy from the playing experience.Preve, Francis. \"Dave Smith\", in \"The 1st Annual Keyboard Hall of Fame\". Keyboard (US). NewBay Media, LLC. Sep 2012. Print. p.18 Devices dedicated to real-time MIDI control provide an ergonomic benefit, and can provide a greater sense of connection with the instrument than an interface that is accessed through a mouse or a push-button digital menu. Controllers may be general-purpose devices that are designed to work with a variety of equipment, or they may be designed to work with a specific piece of software. Examples of the latter include Akai's APC40 controller for Ableton Live, and Korg's MS-20ic controller that is a reproduction of their MS-20 analog synthesizer. The MS-20ic controller includes patch cables that can be used to control signal routing in their virtual reproduction of the MS-20 synthesizer, and can also control third-party devices.\"[http://www.vintagesynth.com/korg/legacy.php Korg Legacy Collection] \". vintagesynth.com. Vintage Synth Explorer. n.d. Web. 21 August 2012\n\nInstruments\n\nA MIDI instrument contains ports to send and receive MIDI signals, a CPU to process those signals, an interface that allows user programming, audio circuitry to generate sound, and controllers. The operating system and factory sounds are often stored in a Read-only memory (ROM) unit.\n\nA MIDI instrument can also be a stand-alone module (without a piano style keyboard) consisting of a General MIDI soundboard (GM, GS and/XG), onboard editing, including transposing/pitch changes, MIDI instrument changes and adjusting volume, pan, reverb levels and other MIDI controllers. Typically, the MIDI Module will include a large screen, enabling the user to view information depending on the function selected at that time. Features can include scrolling lyrics, usually embedded in a MIDI file or karaoke MIDI, playlists, song library and editing screens. Some MIDI Modules include a Harmonizer and the ability to playback and transpose MP3 audio files.\n\nSynthesizers\n\nSynthesizers may employ any of a variety of sound generation techniques. They may include an integrated keyboard, or may exist as \"sound modules\" or \"expanders\" that generate sounds when triggered by an external controller, such as a MIDI keyboard. Sound modules are typically designed to be mounted in a 19-inch rack. Manufacturers commonly produce a synthesizer in both standalone and rack-mounted versions, and often offer the keyboard version in a variety of sizes.\n\nSamplers\n\nA sampler can record and digitize audio, store it in random-access memory (RAM), and play it back. Samplers typically allow a user to edit a sample and save it to a hard disk, apply effects to it, and shape it with the same tools that synthesizers use. They also may be available in either keyboard or rack-mounted form. Instruments that generate sounds through sample playback, but have no recording capabilities, are known as \"ROMplers\".\n\nSamplers did not become established as viable MIDI instruments as quickly as synthesizers did, due to the expense of memory and processing power at the time. The first low-cost MIDI sampler was the Ensoniq Mirage, introduced in 1984. MIDI samplers are typically limited by displays that are too small to use to edit sampled waveforms, although some can be connected to a computer monitor.\n\nDrum machines\n\nDrum machines typically are sample playback devices that specialize in drum and percussion sounds. They commonly contain a sequencer that allows the creation of drum patterns, and allows them to be arranged into a song. There often are multiple audio outputs, so that each sound or group of sounds can be routed to a separate output. The individual drum voices may be playable from another MIDI instrument, or from a sequencer.\n\nWorkstations and hardware sequencers\n\nSequencer technology predates MIDI. Analog sequencers use CV/Gate signals to control pre-MIDI analog synthesizers. MIDI sequencers typically are operated by transport features modeled after those of  tape decks. They are capable of recording MIDI performances, and arranging them into individual tracks along a multitrack recording concept. Music workstations combine controller keyboards with an internal sound generator and a sequencer. These can be used to build complete arrangements and play them back using their own internal sounds, and function as self-contained music production studios. They commonly include file storage and transfer capabilities.\n\nEffects devices\n\nAudio effects units that are frequently used in stage and recording, such as reverbs, delays and choruses, can be remotely adjusted via MIDI signals. Some units allow only a limited number of parameters to be controlled this way, but most will respond to program change messages. The Eventide H3000 Ultra-harmonizer is an example of a unit that allows such extensive MIDI control that it is playable as a synthesizer.Manning, Peter. Electronic and Computer Music. 1985. Oxford: Oxford University Press, 1994. Print.\n\nTechnical specifications\n\nMIDI messages are made up of 8-bit words (commonly called bytes) that are transmitted serially at a rate of 31.25 kbit/s. This rate was chosen because it is an exact division of 1 MHz, the speed at which many early microprocessors operated. The first bit of each word identifies whether the word is a status byte or a data byte, and is followed by seven bits of information. A start bit and a stop bit are added to each byte for framing purposes, so a MIDI byte requires ten bits for transmission.\n\nA MIDI link can carry sixteen independent channels of information. The channels are numbered 1–16, but their actual corresponding binary encoding is 0–15. A device can be configured to only listen to specific channels and to ignore the messages sent on other channels (\"Omni Off\" mode), or it can listen to all channels, effectively ignoring the channel address (\"Omni On\"). An individual device may be monophonic (the start of a new \"note-on\" MIDI command implies the termination of the previous note), or polyphonic (multiple notes may be sounding at once, until the polyphony limit of the instrument is reached, or the notes reach the end of their decay envelope, or explicit \"note-off\" MIDI commands are received). Receiving devices can typically be set to all four combinations of \"omni off/on\" versus \"mono/poly\" modes.\n\nMessages\n\nA MIDI message is an instruction that controls some aspect of the receiving device. A MIDI message consists of a status byte, which indicates the type of the message, followed by up to two data bytes that contain the parameters.Brewster, Stephen. \"Nonspeech Auditory Output\". The Human-Computer Interaction Handbook: Fundamentals, Evolving Technologies, and Emerging Applications. Ed. Julie A. Jacko; Andrew Sears. Mahwah: Lawrence Erlbaum Associates, 2003. p.227 MIDI messages can be \"channel messages\", which are sent on only one of the 16 channels and can be heard only by devices receiving on that channel, or \"system messages\", which are heard by all devices. Any data not relevant to a receiving device is ignored. There are five types of message: Channel Voice, Channel Mode, System Common, System Real-Time, and System Exclusive.Hass, Jeffrey. \"[http://www.indiana.edu/%7Eemusic/etext/MIDI/chapter3_MIDI3.shtml Chapter Three: How MIDI works 3] \". Indiana University Jacobs School of Music. 2010. Web. 13 August 2012.\n\nChannel Voice messages transmit real-time performance data over a single channel. Examples include \"note-on\" messages which contain a MIDI note number that specifies the note's pitch, a velocity value that indicates how forcefully the note was played, and the channel number; \"note-off\" messages that end a note; program change messages that change a device's patch; and control changes that allow adjustment of an instrument's parameters. Channel Mode messages include the Omni/mono/poly mode on and off messages, as well as messages to reset all controllers to their default state or to send \"note-off\" messages for all notes. System messages do not include channel numbers, and are received by every device in the MIDI chain. MIDI time code is an example of a System Common message. System Real-Time messages provide for synchronization, and include MIDI clock and Active Sensing.\n\nSystem Exclusive messages \n\nSystem Exclusive (SysEx) messages are a major reason for the flexibility and longevity of the MIDI standard. They allow manufacturers to create proprietary messages which provide control over their equipment in a way that is more thorough than is provided for by standard MIDI messages. SysEx messages are addressed to a specific device in a system. Each manufacturer has a unique identifier that is included in its SysEx messages, which helps ensure that the messages will be heard only by the targeted device, and ignored by all others. Many instruments also include a SysEx ID setting, which allows two devices of the same model to be addressed independently while connected to the same system.Hass, Jeffrey. \"[http://www.indiana.edu/%7Eemusic/etext/MIDI/chapter3_MIDI9.shtml Chapter Three: How MIDI works 9] \". Indiana University Jacobs School of Music. 2010. Web. 13 August 2012. SysEx messages may include functionality beyond what the MIDI standard provides. They are targeted at a specific instrument, and are ignored by all other devices on the system.\n\nImplementation chart\n\nDevices typically do not respond to every type of message defined by the MIDI specification. The MIDI implementation chart was standardized by the MMA as a way for users to see what specific capabilities an instrument has, and how it responds to messages. A specific MIDI Implementation Chart is usually published for each MIDI device within the device documentation.\n\nElectrical specifications\n\nThe MIDI specification for the electrical interface is based on a fully isolated current loop. The MIDI out port nominally sources a +5 volt source through a 220 ohm resistor out through pin 4 on the MIDI out DIN connector, in on pin 4 of the receiving device's MIDI in DIN connector, through a 220 ohm protection resistor and the LED of an opto-isolator.  The current then returns via pin 5 on the MIDI in port to the originating device's MIDI out port pin 5, again with a 220 ohm resistor in the path, giving a nominal current of about 5 milliamperes.   Despite the cable's appearance, there is no conductive path between the two MIDI devices, only an optically isolated one. Properly designed MIDI devices are relatively immune to ground loops and similar interference.  The data rate on this system is 31,250 bits per second, logic 0 being current on.\n\nThe MIDI specification provides for a ground \"wire\" and a braid or foil shield, connected on pin 2, protecting the two signal-carrying conductors on pins 4 and 5.  Although the MIDI cable is supposed to connect pin 2 and the braid or foil shield to chassis ground, it should do so only at the MIDI out port; the MIDI in port should leave pin 2 unconnected and isolated. Some large manufacturers of MIDI devices use modified MIDI in-only DIN 5-pin sockets with the metallic conductors intentionally omitted at pin positions 1, 2, and 3 so that the maximum voltage isolation is obtained.\n\nExtensions\n\nMIDI's flexibility and widespread adoption have led to many refinements of the standard, and have enabled its application to purposes beyond those for which it was originally intended.\n\nGeneral MIDI\n\nMIDI allows selection of an instrument's sounds through program change messages, but there is no guarantee that any two instruments have the same sound at a given program location.Bello, Juan P. \"[http://www.nyu.edu/classes/bello/FMT_files/10_MIDI_soundcontrol.pdf MIDI: sound control] \". nyu.edu. New York University. n.d. Web. 18 August 2012 Program #0 may be a piano on one instrument, or a flute on another. The General MIDI (GM) standard was established in 1991, and provides a standardized sound bank that allows a Standard MIDI File created on one device to sound similar when played back on another. GM specifies a bank of 128 sounds arranged into 16 families of eight related instruments, and assigns a specific program number to each instrument. Percussion instruments are placed on channel 10, and a specific MIDI note value is mapped to each percussion sound. GM-compliant devices must offer 24-note polyphony. Any given program change will select the same instrument sound on any GM-compatible instrument.\"[http://academic.pgcc.edu/~njudy/mt/MIDI/gm.html General MIDI Standard] \". pgcc.edu. Prince George's Community College. n.d. Web.\n\nThe GM standard eliminates variation in note mapping. Some manufacturers had disagreed over what note number should represent middle C, but GM specifies that note number 69 plays A440, which in turn fixes middle C as note number 60. GM-compatible devices are required to respond to velocity, aftertouch, and pitch bend, to be set to specified default values at startup, and to support certain controller numbers such as for sustain pedal, and Registered Parameter Numbers.Glatt, Jeff. \"[http://home.roadrunner.com/~jgglatt/tutr/gm.htm General MIDI] \". The MIDI Technical Fanatic's Brainwashing Center. n.p. n.d. Web. 17 August 2012 A simplified version of GM, called \"GM Lite\", is used in mobile phones and other devices with limited processing power.\n\nGS, XG, and GM2\n\nA general opinion quickly formed that the GM's 128-instrument sound set was not large enough. Roland's General Standard, or GS, system included additional sounds, drumkits and effects, provided a \"bank select\" command that could be used to access them, and used MIDI Non-Registered Parameter Numbers (NRPNs) to access its new features. Yamaha's Extended General MIDI, or XG, followed in 1994. XG similarly offered extra sounds, drumkits and effects, but used standard controllers instead of NRPNs for editing, and increased polyphony to 32 voices. Both standards feature backward compatibility with the GM specification, but are not compatible with each other.Nagle, Paul. \"[http://www.soundonsound.com/sos/1995_articles/sep95/yamahamu50.html Yamaha MU50 & Yamaha CBX-K1] \". Sound On Sound. SOS Publications. Sep 1995. Print. Neither standard has been adopted beyond its creator, but both are commonly supported by music software titles.\n\nMember companies of Japan's AMEI developed the General MIDI Level 2 specification in 1999. GM2 maintains backward compatibility with GM, but increases polyphony to 32 voices, standardizes several controller numbers such as for sostenuto and soft pedal (una corda), RPNs and Universal System Exclusive Messages, and incorporates the MIDI Tuning Standard.\"[http://www.midi.org/techspecs/gm.php About General MIDI] \". midi.org. MIDI Manufacturers Association. n.d. Web. 17 August 2012 GM2 is the basis of the instrument selection mechanism in Scalable Polyphony MIDI (SP-MIDI), a MIDI variant for low power devices that allows the device's polyphony to scale according to its processing power.\n\nTuning standard\n\nMost MIDI synthesizers use equal temperament tuning. The MIDI tuning standard (MTS), ratified in 1992, allows alternate tunings.\"[http://www.microtonal-synthesis.com/MIDItuning.html The MIDI Tuning Standard] \". microtonal-synthesis.com. n.p. n.d. Web. 17 August 2012 MTS allows microtunings that can be loaded from a bank of up to 128 patches, and allows real-time adjustment of note pitches. Manufacturers are not required to support the standard. Those who do are not required to implement all of its features.\n\nTime code\n\nA sequencer can drive a MIDI system with its internal clock, but when a system contains multiple sequencers, they must synchronize to a common clock. MIDI Time Code (MTC), developed by Digidesign,Glatt, Jeff. \"[http://home.roadrunner.com/~jgglatt/tutr/history.htm The beginnings of MIDI] \". The MIDI Technical Fanatic's Brainwashing Center. n.p. n.d. Web. 13 August 2012. implements SysEx messagesGlatt, Jeff. \"[http://home.roadrunner.com/~jgglatt/tech/mtc.htm MIDI Time Code] \". The MIDI Technical Fanatic's Brainwashing Center. n.p. n.d. Web. 13 August 2012. that have been developed specifically for timing purposes, and is able to translate to and from the SMPTE time code standard. MIDI Clock is based on tempo, but SMPTE time code is based on frames per second, and is independent of tempo. MTC, like SMPTE code, includes position information, and can adjust itself if a timing pulse is lost.White, Paul. \"[http://www.soundonsound.com/sos/1996_articles/jun96/miditimecode.html SMPTE & MTC (MIDI Time Code)] \" Sound On Sound. SOS Publications. Jun 1996. Print. MIDI interfaces such as Mark of the Unicorn's MIDI Timepiece can convert SMPTE code to MTC.\n\nMachine control\n\nMIDI Machine Control (MMC) consists of a set of SysEx commandsGlatt, Jeff. \"[http://home.roadrunner.com/~jgglatt/tech/mmc.htm MIDI Machine Control (MMC)] \". The MIDI Technical Fanatic's Brainwashing Center. n.p. n.d. Web. that operate the transport controls of hardware recording devices. MMC allows a sequencer to send \"Start\", \"Stop\", and \"Record\" commands to a connected tape deck or hard disk recording system, and to fast-forward or rewind the device so that it starts playback at the same point as the sequencer. No synchronization data is involved, although the devices may synchronize through MTC.\"[http://www.sweetwater.com/expert-center/glossary/t--MMC Glossary: MIDI Machine Control (MMC)] \". sweetwater.com. Sweetwater Sound. n.d. Web. 15 August 2012.\n\nShow control\n\nMIDI Show Control (MSC) is a set of SysEx commands which allows sequencing and remote cueing of show control devices such as lighting, music and sound playback, and motion control systems.\"[http://people.virginia.edu/~rlk3p/desource/TechNotes/MSC.html An Inexpensive MIDI show-control System] \". Lighting TechNotes. The University of Virginia. 25 October 2004. Web. 17 August 2012. Applications include stage productions, museum exhibits, recording studio control systems, and amusement park attractions.\n\nTimestamping\n\nOne solution to MIDI timing problems is to mark MIDI events with the times they are to be played, and store them in a buffer in the MIDI interface ahead of time. Sending data beforehand reduces the likelihood that a busy passage will send a large amount of information that will overwhelm the transmission link. Once stored in the interface, the information is no longer subject to timing issues associated with USB jitter and computer operating system interrupts, and can be transmitted with a high degree of accuracy.\"[http://www.sweetwater.com/expert-center/glossary/t--MTS-MOTU Glossary: MTS (MIDI Time Stamping)] \". sweetwater.com. Sweetwater Sound. n.d. Web. 17 August 2012 MIDI timestamping only works when both the hardware and software support it. MOTU's MTS, eMagic's AMT, and Steinberg's Midex 8 were implementations that were incompatible with each other, and required users to own software and hardware manufactured by the same company in order to gain its benefits. Timestamping is built into FireWire MIDI interfaces,Walker, Martin. \"[http://www.soundonsound.com/sos/Oct02/articles/pcmusician1002.asp The Truth About Latency: Part 2] \". Sound On Sound. SOS Publications. Oct 2002. Print. Mac OS X Core Audio, and Linux ALSA Sequencer.\n\nSample dump standard\n\nAn unforeseen capability of SysEx messages was their use for transporting audio samples between instruments. This led to the development of the sample dump standard (SDS), which established a new SysEx format for sample transmission. The SDS was later augmented with a pair of commands that allow the transmission of information about sample loop points, without requiring that the entire sample be transmitted.Glatt, Jeff. [https://web.archive.org/web/20111115234241/http://home.roadrunner.com/~jgglatt/tech/sds.htm]. The MIDI Technical Fanatic's Brainwashing Center. n.p. n.d. Web. 13 August 2012.\n\nDownloadable sounds\n\nThe Downloadable Sounds (DLS) specification, ratified in 1997, allows mobile devices and computer sound cards to expand their wave tables with downloadable sound sets. The DLS Level 2 Specification followed in 2006, and defined a standardized synthesizer architecture. The Mobile DLS standard calls for DLS banks to be combined with SP-MIDI, as self-contained Mobile XMF files.\n\nAlternative hardware transports\n\nIn addition to the original 31.25 kbit/s current-loop transported on 5-pin DIN, other connectors have been used for the same electrical data, and transmission of MIDI streams in different forms over USB, IEEE 1394 a.k.a. FireWire, and Ethernet is now common. Some samplers and hard drive recorders can also pass MIDI data between each other over SCSI.\n\nUSB and FireWire\n\nMembers of the USB-IF in 1999 developed a standard for MIDI over USB, the \"Universal Serial Bus Device Class Definition for MIDI Devices\"Ashour, Gal, et al. . usb.org [http://www.usb.org/developers/docs/devclass_docs/midi10.pdf USB Implementers Forum] . 1 November 1999. Web. 22 August 2012 MIDI over USB has become increasingly common as other interfaces that had been used for MIDI connections (serial, joystick, etc.) disappeared from personal computers. Linux, Microsoft Windows, Macintosh OS X, and Apple iOS operating systems include standard class drivers to support devices that use the \"Universal Serial Bus Device Class Definition for MIDI Devices\". Some manufacturers choose to implement a MIDI interface over USB that is designed to operate differently from the class specification, using custom drivers.\n\nApple Computer developed the FireWire interface during the 1990s. It began to appear on digital video cameras toward the end of the decade, and on G3 Macintosh models in 1999.Wiffen, Paul. \"[http://www.soundonsound.com/sos/aug00/articles/mlan.htm An Introduction To mLAN, Part 1] \". Sound On Sound. SOS Publications. Aug 2000. Print. It was created for use with multimedia applications.  Unlike USB, FireWire uses intelligent controllers that can manage their own transmission without attention from the main CPU.Wiffen, Paul. \"[http://www.soundonsound.com/sos/sep00/articles/mlan.htm An Introduction To mLAN, Part 2] \". Sound On Sound. SOS Publications. Sep 2000. Print. As with standard MIDI devices, FireWire devices can communicate with each other with no computer present.\n\nXLR connectors\n\nThe Octave-Plateau Voyetra-8 synthesizer was an early MIDI implementation using XLR3 connectors in place of the 5-pin DIN. It was released in the pre-MIDI years and later retrofitted with a MIDI interface but keeping its XLR connector.\n\nSerial parallel, and joystick port \n\nAs computer-based studio setups became common, MIDI devices that could connect directly to a computer became available. These typically used the 8-pin mini-DIN connector that was used by Apple for serial and printer ports prior to the introduction of the Blue & White G3 models. MIDI interfaces intended for use as the centerpiece of a studio, such as the Mark of the Unicorn MIDI Time Piece, were made possible by a \"fast\" transmission mode that could take advantage of these serial ports' ability to operate at 20 times the standard MIDI speed.\"[http://www.midi.org/aboutmidi/tut_midicables.php MIDI Cables & Transports] \". midi.org. Music Manufacturers Association. n.d. Web. 27 August 2012. Mini-DIN ports were built into some late-1990s MIDI instruments, and enabled such devices to be connected directly to a computer.\"CS2x Control Synthesizer Owner's Manual\". Yamaha Corporation, 1998. Some devices connected via PCs' DB-25 parallel port, or through the joystick port found in many PC sound cards.\n\nmLAN\n\nYamaha introduced the mLAN protocol in 1999. It was conceived as a Local Area Network for musical instruments using FireWire as the transport, and was designed to carry multiple MIDI channels together with multichannel digital audio, data file transfers, and time code. mLan was used in a number of Yamaha products, notably digital mixing consoles and the Motif synthesizer, and in third-party products such as the PreSonus FIREstation and the Korg Triton Studio. No new mLan products have been released since 2007.\n\nEthernet\n\nThe computer network implementation of MIDI provides network routing capabilities, and provides the high-bandwidth channel that earlier alternatives to MIDI, such as ZIPI, were intended to bring. Proprietary implementations have existed since the 1980s, some of which use fiber optic cables for transmission. The Internet Engineering Task Force's RTP MIDI open specification is gaining industry support, as proprietary MIDI/IP protocols require expensive licensing fees, or provide no advantage, apart from speed, over the original MIDI protocol. Apple has supported this protocol from Mac OS X 10.4 onwards, and a Windows driver based on Apple's implementation exists for Windows XP and newer versions.\"rtpMIDI\". tobias-erichsen.de. n.p. n.d. Web. 22 August 2012 [http://www.tobias-erichsen.de/software/rtpmidi.html Windows RTP-MIDI driver download] \n\nWireless\n\nSystems for wireless MIDI transmission have been available since the 1980s. Several commercially available transmitters allow wireless transmission of MIDI and OSC signals over Wi-Fi and Bluetooth.Kirn, Peter. \"[http://createdigitalmusic.com/2011/03/golden-age-of-wireless-korg-ios-sync-android-midi-hardware-enter-bluetooth-midi/ Golden Age of Wireless: Korg iOS Sync, Android + MIDI Hardware, Enter Bluetooth MIDI?] \". createdigitalmusic.com. n.p. 25 March 2011. Web. iOS devices are able to function as MIDI control surfaces, using Wi-Fi and OSC. An XBee radio can be used to build a wireless MIDI transceiver as a do-it-yourself project.\"[http://ladyada.net/make/xbee/midibee.html XBee Adapter – wireless Arduino programming] \". ladyada.net. n.p. 17 May 2011. Web. 20  Aug 2012. Android devices are able to function as full MIDI control surfaces using several different protocols over Wi-Fi and Bluetooth.\n\n3.5mm audio jack\n\nSome devices use standard TRS audio minijack connectors for MIDI data, including the Korg Electribe 2 and the Arturia Beatstep Pro. Both come with adaptors that break out to standard 5-pin DIN connectors.\n\nNew developments\n\nA new version of MIDI tentatively called \"HD Protocol\" or \"High-Definition Protocol\" has been under discussion since 2005, when it was announced as \"HD-MIDI\".Battino, David. \"[http://blogs.oreilly.com/digitalmedia/2005/10/finally-midi-20.html Finally: MIDI 2.0]\" O'Reilly Digital Media Blog. O'Reilly Media, Inc. 6 October 2005. Web. 22 August 2012 This new standard offers full backward compatibility with MIDI 1.0 and is intended to support higher-speed transports, allow plug-and-play device discovery and enumeration, and provide greater data range and resolution. The numbers of channels and controllers are to be increased and messages are to be simplified. Entirely new kinds of events will be supported, such as a Note Update message and Direct Pitch in the Note message which are aimed at guitar controllers.\"[http://www.midi.org/aboutus/news/hd.php MMA HD Protocol Announcement] \". midi.org. MIDI Manufacturers Association. n.d. Web. 22 August 2012\"[http://pro-music-news.com/html/01/e20105mm.htm General Meeting for MIDI developers by MMA] \". pro-music-news.com. Pro-Music-News. n.d. 22 August 2012 Proposed physical layer transports include Ethernet-based protocols such as RTP MIDI and Audio Video Bridging. The HD Protocol and a User Datagram Protocol (UDP)-based transport are under review by MMA's High-Definition Protocol Working Group (HDWG), which includes representatives from all sizes and types of companies.\n\nPrototype  devices based on the draft standard have been shown privately at NAMM using wired and wireless connections, however it is uncertain if and when the new protocol will be picked up by the industry. As of 2015, the HD Protocol specifications are nearing completion and MMA develops the policies on licensing and product certification.\n\nMIDI Polyphonic Expression\n\nMIDI Polyphonic Expression (MPE) is a method of using MIDI that enables pitch bend, and other dimensions of expressive control, to be adjusted continuously for individual notes. Instruments like the Continuum Fingerboard, Linnstrument, and Seaboard let users control pitch, timbre, and other nuances for individual notes within chords. A growing number of soft synths and effects are also compatible with MPE (such as Equator, UVI Falcon, and Sandman Pro), as well as a few hardware synths (such as Modal Electronics 002, Futuresonus Parva, and Modor NF-1). MPE works by assigning each note to its own MIDI channel so that particular messages can be applied to each note individually.",
  "entityProperties" : [ {
    "name" : "title",
    "type" : "String",
    "values" : [ "MIDI" ],
    "synthetic" : false
  }, {
    "name" : "url",
    "type" : "String",
    "values" : [ "http://en.wikipedia.org/?curid=19996" ],
    "synthetic" : false
  } ],
  "classifications" : [ "xml-export" ],
  "technicalAttributes" : {
    "technicalAttributes" : null,
    "aggregatedText" : "MIDI (; short for Musical Instrument Digital Interface) is a technical standard that describes a communications protocol, digital interface, and electrical connectors and allows a wide variety of electronic musical instruments, computers and other related music and audio devices to connect and communicate with one another. A single MIDI link can carry up to sixteen channels of information, each of which can be routed to a separate device.\n\nMIDI carries event messages that specify notation, pitch and velocity (loudness or softness), control signals for parameters such as volume, vibrato, audio panning from left to right, cues in theatre, and clock signals that set and synchronize tempo between multiple devices. These messages are sent via a MIDI cable to other devices where they control sound generation and other features. A simple example of a MIDI setup is the use of a MIDI controller such as an electronic musical keyboard to trigger sounds created by a sound module, which is in turn plugged into a keyboard amplifier. This MIDI data can also be recorded into a hardware or software device called a sequencer, which can be used to edit the data and to play it back at a later time. \n\nA file format to store and exchange the data has also been defined. Advantages of MIDI include small file size, ease of modification and manipulation and a wide choice of electronic instruments and synthesizer or digitally-sampled sounds. Prior to the development of MIDI, electronic musical instruments from different manufacturers could generally not communicate with each other. With MIDI, any MIDI-compatible keyboard (or other controller device) can be connected to any other MIDI-compatible sequencer, sound module, drum machine, synthesizer, or computer, even if they are made by different manufacturers.\n\nMIDI technology was standardized in 1983 by a panel of music industry representatives, and is maintained by the MIDI Manufacturers Association (MMA). All official MIDI standards are jointly developed and published by the MMA in Los Angeles, and the MIDI Committee of the Association of Musical Electronics Industry (AMEI) in Tokyo. In 2016, the MMA established The MIDI Association (TMA) to support a global community of people who work, play, or create with MIDI.\n\nHistory\n\nDevelopment\n\nIn 1980, Roland introduced the DIN sync interface to synchronize different electronic musical instruments. It was introduced with the Roland TR-808 in 1980, followed by other Roland equipment in 1981. In 1981 Roland introduced Digital Control Bus (DCB). DCB was the precursor to MIDI, which adopted most of its features from the DCB protocol and used the same type of connectors as DIN sync.\n\nIn June 1981, Roland founder Ikutaro Kakehashi proposed the idea of standardization to Oberheim Electronics founder Tom Oberheim, who then discussed it with Sequential Circuits president Dave Smith. In October 1981, Kakehashi, Oberheim and Smith discussed the idea with representatives from Yamaha, Korg and Kawai. They then discussed how to develop a common standard, working with Roland's pre-existing DCB as a basis. Sequential Circuits engineers and synthesizer designers, Dave Smith and Chet Wood, devised a universal synthesizer interface, which would allow direct communication between equipment from different manufacturers. Smith proposed this standard at the Audio Engineering Society show in November 1981. The standard was then discussed and modified by representatives of Roland, Yamaha, Korg, Kawai, and Sequential Circuits,Holmes, Thom. Electronic and Experimental Music: Pioneers in Technology and Composition. New York: Routledge, 2003 and was named Musical Instrument Digital Interface. MIDI's development was announced to the public by Robert Moog, in the October 1982 edition of Keyboard magazine.\n\nBy the time of the January 1983 Winter NAMM Show, Smith was able to demonstrate a MIDI connection between his Prophet 600 analog synthesizer with a Roland JP-6 synthesizer. The MIDI Specification was published in August 1983. The MIDI standard was unveiled by Ikutaro Kakehashi and Dave Smith, who both later received Technical Grammy Awards in 2013 for their key roles in the development of MIDI.\n\nThe first MIDI synthesizers were the Roland Jupiter-6 and the Prophet 600, both released in 1982. The first MIDI drum machine was the Roland TR-909, released in 1983.Butler, Mark Jonathan. \"Unlocking the Groove: Rhythm, Meter, and Musical Design in Electronic Dance Music\". Indiana University Press, 2006. . p. 64 The first MIDI music sequencer was the Roland MSQ-700, released in 1983. The first computers to support MIDI were the NEC PC-88 and PC-98 in 1982, and the MSX (Yamaha CX5M)Helen Casabona, David Frederick, [https://books.google.co.uk/books?id\n6K5Tpl_zBoEC&pgPA15 Advanced MIDI Applications, page 15] , Alfred Music released in 1983.Martin Russ, [https://books.google.co.uk/books?id\nX9h5AgAAQBAJ&pg=PA85 Sound Synthesis and Sampling, page 85] , CRC Press\n\nImpact on the music industry\n\nMIDI's appeal was originally limited to professional musicians and record producers who wanted to use electronic instruments in the production of popular music. The standard allowed different instruments to \"speak\" with each other and with computers, and this spurred a rapid expansion of the sales and production of electronic instruments and music software. This interoperability allowed one device to be controlled from another, which reduced the amount of hardware musicians needed to own. MIDI's introduction coincided with the dawn of the personal computer era and the introductions of samplers and digital synthesizers.Macan, Edward. Rocking the Classics: English Progressive Rock and the Counterculture. New York: Oxford University Press, 1997. p.191 The creative possibilities brought about by MIDI technology have been credited as having helped to revive the music industry in the 1980s.Shuker, Roy. Understanding Popular Music. London: Routledge, 1994. p.286\n\nMIDI introduced many capabilities which transformed the way musicians work. MIDI sequencing made it possible for a user with no notation skills to build complex arrangements.Demorest, Steven M. Building Choral Excellence: Teaching Sight-Singing in the Choral Rehearsal. New York: Oxford University Press, 2003. p. 17 A musical act with as few as one or two members, each operating multiple MIDI-enabled devices, could deliver a performance which sounds similar to that of a much larger group of musicians.Pertout, Andrian. [http://www.pertout.com/Midi.htm Mixdown Monthly] , #26. 26 June 1996. Web. 22 August 2012 The expense of hiring outside musicians for a project could be reduced or eliminated, and complex productions could be realized on a system as small as a synthesizer with integrated keyboard and sequencer. MIDI helped establish home recording. By performing preproduction in a home environment, an artist can reduce recording costs by arriving at a recording studio with a song that is already partially completed and worked out. Educational technology enabled by MIDI has transformed music education.Crawford, Renee. An Australian Perspective: Technology in Secondary School Music. Journal of Historical Research in Music Education. Vol. 30, No. 2. Apr 2009. Print.\n\nComparison with digital audio \n\nThose new to the subject of MIDI might confuse it with digital audio. While it may appear that MIDI and digital audio equipment do the same task, recording of multiple channels of music using digital equipment, this is done differently by MIDI and digital audio systems. MIDI symbolically represents a note. When the synth player presses a key on a keyboard, MIDI records which key was pressed, with which velocity and which duration, whereas digital audio represents the sound produced by the instrument.\n\nApplications\n\nInstrument control\n\nMIDI was invented so that electronic or digital musical instruments could communicate with each other and so that one instrument can control another. For example, a MIDI-compatible sequencer can trigger beats produced by a drum sound module. Analog synthesizers that have no digital component and were built prior to MIDI's development can be retrofit with kits that convert MIDI messages into analog control voltages. When a note is played on a MIDI instrument, it generates a digital signal that can be used to trigger a note on another instrument. The capability for remote control allows full-sized instruments to be replaced with smaller sound modules, and allows musicians to combine instruments to achieve a fuller sound, or to create combinations of synthesized instrument sounds, such as acoustic piano and strings.Lau, Paul. \"[http://www.highbeam.com/doc/1P3-1610624011.html Why Still MIDI?].\"  Canadian Musician. Norris-Whitney Communications Inc. 2008. HighBeam Research. 4 September 2012 MIDI also enables other instrument parameters (volume, effects, etc.) to be controlled remotely.\n\nSynthesizers and samplers contain various tools for shaping an electronic or digital sound. Filters adjust timbre (bass and treble), and envelopes automate the way a sound evolves over time after a note is triggered.Sasso, Len. \"[http://www.emusician.com/news/0766/sound-programming-101/145154 Sound Programming 101] \". Electronic Musician. NewBay Media. 1 October 2002. Web. 4 September 2012. The frequency of a filter and the envelope attack, or the time it takes for a sound to reach its maximum level, are examples of synthesizer parameters, and can be controlled remotely through MIDI. Effects devices have different parameters, such as delay feedback or reverb time. When a MIDI continuous controller number (CCN) is assigned to one of these parameters, the device will respond to any messages it receives that are identified by that number. Controls such as knobs, switches, and pedals can be used to send these messages. A set of adjusted parameters can be saved to a device's internal memory as a \"patch\", and these patches can be remotely selected by MIDI program changes. The MIDI standard allows selection of 128 different programs, but devices can provide more by arranging their patches into banks of 128 programs each, and combining a program change message with a bank select message.Anderton, Craig. \"[http://www.soundonsound.com/sos/1995_articles/may95/midiforguitarists.html MIDI For Guitarists: A Crash Course In MIDI Effects Control] \". Sound On Sound. SOS Publications. May 1995.\n\nComposition\n\nMIDI events can be sequenced with computer software, or in specialized hardware music workstations. Many digital audio workstations (DAWs) are specifically designed to work with MIDI as an integral component. MIDI piano rolls have been developed in many DAWs so that the recorded MIDI messages can be extensively modified. These tools allow composers to audition and edit their work much more quickly and efficiently than did older solutions, such as multitrack recording.\n\nBecause MIDI is a set of commands that create sound, MIDI sequences can be manipulated in ways that prerecorded audio cannot. It is possible to change the key, instrumentation or tempo of a MIDI arrangement, and to reorder its individual sections.Campbell, Drew. \"\"Click, Click. Audio\" Stage Directions. Vol. 16, No. 3. Mar 2003. The ability to compose ideas and quickly hear them played back enables composers to experiment.McCutchan, Ann. The Muse That Sings: Composers Speak about the Creative Process. New York: Oxford University Press, 1999. p. 67-68,72 Algorithmic composition programs provide computer-generated performances that can be used as song ideas or accompaniment.\n\nSome composers may take advantage of MIDI 1.0 and General MIDI (GM) technology to allow musical data files to be shared among various electronic instruments by using a standard, portable set of commands and parameters. The data composed via the sequenced MIDI recordings can be saved as a Standard MIDI File (SMF), digitally distributed, and reproduced by any computer or electronic instrument that also adheres to the same MIDI, GM, and SMF standards. MIDI data files are much smaller than recorded audio files.\n\nUse with computers\n\nAt the time of MIDI's introduction, the computing industry was mainly devoted to mainframe computers, and personal computers were not commonly owned. The personal computer market stabilized at the same time that MIDI appeared, and computers became a viable option for music production. It was not until the advent of MIDI in 1983 that general-purpose computers started to play a role in mainstream music production.\n\nIn the years immediately after the 1983 ratification of the MIDI specification, MIDI features were adapted to several early computer platforms. NEC's PC-88 and PC-98 began supporting MIDI as early as 1982. Yamaha modules introduced MIDI support and sequencing to the MSX in 1983.\n\nThe spread of MIDI on personal computers was largely facilitated by Roland Corporation's MPU-401, released in 1984, as the first MIDI-equipped PC sound card, capable of MIDI sound processing and sequencing.[http://www.piclist.com/techref/io/serial/midi/mpu.html Programming the MPU-401 in UART mode] [ftp://ftp.oldskool.org/pub/drivers/Roland/MPU-401%20technical%20reference%20manual.pdf MIDI PROCESSING UNIT MPU-401 TECHNICAL REFERENCE MANUAL], Roland Corporation After Roland sold MPU sound chips to other sound card manufacturers,[http://www.textfiles.com/music/midi-em.txt MIDI INTERFACES FOR THE IBM PC] , Electronic Musician, September 1990 it established a universal standard MIDI-to-PC interface.Peter Manning (2013), [https://books.google.co.uk/books?idryet1i-8OlYC Electronic and Computer Music] , page 319, Oxford University Press The widespread adoption of MIDI led to computer-based MIDI software being developed. Soon after, a number of platforms began supporting MIDI, including the Plus and IIe, Apple Macintosh, Commodore 64 and Amiga, Atari ST, Acorn Archimedes, and PC DOS. The Macintosh was a favorite among US musicians, as it was marketed at a competitive price, and it took several years for PC systems to catch up with its efficiency and graphical interface.\n\nThe Atari ST was favored in Europe, where Macintoshes were more expensive. The Apple IIGS used a digital sound chip designed for the Ensoniq Mirage synthesizer, and later models used a custom sound system and upgraded processors, which drove other companies to improve their own offerings. The Atari ST was favored for its MIDI ports that were built directly into the computer. Most music software in MIDI's first decade was published for either the Apple or the Atari. By the time of Windows 3.0's 1990 release, PCs had caught up in processing power and had acquired a graphical interface, and software titles began to see release on multiple platforms.\n\nStandard files\n\nThe Standard MIDI File (SMF) is a file format that provides a standardized way for music sequences to be saved, transported, and opened in other systems. The compact size of these files led to their widespread use in computers, mobile phone ringtones, webpage authoring and musical greeting cards. These files are intended for universal use, and include such information as note values, timing and track names. Lyrics may be included as metadata, and can be displayed by karaoke machines.Hass, Jeffrey. \"[http://www.indiana.edu/%7Eemusic/etext/MIDI/chapter3_MIDI10.shtml Chapter Three: How MIDI works 10] \". Indiana University Jacobs School of Music. 2010. Web 13 August 2012 The SMF specification was developed and is maintained by the MMA.\n\nSMFs are created as an export format of software sequencers or hardware workstations. They organize MIDI messages into one or more parallel tracks, and timestamp the events so that they can be played back in sequence. A header contains the arrangement's track count, tempo and which of three SMF formats the file is in.  A type 0 file contains the entire performance, merged onto a single track, while type 1 files may contain any number of tracks that are performed in synchrony. Type 2 files are rarely used\"[http://www.midi.org/aboutmidi/tut_midifiles.php MIDI Files] \". midi.org Music Manufacturers Association. n.d. Web. 27 August 2012 and store multiple arrangements, with each arrangement having its own track and intended to be played in sequence.\nMicrosoft Windows bundles SMFs together with Downloadable Sounds (DLS) in a Resource Interchange File Format (RIFF) wrapper, as RMID files with a .rmi extension. RIFF-RMID has been deprecated in favor of Extensible Music Files (XMF).\"[http://www.digitalpreservation.gov/formats/fdd/fdd000120.shtml RIFF-based MIDI File Format] \". digitalpreservation.gov. Library of Congress. 26 March 2012. Web. 18 August 2012\n\nFile sharing\n\nA MIDI file is not a recording of actual audio. Rather, it is a set of instructions (e.g., for pitch, rhythm and other elements), and can use a thousand times less disk space than the equivalent recorded audio.Crawford, Walt. \"MIDI and Wave: Coping with the Language\". Online. Vol. 20, No. 1. Jan/Feb 1996 This made MIDI file arrangements an attractive way to share music, before the advent of broadband internet access and multi-gigabyte hard drives. Licensed MIDI files on floppy disks were commonly available in stores in Europe and Japan during the 1990s.\"MIDI Assoc. pushes for new licensing agreement. (MIDI Manufacturers Association).\" Music Trades. Music Trades Corp. 1996. HighBeam Research. 4 September 2012  The major drawback to this is the wide variation in quality of users' audio cards, and in the actual audio contained as samples or synthesized sound in the card that the MIDI data only refers to symbolically. There is no standardization of how symbols are expressed. Even a sound card that contains high-quality sampled sounds can have inconsistent quality from one sampled instrument to another, while different model cards have no guarantee of consistent sound of the same instrument. Early budget-priced cards, such as the AdLib and the Sound Blaster and its compatibles, used a stripped-down version of Yamaha's frequency modulation synthesis (FM synthesis) technologyWiffen, Paul. \"[http://www.soundonsound.com/sos/1997_articles/sep97/synthschool3.html Synth School, Part 3: Digital Synthesis (FM, PD & VPM)] \". Sound on Sound Sep 1997. Print. played back through low-quality digital-to-analog converters. The low-fidelity reproduction of these ubiquitous cards was often assumed to somehow be a property of MIDI itself. This created a perception of MIDI as low-quality audio, while in reality MIDI itself contains no sound, and the quality of its playback depends entirely on the quality of the sound-producing device (and of samples in the device).\n\nSoftware\n\nThe main advantage of the personal computer in a MIDI system is that it can serve a number of different purposes, depending on the software that is loaded. Multitasking allows simultaneous operation of programs that may be able to share data with each other.\n\nSequencers\n\nSequencing software provides a number of benefits to a composer or arranger. It allows recorded MIDI to be manipulated using standard computer editing features such as cut, copy and paste and drag and drop. Keyboard shortcuts can be used to streamline workflow, and editing functions are often selectable via MIDI commands. The sequencer allows each channel to be set to play a different sound, and gives a graphical overview of the arrangement. A variety of editing tools are made available, including a notation display that can be used to create printed parts for musicians. Tools such as looping, quantization, randomization, and transposition simplify the arranging process.\n\nBeat creation is simplified, and groove templates can be used to duplicate another track's rhythmic feel. Realistic expression can be added through the manipulation of real-time controllers. Mixing can be performed, and MIDI can be synchronized with recorded audio and video tracks. Work can be saved, and transported between different computers or studios.Gellerman, Elizabeth. \"Audio Editing SW Is Music to Multimedia Developers' Ears\". Technical Horizons in Education  Journal. Vol. 22, No. 2. Sep 1994Desmond, Peter. \"ICT in the Secondary Music Curriculum\". Aspects of Teaching Secondary Music: Perspectives on Practice. ed. Gary Spruce. New York: RoutledgeFalmer, 2002\n\nSequencers may take alternate forms, such as drum pattern editors that allow users to create beats by clicking on pattern grids, and loop sequencers such as ACID Pro, which allow MIDI to be combined with prerecorded audio loops whose tempos and keys are matched to each other. Cue list sequencing is used to trigger dialogue, sound effect, and music cues in stage and broadcast production.\n\nNotation/scoring software\n\nWith MIDI, notes played on a keyboard can automatically be transcribed to sheet music.Holmes, Thom. Electronic and Experimental Music: Pioneers in Technology and Composition. New York: Routledge, 2003 Scorewriting software typically lacks advanced sequencing tools, and is optimized for the creation of a neat, professional printout designed for live instrumentalists. These programs provide support for dynamics and expression markings, chord and lyric display, and complex score styles. Software is available that can print scores in braille.Solomon, Karen. \"[https://www.wired.com/culture/lifestyle/news/2000/02/34495 You Gotta Feel the Music] \". wired.com. Condé Nast. 27 February 2000. Web. 13 August 2012.\n\nSmartScore software can produce MIDI files from scanned sheet music.Cook, Janet Harniman. \"[http://www.soundonsound.com/sos/dec98/articles/midiscan.265.htm Musitek Midiscan v2.51] \". Sound On Sound. SOS Publications. Dec 1998. Print. Other notation programs include Finale, Encore, Sibelius and MuseScore.\n\nEditor/librarians\n\nPatch editors allow users to program their equipment through the computer interface. These became essential with the appearance of complex synthesizers such as the Yamaha FS1R,Johnson, Derek. \"[http://www.soundonsound.com/sos/mar99/articles/yamahafs1r.htm Yamaha FS1R Editor Software] \". Sound on Sound. Mar 1999. which contained several thousand programmable parameters, but had an interface that consisted of fifteen tiny buttons, four knobs and a small LCD.Johnson, Derek, and Debbie Poyser. \"[http://www.soundonsound.com/sos/dec98/articles/yamfs1r.549.htm Yamaha FS1R] \". Sound on Sound. Dec 1998. Digital instruments typically discourage users from experimentation, due to their lack of the feedback and direct control that switches and knobs would provide, but patch editors give owners of hardware instruments and effects devices the same editing functionality that is available to users of software synthesizers.\"[http://www.squest.com/Products/MidiQuest11/index.html Sound Quest MIDI Quest 11 Universal Editor] \". squest.com. n.p. n.d. Web. 21 August 2012 Some editors are designed for a specific instrument or effects device, while other, \"universal\" editors support a variety of equipment, and ideally can control the parameters of every device in a setup through the use of System Exclusive commands.\n\nPatch librarians have the specialized function of organizing the sounds in a collection of equipment, and allow transmission of entire banks of sounds between an instrument and a computer. This allows the user to augment the device's limited patch storage with a computer's much greater disk capacity, and to share custom patches with other owners of the same instrument.\"[http://www.cakewalk.com/support/kb/reader.aspx/2007013074 Desktop Music Handbook – MIDI] \". cakewalk.com. Cakewalk, Inc. 26 November 2010. Web. Retrieved 7 August 2012. Universal editor/librarians that combine the two functions were once common, and included Opcode Systems' Galaxy and eMagic's SoundDiver. These programs have been largely abandoned with the trend toward computer-based synthesis, although Mark of the Unicorn's (MOTU)'s Unisyn and Sound Quest's Midi Quest remain available. Native Instruments' Kore was an effort to bring the editor/librarian concept into the age of software instruments.\n\nAuto-accompaniment programs\n\nPrograms that can dynamically generate accompaniment tracks are called \"auto-accompaniment\" programs. These create a full band arrangement in a style that the user selects, and send the result to a MIDI sound generating device for playback. The generated tracks can be used as educational or practice tools, as accompaniment for live performances, or as a songwriting aid.\n\nSynthesis and sampling\n\nComputers can use software to generate sounds, which are then passed through a digital-to-analog converter (DAC) to a power amplifier and loudspeaker system. The number of sounds that can be played simultaneously (the polyphony) is dependent on the power of the computer's CPU, as are the sample rate and bit depth of playback, which directly affect the quality of the sound.Lehrman, Paul D. \"[http://www.soundonsound.com/sos/1995_articles/oct95/softwaresynthesis.html Software Synthesis: The Wave Of The Future?] \" Sound On Sound. SOS Publications. Oct 1995. Print. Synthesizers implemented in software are subject to timing issues that are not present with hardware instruments, whose dedicated operating systems are not subject to interruption from background tasks as desktop operating systems are. These timing issues can cause synchronization problems, and clicks and pops when sample playback is interrupted. Software synthesizers also exhibit a noticeable delay known as latency in their sound generation, because computers use an audio buffer that delays playback and disrupts MIDI timing.Walker, Martin. \"[http://www.soundonsound.com/sos/mar01/articles/pcmusician.asp Identifying & Solving PC MIDI & Audio Timing Problems] \". Sound On Sound. SOS Publications. Mar 2001. Print.\n\nSoftware synthesis' roots go back as far as the 1950s, when Max Mathews of Bell Labs wrote the MUSIC-N programming language, which was capable of non-real-time sound generation.Miller, Dennis. \"[http://www.soundonsound.com/sos/1997_articles/may97/softwaresynth2.html Sound Synthesis On A Computer, Part 2] \". Sound On Sound. SOS Publications. May 1997. Print. The first synthesizer to run directly on a host computer's CPU\"[http://www.keyboardmag.com/article/Midi-Ancestors-and-Milestones/2171 MIDI Ancestors and Milestones] \". keyboardmag.com. New Bay Media. n.d. Web. 6 August 2012. was Reality, by Dave Smith's Seer Systems, which achieved a low latency through tight driver integration, and therefore could run only on Creative Labs soundcards.Walker, Martin. \"[http://www.soundonsound.com/sos/1997_articles/nov97/seerreality.html Reality PC] \". Sound On Sound. SOS Publications. Nov 1997. Print. Some systems use dedicated hardware to reduce the load on the host CPU, as with Symbolic Sound Corporation's Kyma System, and the Creamware/Sonic Core Pulsar/SCOPE systems,Wherry, Mark. \"[http://www.soundonsound.com/sos/jun03/articles/creamwarescope.asp Creamware SCOPE] \". Sound On Sound. SOS Publications. Jun 2003. Print. which power an entire recording studio's worth of instruments, effect units, and mixers.Anderton, Craig. \"[http://www.keyboardmag.com/article/sonic-core-scope-xite-1/147874 Sonic Core SCOPE Xite-1] \". keyboardmag.com. New Bay Media, LLC. n.d. Web.\n\nThe ability to construct full MIDI arrangements entirely in computer software allows a composer to render a finalized result directly as an audio file.\n\nGame music\n\nEarly PC games were distributed on floppy disks, and the small size of MIDI files made them a viable means of providing soundtracks. Games of the DOS and early Windows eras typically required compatibility with either Ad Lib or Sound Blaster audio cards. These cards used FM synthesis, which generates sound through modulation of sine waves. John Chowning, the technique's pioneer, theorized that the technology would be capable of accurate recreation of any sound if enough sine waves were used, but budget computer audio cards performed FM synthesis with only two sine waves. Combined with the cards' 8-bit audio, this resulted in a sound described as \"artificial\"David Nicholson. \"[http://www.highbeam.com/doc/1P2-946733.html HARDWARE].\"  The Washington Post. Washingtonpost Newsweek Interactive. 1993. HighBeam Research. 4 September 2012 and \"primitive\".Levy, David S. \"[http://www.highbeam.com/doc/1G1-14803399.html Aztech's WavePower daughtercard improves FM reception. (Aztech Labs Inc.'s wavetable synthesis add-on card for Sound Blaster 16 or Sound Galaxy Pro 16 sound cards) (Hardware Review) (Evaluation).] \" Computer Shopper. SX2 Media Labs LLC. 1994. HighBeam Research. 4 September 2012\n\nWavetable daughterboards that were later available provided audio samples that could be used in place of the FM sound. These were expensive, but often used the sounds from respected MIDI instruments such as the E-mu Proteus. The computer industry moved in the mid-1990s toward wavetable-based soundcards with 16-bit playback, but standardized on a 2MB ROM, a space too small in which to fit good-quality samples of 128 instruments plus drum kits. Some manufacturers used 12-bit samples, and padded those to 16 bits.Labriola, Don. \"[http://www.highbeam.com/doc/1G1-16232686.html MIDI masters: wavetable synthesis brings sonic realism to inexpensive sound cards. (review of eight Musical Instrument Digital Interface sound cards) (includes related articles about testing methodology, pitfalls of wavetable technology, future wavetable developments) (Hardware Review) (Evaluation).]\"  Computer Shopper. SX2 Media Labs LLC. 1994. HighBeam Research. 4 September 2012\n\nOther applications\n\nMIDI has been adopted as a control protocol in a number of non-musical applications. MIDI Show Control uses MIDI commands to direct stage lighting systems and to trigger cued events in theatrical productions. VJs and turntablists use it to cue clips, and to synchronize equipment, and recording systems use it for synchronization and automation. Apple Motion allows control of animation parameters through MIDI. The 1987 first-person shooter game MIDI Maze and the 1990 Atari ST computer puzzle game Oxyd used MIDI to network computers together, and kits are available that allow MIDI control over home lighting and appliances.\"[http://midikits.net23.net/midi_10_out/interface_circuits.htm Interface Circuits] \". MIDI Kits. n.p. 30 August 2012. Web. 30 August 2012.\n\nDespite its association with music devices, MIDI can control any electronic or digital device that can read and process a MIDI command. It is therefore possible to send a spacecraft from Earth to another destination in space, control home lighting, heating and air conditioning and even sequence traffic light signals all through MIDI commands. The receiving device or object would require a General MIDI processor, however in this instance, the program changes would trigger a function on that device rather than notes from a MIDI instrument's controller.  Each function can be set to a timer (also controlled by MIDI) or other condition or trigger determined by the device's creator.\n\nDevices\n\nConnectors\n\nThe cables terminate in a 180° five-pin DIN connector. Standard applications use only three of the five conductors: a ground wire, and a balanced pair of conductors that carry a +5 volt signal.Bozeman, William C. Educational Technology: Best Practices from America's Schools. Larchmont: Eye on Education, 1999.  This connector configuration can only carry messages in one direction, so a second cable is necessary for two-way communication. Some proprietary applications, such as phantom-powered footswitch controllers, use the spare pins for direct current (DC) power transmission.Lockwood, Dave. \"[http://www.soundonsound.com/sos/dec01/articles/tcgmajor.asp TC Electronic G Major] \". Sound On Sound. SOS Publications. Dec 2001. Print.\n\nOpto-isolators keep MIDI devices electrically separated from their connectors, which prevents the occurrence of ground loopsMornington-West, Allen. \"Digital Theory\". Sound Recording Practice. 4th Ed. Ed. John Borwick. Oxford: Oxford University Press, 1996. and protects equipment from voltage spikes. There is no error detection capability in MIDI, so the maximum cable length is set at 15 meters (50 feet) in order to limit interference.\"[http://www.richmondsounddesign.com/faq.html#midilen Richmond Sound Design – Frequently Asked Questions] \". richmondsounddesign.com. Web. 5 August 2012.\n\nMost devices do not copy messages from their input to their output port. A third type of port, the \"thru\" port, emits a copy of everything received at the input port, allowing data to be forwarded to another instrument in a \"daisy chain\" arrangement.Hass, Jeffrey. \"[http://www.indiana.edu/%7Eemusic/etext/MIDI/chapter3_MIDI2.shtml Chapter Three: How MIDI works 2] \". Indiana University Jacobs School of Music. 2010. Web. 13 August 2012. Not all devices contain thru ports, and devices that lack the ability to generate MIDI data, such as effects units and sound modules, may not include out ports.Gibbs, Jonathan (Rev. by Peter Howell) \"Electronic Music\". Sound Recording Practice, 4th Ed. Ed. John Borwick. Oxford: Oxford University Press, 1996\n\nManagement devices\n\nEach device in a daisy chain adds delay to the system. This is avoided with a MIDI thru box, which contains several outputs that provide an exact copy of the box's input signal. A MIDI merger is able to combine the input from multiple devices into a single stream, and allows multiple controllers to be connected to a single device. A MIDI switcher allows switching between multiple devices, and eliminates the need to physically repatch cables. MIDI patch bays combine all of these functions. They contain multiple inputs and outputs, and allow any combination of input channels to be routed to any combination of output channels. Routing setups can be created using computer software, stored in memory, and selected by MIDI program change commands. This enables the devices to function as standalone MIDI routers in situations where no computer is present. MIDI patch bays also clean up any skewing of MIDI data bits that occurs at the input stage.\n\nMIDI data processors are used for utility tasks and special effects. These include MIDI filters, which remove unwanted MIDI data from the stream, and MIDI delays, effects which send a repeated copy of the input data at a set time.\n\nInterfaces\n\nA computer MIDI interface's main function is to match clock speeds between the MIDI device and the computer. Some computer sound cards include a standard MIDI connector, whereas others connect by any of various means that include the D-subminiature DA-15 game port, USB, FireWire, Ethernet or a proprietary connection. The increasing use of USB connectors in the 2000s has led to the availability of MIDI-to-USB data interfaces that can transfer MIDI channels to USB-equipped computers. Some MIDI keyboard controllers are equipped with USB jacks, and can be plugged into computers that run music software.\n\nMIDI's serial transmission leads to timing problems. Experienced musicians can detect time differences of as small as 1/3 of a millisecond (ms) (which is how long it takes sound to travel 4 inches), and a three-byte MIDI message requires nearly 1ms for transmission.Robinson, Herbie. \"[http://lists.apple.com/archives/coreaudio-api/2005/Jul/msg00120.html Re: core midi time stamping] \". Apple Coreaudio-api Mailing List. Apple, Inc. 18 July 2005. 8 August 2012. Because MIDI is serial, it can only send one event at a time. If an event is sent on two channels at once, the event on the higher-numbered channel cannot transmit until the first one is finished, and so is delayed by 1ms. If an event is sent on all channels at the same time, the highest-numbered channel's transmission will be delayed by as much as 16ms. This contributed to the rise of MIDI interfaces with multiple in- and out-ports, because timing improves when events are spread between multiple ports as opposed to multiple channels on the same port. The term \"MIDI slop\" refers to audible timing errors that result when MIDI transmission is delayed.Shirak, Rob. \"[http://www.emusician.com/news/0766/mark-of-the-unicorn/140335 Mark of the Unicorn] \". emusician.com. New Bay Media. 1 October 2000. Web. Retrieved 8 August 2012.\n\nControllers\n\nThere are two types of MIDI controllers: performance controllers that generate notes and are used to perform music,\"[http://www.rolandmusiced.com/spotlight/article.php?ArticleId=1040 MIDI Performance Instruments] \". Instruments of Change. Vol. 3, No. 1 (Winter 1999). Roland Corporation, U.S. and controllers which may not send notes, but transmit other types of real-time events. Many devices are some combination of the two types.\n\nKeyboards are by far the most common type of MIDI controller. MIDI was designed with keyboards in mind, and any controller that is not a keyboard is considered an \"alternative\" controller.\"[http://www.midi.org/aboutmidi/products.php MIDI Products] \". midi.org. MIDI Manufacturers Association. n.d. 1 August 1012 This was seen as a limitation by composers who were not interested in keyboard-based music, but the standard proved flexible, and MIDI compatibility was introduced to other types of controllers, including guitars, stringed and wind instruments, drums and specialized and experimental controllers. Other controllers include drum controllers and wind controllers, which can emulate the playing of drum kit and wind instruments, respectively.\n\nSoftware synthesizers offer great power and versatility, but some players feel that division of attention between a MIDI keyboard and a computer keyboard and mouse robs some of the immediacy from the playing experience.Preve, Francis. \"Dave Smith\", in \"The 1st Annual Keyboard Hall of Fame\". Keyboard (US). NewBay Media, LLC. Sep 2012. Print. p.18 Devices dedicated to real-time MIDI control provide an ergonomic benefit, and can provide a greater sense of connection with the instrument than an interface that is accessed through a mouse or a push-button digital menu. Controllers may be general-purpose devices that are designed to work with a variety of equipment, or they may be designed to work with a specific piece of software. Examples of the latter include Akai's APC40 controller for Ableton Live, and Korg's MS-20ic controller that is a reproduction of their MS-20 analog synthesizer. The MS-20ic controller includes patch cables that can be used to control signal routing in their virtual reproduction of the MS-20 synthesizer, and can also control third-party devices.\"[http://www.vintagesynth.com/korg/legacy.php Korg Legacy Collection] \". vintagesynth.com. Vintage Synth Explorer. n.d. Web. 21 August 2012\n\nInstruments\n\nA MIDI instrument contains ports to send and receive MIDI signals, a CPU to process those signals, an interface that allows user programming, audio circuitry to generate sound, and controllers. The operating system and factory sounds are often stored in a Read-only memory (ROM) unit.\n\nA MIDI instrument can also be a stand-alone module (without a piano style keyboard) consisting of a General MIDI soundboard (GM, GS and/XG), onboard editing, including transposing/pitch changes, MIDI instrument changes and adjusting volume, pan, reverb levels and other MIDI controllers. Typically, the MIDI Module will include a large screen, enabling the user to view information depending on the function selected at that time. Features can include scrolling lyrics, usually embedded in a MIDI file or karaoke MIDI, playlists, song library and editing screens. Some MIDI Modules include a Harmonizer and the ability to playback and transpose MP3 audio files.\n\nSynthesizers\n\nSynthesizers may employ any of a variety of sound generation techniques. They may include an integrated keyboard, or may exist as \"sound modules\" or \"expanders\" that generate sounds when triggered by an external controller, such as a MIDI keyboard. Sound modules are typically designed to be mounted in a 19-inch rack. Manufacturers commonly produce a synthesizer in both standalone and rack-mounted versions, and often offer the keyboard version in a variety of sizes.\n\nSamplers\n\nA sampler can record and digitize audio, store it in random-access memory (RAM), and play it back. Samplers typically allow a user to edit a sample and save it to a hard disk, apply effects to it, and shape it with the same tools that synthesizers use. They also may be available in either keyboard or rack-mounted form. Instruments that generate sounds through sample playback, but have no recording capabilities, are known as \"ROMplers\".\n\nSamplers did not become established as viable MIDI instruments as quickly as synthesizers did, due to the expense of memory and processing power at the time. The first low-cost MIDI sampler was the Ensoniq Mirage, introduced in 1984. MIDI samplers are typically limited by displays that are too small to use to edit sampled waveforms, although some can be connected to a computer monitor.\n\nDrum machines\n\nDrum machines typically are sample playback devices that specialize in drum and percussion sounds. They commonly contain a sequencer that allows the creation of drum patterns, and allows them to be arranged into a song. There often are multiple audio outputs, so that each sound or group of sounds can be routed to a separate output. The individual drum voices may be playable from another MIDI instrument, or from a sequencer.\n\nWorkstations and hardware sequencers\n\nSequencer technology predates MIDI. Analog sequencers use CV/Gate signals to control pre-MIDI analog synthesizers. MIDI sequencers typically are operated by transport features modeled after those of  tape decks. They are capable of recording MIDI performances, and arranging them into individual tracks along a multitrack recording concept. Music workstations combine controller keyboards with an internal sound generator and a sequencer. These can be used to build complete arrangements and play them back using their own internal sounds, and function as self-contained music production studios. They commonly include file storage and transfer capabilities.\n\nEffects devices\n\nAudio effects units that are frequently used in stage and recording, such as reverbs, delays and choruses, can be remotely adjusted via MIDI signals. Some units allow only a limited number of parameters to be controlled this way, but most will respond to program change messages. The Eventide H3000 Ultra-harmonizer is an example of a unit that allows such extensive MIDI control that it is playable as a synthesizer.Manning, Peter. Electronic and Computer Music. 1985. Oxford: Oxford University Press, 1994. Print.\n\nTechnical specifications\n\nMIDI messages are made up of 8-bit words (commonly called bytes) that are transmitted serially at a rate of 31.25 kbit/s. This rate was chosen because it is an exact division of 1 MHz, the speed at which many early microprocessors operated. The first bit of each word identifies whether the word is a status byte or a data byte, and is followed by seven bits of information. A start bit and a stop bit are added to each byte for framing purposes, so a MIDI byte requires ten bits for transmission.\n\nA MIDI link can carry sixteen independent channels of information. The channels are numbered 1–16, but their actual corresponding binary encoding is 0–15. A device can be configured to only listen to specific channels and to ignore the messages sent on other channels (\"Omni Off\" mode), or it can listen to all channels, effectively ignoring the channel address (\"Omni On\"). An individual device may be monophonic (the start of a new \"note-on\" MIDI command implies the termination of the previous note), or polyphonic (multiple notes may be sounding at once, until the polyphony limit of the instrument is reached, or the notes reach the end of their decay envelope, or explicit \"note-off\" MIDI commands are received). Receiving devices can typically be set to all four combinations of \"omni off/on\" versus \"mono/poly\" modes.\n\nMessages\n\nA MIDI message is an instruction that controls some aspect of the receiving device. A MIDI message consists of a status byte, which indicates the type of the message, followed by up to two data bytes that contain the parameters.Brewster, Stephen. \"Nonspeech Auditory Output\". The Human-Computer Interaction Handbook: Fundamentals, Evolving Technologies, and Emerging Applications. Ed. Julie A. Jacko; Andrew Sears. Mahwah: Lawrence Erlbaum Associates, 2003. p.227 MIDI messages can be \"channel messages\", which are sent on only one of the 16 channels and can be heard only by devices receiving on that channel, or \"system messages\", which are heard by all devices. Any data not relevant to a receiving device is ignored. There are five types of message: Channel Voice, Channel Mode, System Common, System Real-Time, and System Exclusive.Hass, Jeffrey. \"[http://www.indiana.edu/%7Eemusic/etext/MIDI/chapter3_MIDI3.shtml Chapter Three: How MIDI works 3] \". Indiana University Jacobs School of Music. 2010. Web. 13 August 2012.\n\nChannel Voice messages transmit real-time performance data over a single channel. Examples include \"note-on\" messages which contain a MIDI note number that specifies the note's pitch, a velocity value that indicates how forcefully the note was played, and the channel number; \"note-off\" messages that end a note; program change messages that change a device's patch; and control changes that allow adjustment of an instrument's parameters. Channel Mode messages include the Omni/mono/poly mode on and off messages, as well as messages to reset all controllers to their default state or to send \"note-off\" messages for all notes. System messages do not include channel numbers, and are received by every device in the MIDI chain. MIDI time code is an example of a System Common message. System Real-Time messages provide for synchronization, and include MIDI clock and Active Sensing.\n\nSystem Exclusive messages \n\nSystem Exclusive (SysEx) messages are a major reason for the flexibility and longevity of the MIDI standard. They allow manufacturers to create proprietary messages which provide control over their equipment in a way that is more thorough than is provided for by standard MIDI messages. SysEx messages are addressed to a specific device in a system. Each manufacturer has a unique identifier that is included in its SysEx messages, which helps ensure that the messages will be heard only by the targeted device, and ignored by all others. Many instruments also include a SysEx ID setting, which allows two devices of the same model to be addressed independently while connected to the same system.Hass, Jeffrey. \"[http://www.indiana.edu/%7Eemusic/etext/MIDI/chapter3_MIDI9.shtml Chapter Three: How MIDI works 9] \". Indiana University Jacobs School of Music. 2010. Web. 13 August 2012. SysEx messages may include functionality beyond what the MIDI standard provides. They are targeted at a specific instrument, and are ignored by all other devices on the system.\n\nImplementation chart\n\nDevices typically do not respond to every type of message defined by the MIDI specification. The MIDI implementation chart was standardized by the MMA as a way for users to see what specific capabilities an instrument has, and how it responds to messages. A specific MIDI Implementation Chart is usually published for each MIDI device within the device documentation.\n\nElectrical specifications\n\nThe MIDI specification for the electrical interface is based on a fully isolated current loop. The MIDI out port nominally sources a +5 volt source through a 220 ohm resistor out through pin 4 on the MIDI out DIN connector, in on pin 4 of the receiving device's MIDI in DIN connector, through a 220 ohm protection resistor and the LED of an opto-isolator.  The current then returns via pin 5 on the MIDI in port to the originating device's MIDI out port pin 5, again with a 220 ohm resistor in the path, giving a nominal current of about 5 milliamperes.   Despite the cable's appearance, there is no conductive path between the two MIDI devices, only an optically isolated one. Properly designed MIDI devices are relatively immune to ground loops and similar interference.  The data rate on this system is 31,250 bits per second, logic 0 being current on.\n\nThe MIDI specification provides for a ground \"wire\" and a braid or foil shield, connected on pin 2, protecting the two signal-carrying conductors on pins 4 and 5.  Although the MIDI cable is supposed to connect pin 2 and the braid or foil shield to chassis ground, it should do so only at the MIDI out port; the MIDI in port should leave pin 2 unconnected and isolated. Some large manufacturers of MIDI devices use modified MIDI in-only DIN 5-pin sockets with the metallic conductors intentionally omitted at pin positions 1, 2, and 3 so that the maximum voltage isolation is obtained.\n\nExtensions\n\nMIDI's flexibility and widespread adoption have led to many refinements of the standard, and have enabled its application to purposes beyond those for which it was originally intended.\n\nGeneral MIDI\n\nMIDI allows selection of an instrument's sounds through program change messages, but there is no guarantee that any two instruments have the same sound at a given program location.Bello, Juan P. \"[http://www.nyu.edu/classes/bello/FMT_files/10_MIDI_soundcontrol.pdf MIDI: sound control] \". nyu.edu. New York University. n.d. Web. 18 August 2012 Program #0 may be a piano on one instrument, or a flute on another. The General MIDI (GM) standard was established in 1991, and provides a standardized sound bank that allows a Standard MIDI File created on one device to sound similar when played back on another. GM specifies a bank of 128 sounds arranged into 16 families of eight related instruments, and assigns a specific program number to each instrument. Percussion instruments are placed on channel 10, and a specific MIDI note value is mapped to each percussion sound. GM-compliant devices must offer 24-note polyphony. Any given program change will select the same instrument sound on any GM-compatible instrument.\"[http://academic.pgcc.edu/~njudy/mt/MIDI/gm.html General MIDI Standard] \". pgcc.edu. Prince George's Community College. n.d. Web.\n\nThe GM standard eliminates variation in note mapping. Some manufacturers had disagreed over what note number should represent middle C, but GM specifies that note number 69 plays A440, which in turn fixes middle C as note number 60. GM-compatible devices are required to respond to velocity, aftertouch, and pitch bend, to be set to specified default values at startup, and to support certain controller numbers such as for sustain pedal, and Registered Parameter Numbers.Glatt, Jeff. \"[http://home.roadrunner.com/~jgglatt/tutr/gm.htm General MIDI] \". The MIDI Technical Fanatic's Brainwashing Center. n.p. n.d. Web. 17 August 2012 A simplified version of GM, called \"GM Lite\", is used in mobile phones and other devices with limited processing power.\n\nGS, XG, and GM2\n\nA general opinion quickly formed that the GM's 128-instrument sound set was not large enough. Roland's General Standard, or GS, system included additional sounds, drumkits and effects, provided a \"bank select\" command that could be used to access them, and used MIDI Non-Registered Parameter Numbers (NRPNs) to access its new features. Yamaha's Extended General MIDI, or XG, followed in 1994. XG similarly offered extra sounds, drumkits and effects, but used standard controllers instead of NRPNs for editing, and increased polyphony to 32 voices. Both standards feature backward compatibility with the GM specification, but are not compatible with each other.Nagle, Paul. \"[http://www.soundonsound.com/sos/1995_articles/sep95/yamahamu50.html Yamaha MU50 & Yamaha CBX-K1] \". Sound On Sound. SOS Publications. Sep 1995. Print. Neither standard has been adopted beyond its creator, but both are commonly supported by music software titles.\n\nMember companies of Japan's AMEI developed the General MIDI Level 2 specification in 1999. GM2 maintains backward compatibility with GM, but increases polyphony to 32 voices, standardizes several controller numbers such as for sostenuto and soft pedal (una corda), RPNs and Universal System Exclusive Messages, and incorporates the MIDI Tuning Standard.\"[http://www.midi.org/techspecs/gm.php About General MIDI] \". midi.org. MIDI Manufacturers Association. n.d. Web. 17 August 2012 GM2 is the basis of the instrument selection mechanism in Scalable Polyphony MIDI (SP-MIDI), a MIDI variant for low power devices that allows the device's polyphony to scale according to its processing power.\n\nTuning standard\n\nMost MIDI synthesizers use equal temperament tuning. The MIDI tuning standard (MTS), ratified in 1992, allows alternate tunings.\"[http://www.microtonal-synthesis.com/MIDItuning.html The MIDI Tuning Standard] \". microtonal-synthesis.com. n.p. n.d. Web. 17 August 2012 MTS allows microtunings that can be loaded from a bank of up to 128 patches, and allows real-time adjustment of note pitches. Manufacturers are not required to support the standard. Those who do are not required to implement all of its features.\n\nTime code\n\nA sequencer can drive a MIDI system with its internal clock, but when a system contains multiple sequencers, they must synchronize to a common clock. MIDI Time Code (MTC), developed by Digidesign,Glatt, Jeff. \"[http://home.roadrunner.com/~jgglatt/tutr/history.htm The beginnings of MIDI] \". The MIDI Technical Fanatic's Brainwashing Center. n.p. n.d. Web. 13 August 2012. implements SysEx messagesGlatt, Jeff. \"[http://home.roadrunner.com/~jgglatt/tech/mtc.htm MIDI Time Code] \". The MIDI Technical Fanatic's Brainwashing Center. n.p. n.d. Web. 13 August 2012. that have been developed specifically for timing purposes, and is able to translate to and from the SMPTE time code standard. MIDI Clock is based on tempo, but SMPTE time code is based on frames per second, and is independent of tempo. MTC, like SMPTE code, includes position information, and can adjust itself if a timing pulse is lost.White, Paul. \"[http://www.soundonsound.com/sos/1996_articles/jun96/miditimecode.html SMPTE & MTC (MIDI Time Code)] \" Sound On Sound. SOS Publications. Jun 1996. Print. MIDI interfaces such as Mark of the Unicorn's MIDI Timepiece can convert SMPTE code to MTC.\n\nMachine control\n\nMIDI Machine Control (MMC) consists of a set of SysEx commandsGlatt, Jeff. \"[http://home.roadrunner.com/~jgglatt/tech/mmc.htm MIDI Machine Control (MMC)] \". The MIDI Technical Fanatic's Brainwashing Center. n.p. n.d. Web. that operate the transport controls of hardware recording devices. MMC allows a sequencer to send \"Start\", \"Stop\", and \"Record\" commands to a connected tape deck or hard disk recording system, and to fast-forward or rewind the device so that it starts playback at the same point as the sequencer. No synchronization data is involved, although the devices may synchronize through MTC.\"[http://www.sweetwater.com/expert-center/glossary/t--MMC Glossary: MIDI Machine Control (MMC)] \". sweetwater.com. Sweetwater Sound. n.d. Web. 15 August 2012.\n\nShow control\n\nMIDI Show Control (MSC) is a set of SysEx commands which allows sequencing and remote cueing of show control devices such as lighting, music and sound playback, and motion control systems.\"[http://people.virginia.edu/~rlk3p/desource/TechNotes/MSC.html An Inexpensive MIDI show-control System] \". Lighting TechNotes. The University of Virginia. 25 October 2004. Web. 17 August 2012. Applications include stage productions, museum exhibits, recording studio control systems, and amusement park attractions.\n\nTimestamping\n\nOne solution to MIDI timing problems is to mark MIDI events with the times they are to be played, and store them in a buffer in the MIDI interface ahead of time. Sending data beforehand reduces the likelihood that a busy passage will send a large amount of information that will overwhelm the transmission link. Once stored in the interface, the information is no longer subject to timing issues associated with USB jitter and computer operating system interrupts, and can be transmitted with a high degree of accuracy.\"[http://www.sweetwater.com/expert-center/glossary/t--MTS-MOTU Glossary: MTS (MIDI Time Stamping)] \". sweetwater.com. Sweetwater Sound. n.d. Web. 17 August 2012 MIDI timestamping only works when both the hardware and software support it. MOTU's MTS, eMagic's AMT, and Steinberg's Midex 8 were implementations that were incompatible with each other, and required users to own software and hardware manufactured by the same company in order to gain its benefits. Timestamping is built into FireWire MIDI interfaces,Walker, Martin. \"[http://www.soundonsound.com/sos/Oct02/articles/pcmusician1002.asp The Truth About Latency: Part 2] \". Sound On Sound. SOS Publications. Oct 2002. Print. Mac OS X Core Audio, and Linux ALSA Sequencer.\n\nSample dump standard\n\nAn unforeseen capability of SysEx messages was their use for transporting audio samples between instruments. This led to the development of the sample dump standard (SDS), which established a new SysEx format for sample transmission. The SDS was later augmented with a pair of commands that allow the transmission of information about sample loop points, without requiring that the entire sample be transmitted.Glatt, Jeff. [https://web.archive.org/web/20111115234241/http://home.roadrunner.com/~jgglatt/tech/sds.htm]. The MIDI Technical Fanatic's Brainwashing Center. n.p. n.d. Web. 13 August 2012.\n\nDownloadable sounds\n\nThe Downloadable Sounds (DLS) specification, ratified in 1997, allows mobile devices and computer sound cards to expand their wave tables with downloadable sound sets. The DLS Level 2 Specification followed in 2006, and defined a standardized synthesizer architecture. The Mobile DLS standard calls for DLS banks to be combined with SP-MIDI, as self-contained Mobile XMF files.\n\nAlternative hardware transports\n\nIn addition to the original 31.25 kbit/s current-loop transported on 5-pin DIN, other connectors have been used for the same electrical data, and transmission of MIDI streams in different forms over USB, IEEE 1394 a.k.a. FireWire, and Ethernet is now common. Some samplers and hard drive recorders can also pass MIDI data between each other over SCSI.\n\nUSB and FireWire\n\nMembers of the USB-IF in 1999 developed a standard for MIDI over USB, the \"Universal Serial Bus Device Class Definition for MIDI Devices\"Ashour, Gal, et al. . usb.org [http://www.usb.org/developers/docs/devclass_docs/midi10.pdf USB Implementers Forum] . 1 November 1999. Web. 22 August 2012 MIDI over USB has become increasingly common as other interfaces that had been used for MIDI connections (serial, joystick, etc.) disappeared from personal computers. Linux, Microsoft Windows, Macintosh OS X, and Apple iOS operating systems include standard class drivers to support devices that use the \"Universal Serial Bus Device Class Definition for MIDI Devices\". Some manufacturers choose to implement a MIDI interface over USB that is designed to operate differently from the class specification, using custom drivers.\n\nApple Computer developed the FireWire interface during the 1990s. It began to appear on digital video cameras toward the end of the decade, and on G3 Macintosh models in 1999.Wiffen, Paul. \"[http://www.soundonsound.com/sos/aug00/articles/mlan.htm An Introduction To mLAN, Part 1] \". Sound On Sound. SOS Publications. Aug 2000. Print. It was created for use with multimedia applications.  Unlike USB, FireWire uses intelligent controllers that can manage their own transmission without attention from the main CPU.Wiffen, Paul. \"[http://www.soundonsound.com/sos/sep00/articles/mlan.htm An Introduction To mLAN, Part 2] \". Sound On Sound. SOS Publications. Sep 2000. Print. As with standard MIDI devices, FireWire devices can communicate with each other with no computer present.\n\nXLR connectors\n\nThe Octave-Plateau Voyetra-8 synthesizer was an early MIDI implementation using XLR3 connectors in place of the 5-pin DIN. It was released in the pre-MIDI years and later retrofitted with a MIDI interface but keeping its XLR connector.\n\nSerial parallel, and joystick port \n\nAs computer-based studio setups became common, MIDI devices that could connect directly to a computer became available. These typically used the 8-pin mini-DIN connector that was used by Apple for serial and printer ports prior to the introduction of the Blue & White G3 models. MIDI interfaces intended for use as the centerpiece of a studio, such as the Mark of the Unicorn MIDI Time Piece, were made possible by a \"fast\" transmission mode that could take advantage of these serial ports' ability to operate at 20 times the standard MIDI speed.\"[http://www.midi.org/aboutmidi/tut_midicables.php MIDI Cables & Transports] \". midi.org. Music Manufacturers Association. n.d. Web. 27 August 2012. Mini-DIN ports were built into some late-1990s MIDI instruments, and enabled such devices to be connected directly to a computer.\"CS2x Control Synthesizer Owner's Manual\". Yamaha Corporation, 1998. Some devices connected via PCs' DB-25 parallel port, or through the joystick port found in many PC sound cards.\n\nmLAN\n\nYamaha introduced the mLAN protocol in 1999. It was conceived as a Local Area Network for musical instruments using FireWire as the transport, and was designed to carry multiple MIDI channels together with multichannel digital audio, data file transfers, and time code. mLan was used in a number of Yamaha products, notably digital mixing consoles and the Motif synthesizer, and in third-party products such as the PreSonus FIREstation and the Korg Triton Studio. No new mLan products have been released since 2007.\n\nEthernet\n\nThe computer network implementation of MIDI provides network routing capabilities, and provides the high-bandwidth channel that earlier alternatives to MIDI, such as ZIPI, were intended to bring. Proprietary implementations have existed since the 1980s, some of which use fiber optic cables for transmission. The Internet Engineering Task Force's RTP MIDI open specification is gaining industry support, as proprietary MIDI/IP protocols require expensive licensing fees, or provide no advantage, apart from speed, over the original MIDI protocol. Apple has supported this protocol from Mac OS X 10.4 onwards, and a Windows driver based on Apple's implementation exists for Windows XP and newer versions.\"rtpMIDI\". tobias-erichsen.de. n.p. n.d. Web. 22 August 2012 [http://www.tobias-erichsen.de/software/rtpmidi.html Windows RTP-MIDI driver download] \n\nWireless\n\nSystems for wireless MIDI transmission have been available since the 1980s. Several commercially available transmitters allow wireless transmission of MIDI and OSC signals over Wi-Fi and Bluetooth.Kirn, Peter. \"[http://createdigitalmusic.com/2011/03/golden-age-of-wireless-korg-ios-sync-android-midi-hardware-enter-bluetooth-midi/ Golden Age of Wireless: Korg iOS Sync, Android + MIDI Hardware, Enter Bluetooth MIDI?] \". createdigitalmusic.com. n.p. 25 March 2011. Web. iOS devices are able to function as MIDI control surfaces, using Wi-Fi and OSC. An XBee radio can be used to build a wireless MIDI transceiver as a do-it-yourself project.\"[http://ladyada.net/make/xbee/midibee.html XBee Adapter – wireless Arduino programming] \". ladyada.net. n.p. 17 May 2011. Web. 20  Aug 2012. Android devices are able to function as full MIDI control surfaces using several different protocols over Wi-Fi and Bluetooth.\n\n3.5mm audio jack\n\nSome devices use standard TRS audio minijack connectors for MIDI data, including the Korg Electribe 2 and the Arturia Beatstep Pro. Both come with adaptors that break out to standard 5-pin DIN connectors.\n\nNew developments\n\nA new version of MIDI tentatively called \"HD Protocol\" or \"High-Definition Protocol\" has been under discussion since 2005, when it was announced as \"HD-MIDI\".Battino, David. \"[http://blogs.oreilly.com/digitalmedia/2005/10/finally-midi-20.html Finally: MIDI 2.0]\" O'Reilly Digital Media Blog. O'Reilly Media, Inc. 6 October 2005. Web. 22 August 2012 This new standard offers full backward compatibility with MIDI 1.0 and is intended to support higher-speed transports, allow plug-and-play device discovery and enumeration, and provide greater data range and resolution. The numbers of channels and controllers are to be increased and messages are to be simplified. Entirely new kinds of events will be supported, such as a Note Update message and Direct Pitch in the Note message which are aimed at guitar controllers.\"[http://www.midi.org/aboutus/news/hd.php MMA HD Protocol Announcement] \". midi.org. MIDI Manufacturers Association. n.d. Web. 22 August 2012\"[http://pro-music-news.com/html/01/e20105mm.htm General Meeting for MIDI developers by MMA] \". pro-music-news.com. Pro-Music-News. n.d. 22 August 2012 Proposed physical layer transports include Ethernet-based protocols such as RTP MIDI and Audio Video Bridging. The HD Protocol and a User Datagram Protocol (UDP)-based transport are under review by MMA's High-Definition Protocol Working Group (HDWG), which includes representatives from all sizes and types of companies.\n\nPrototype  devices based on the draft standard have been shown privately at NAMM using wired and wireless connections, however it is uncertain if and when the new protocol will be picked up by the industry. As of 2015, the HD Protocol specifications are nearing completion and MMA develops the policies on licensing and product certification.\n\nMIDI Polyphonic Expression\n\nMIDI Polyphonic Expression (MPE) is a method of using MIDI that enables pitch bend, and other dimensions of expressive control, to be adjusted continuously for individual notes. Instruments like the Continuum Fingerboard, Linnstrument, and Seaboard let users control pitch, timbre, and other nuances for individual notes within chords. A growing number of soft synths and effects are also compatible with MPE (such as Equator, UVI Falcon, and Sandman Pro), as well as a few hardware synths (such as Modal Electronics 002, Futuresonus Parva, and Modor NF-1). MPE works by assigning each note to its own MIDI channel so that particular messages can be applied to each note individually. MIDI. http://en.wikipedia.org/?curid=19996."
  }
}
