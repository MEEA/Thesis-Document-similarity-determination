{
  "datasourceIdentifier" : "awesome wiki export",
  "backlink" : "http://en.wikipedia.org/?curid=4890",
  "eid" : "d11fa080-52b1-11e8-ad1f-273b2f3b71fa",
  "loadTime" : 1525778430600,
  "textBody" : "Bayesian probability is an interpretation of the concept of probability, in which, instead of frequency or propensity of some phenomenon, probability is interpreted as reasonable expectation representing a state of knowledgeJaynes, E.T. \"Bayesian Methods: General Background.\" In Maximum-Entropy and Bayesian Methods in Applied Statistics, by J. H. Justice (ed.). Cambridge: Cambridge Univ. Press, 1986 or as quantification of a personal belief.\n\nThe Bayesian interpretation of probability can be seen as an extension of propositional logic that enables reasoning with hypotheses, i.e., the propositions whose truth or falsity is uncertain. In the Bayesian view, a probability is assigned to a hypothesis, whereas under frequentist inference, a hypothesis is typically tested without being assigned a probability.\n\nBayesian probability belongs to the category of evidential probabilities; to evaluate the probability of a hypothesis, the Bayesian probabilist specifies some prior probability, which is then updated to a posterior probability in the light of new, relevant data (evidence).Paulos, John Allen. [https://www.nytimes.com/2011/08/07/books/review/the-theory-that-would-not-die-by-sharon-bertsch-mcgrayne-book-review.html?_r\n1&scp1&sq\nthomas%20bayes&st=cse The Mathematics of Changing Your Mind,] New York Times (US). August 5, 2011; retrieved 2011-08-06 The Bayesian interpretation provides a standard set of procedures and formulae to perform this calculation.\n\nThe term Bayesian derives from the 18th century mathematician and theologian Thomas Bayes, who provided the first mathematical treatment of a non-trivial problem of Bayesian inference.Stigler, Stephen M. (1986) The history of statistics. Harvard University Press. pg 131. Mathematician Pierre-Simon Laplace pioneered and popularised what is now called Bayesian probability.Stigler, Stephen M. (1986) The history of statistics., Harvard University press. pp. 97–98, 131.\n\nBroadly speaking, there are two views on Bayesian probability that interpret the probability concept in different ways. According to the objectivist view, probability is a reasonable expectation that represents the state of knowledge, can be interpreted as an extension of logic, and its rules can be justified by Cox's theorem.Cox, Richard T. Algebra of Probable Inference, The Johns Hopkins University Press, 2001 According to the subjectivist view, probability quantifies a personal belief, and its rules can be justified by requirements of rationality and coherence following from the Dutch book argument or from the decision theory and de Finetti's theorem.de Finetti, B. (1974) Theory of probability (2 vols.), J. Wiley & Sons, Inc., New York\n\nBayesian methodology\n\nBayesian methods are characterized by concepts and procedures as follows:\n* The use of random variables, or more generally unknown quantities, to model all sources of uncertainty in statistical models including uncertainty resulting from lack of information (see also aleatoric and epistemic uncertainty).\n* The need to determine the prior probability distribution taking into account the available (prior) information.\n* The sequential use of Bayes' formula: when more data become available, calculate the posterior distribution using Bayes' formula; subsequently, the posterior distribution becomes the next prior.\n* While for the frequentist a hypothesis is a proposition (which must be either true or false), so that the frequentist probability of a hypothesis is either 0 or 1, in Bayesian statistics the probability that can be assigned to a hypothesis can also be in a range from 0 to 1 if the truth value is uncertain.\n\nObjective and subjective Bayesian probabilities\n\nBroadly speaking, there are two interpretations on Bayesian probability. For objectivists, interpreting probability as extension of logic, probability quantifies the reasonable expectation everyone (even a \"robot\") sharing the same knowledge should share in accordance with the rules of Bayesian statistics, which can be justified by Cox's theorem. For subjectivists, probability corresponds to a personal belief. Rationality and coherence allow for substantial variation within the constraints they pose; the constraints are justified by the Dutch book argument or by the decision theory and de Finetti's theorem. The objective and subjective variants of Bayesian probability differ mainly in their interpretation and construction of the prior probability.\n\nHistory\n\nThe term Bayesian refers to Thomas Bayes (1702–1761), who proved a special case of what is now called Bayes' theorem in a paper titled \"An Essay towards solving a Problem in the Doctrine of Chances\".McGrayne, Sharon Bertsch. (2011).  In that special case, the prior and posterior distributions were Beta distributions and the data came from Bernoulli trials. It was Pierre-Simon Laplace (1749–1827) who introduced a general version of the theorem and used it to approach problems in celestial mechanics, medical statistics, reliability, and jurisprudence.Stigler, Stephen M. (1986) The history of statistics. Harvard University press. Chapter 3.  Early Bayesian inference, which used uniform priors following Laplace's principle of insufficient reason, was called \"inverse probability\" (because it infers backwards from observations to parameters, or from effects to causes).Fienberg, Stephen. E. (2006) [http://ba.stat.cmu.edu/journal/2006/vol01/issue01/fienberg.pdf When did Bayesian Inference become \"Bayesian\"?]  Bayesian Analysis, 1 (1), 1–40. See page 5. After the 1920s, \"inverse probability\" was largely supplanted by a collection of methods that came to be called frequentist statistics.\n\nIn the 20th century, the ideas of Laplace developed in two directions, giving rise to objective and subjective currents in Bayesian practice.\nHarold Jeffreys' Theory of Probability (first published in 1939) played an important role in the revival of the Bayesian view of probability, followed by works by Abraham Wald (1950) and Leonard J. Savage (1954). The adjective Bayesian itself dates to the 1950s;  the derived Bayesianism, neo-Bayesianism is of 1960s coinage.\n\"The works of Wald, Statistical Decision Functions (1950) and Savage, The Foundation of Statistics (1954) are commonly regarded starting points for current Bayesian approaches\";\n\"Recent developments of the so-called Bayesian approach to statistics\"\nMarshall Dees Harris, Legal-economic research, University of Iowa. Agricultural Law Center (1959), p. 125 (fn. 52); p. 126.\n\n\"This revolution, which may or may not succeed, is neo-Bayesianism. Jeffreys tried to introduce this approach, but did not succeed at the time in giving it general appeal.\"  Annals of the Computation Laboratory of Harvard University 31 (1962), p. 180.\n\"It is curious that even in its activities unrelated to ethics, humanity searches for a religion. At the present time, the religion being 'pushed' the hardest is Bayesianism.\"\nOscar Kempthorne, 'The Classical Problem of Inference—Goodness of Fit', Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability  (1967), [https://books.google.com/books?idIC4Ku_7dBFUC&pg\nPA235#vonepage&q&f\nfalse p. 235].\nIn the objectivist stream, the statistical analysis depends on only the model assumed and the data analysed. No subjective decisions need to be involved. In contrast, \"subjectivist\" statisticians deny the possibility of fully objective analysis for the general case.\n\nIn the 1980s there was a dramatic growth in research and applications of Bayesian methods, mostly attributed to the discovery of Markov chain Monte Carlo methods and the consequent removal of many of the computational problems, and to an increasing interest in nonstandard, complex applications.Wolpert, R.L. (2004) A conversation with James O. Berger, Statistical science, 9, 205–218 While frequentist statistics remains strong (as seen by the fact that most undergraduate teaching is still based on it Bernardo, José M. (2006) [http://www.ime.usp.br/~abe/ICOTS7/Proceedings/PDFs/InvitedPapers/3I2_BERN.pdf A Bayesian mathematical statistics primer]. ICOTS-7), Bayesian methods are widely accepted and used, e.g., in the field of machine learning.Bishop, C.M. Pattern Recognition and Machine Learning. Springer, 2007\n\nJustification of Bayesian probabilities\n\nThe use of Bayesian probabilities as the basis of Bayesian inference has been supported by several arguments, such as Cox axioms, the Dutch book argument, arguments based on decision theory and de Finetti's theorem.\n\nAxiomatic approach\n\nRichard T. Cox showed that Bayesian updating follows from several axioms, including two functional equations and a hypothesis of differentiability. The assumption of differentiability or even continuity is controversial; Halpern found a counterexample based on his observation that the Boolean algebra of statements may be finite.Halpern, J. A counterexample to theorems of Cox and Fine, Journal of Artificial Intelligence Research, 10: 67–85. Other axiomatizations have been suggested by various authors with the purpose of making the theory more rigorous.Dupré, Maurice J., Tipler, Frank J. [http://projecteuclid.org/download/pdf_1/euclid.ba/1340369856 New Axioms For Rigorous Bayesian Probability], Bayesian Analysis (2009), Number 3, pp. 599–606\n\nDutch book approach\n\nThe Dutch book argument was proposed by de Finetti; it is based on betting. A Dutch book is made when a clever gambler places a set of bets that guarantee a profit, no matter what the outcome of the bets. If a bookmaker follows the rules of the Bayesian calculus in the construction of his odds, a Dutch book cannot be made.\n\nHowever, Ian Hacking noted that traditional Dutch book arguments did not specify Bayesian updating: they left open the possibility that non-Bayesian updating rules could avoid Dutch books. For example, Hacking writesHacking (1967, Section 3, page 316), Hacking (1988, page 124) \"And neither the Dutch book argument, nor any other in the personalist arsenal of proofs of the probability axioms, entails the dynamic assumption. Not one entails Bayesianism. So the personalist requires the dynamic assumption to be Bayesian. It is true that in consistency a personalist could abandon the Bayesian model of learning from experience. Salt could lose its savour.\"\n\nIn fact, there are non-Bayesian updating rules that also avoid Dutch books (as discussed in the literature on \"probability kinematics\" following the publication of Richard C. Jeffreys' rule, which is itself regarded as Bayesian). The additional hypotheses sufficient to (uniquely) specify Bayesian updating are substantial and not universally seen as satisfactory.van Frassen, B. (1989) Laws and Symmetry, Oxford University Press. \n\nDecision theory approach\n\nA decision-theoretic justification of the use of Bayesian inference (and hence of Bayesian probabilities) was given by Abraham Wald, who proved that every admissible statistical procedure is either a Bayesian procedure or a limit of Bayesian procedures.Wald, Abraham. Statistical Decision Functions. Wiley 1950. Conversely, every Bayesian procedure is admissible.Bernardo, José M., Smith, Adrian F.M. Bayesian Theory. John Wiley 1994. .\n\nPersonal probabilities and objective methods for constructing priors\n\nFollowing the work on expected utility theory of Ramsey and von Neumann,  decision-theorists have accounted for rational behavior using a probability distribution for the agent. Johann Pfanzagl completed the Theory of Games and Economic Behavior by providing an axiomatization of subjective probability and utility, a task left uncompleted by von Neumann and Oskar Morgenstern: their original theory supposed that all the agents had the same probability distribution, as a convenience.Pfanzagl (1967, 1968) Pfanzagl's axiomatization was endorsed by Oskar Morgenstern: \"Von Neumann and I have anticipated\" the question whether probabilities \"might, perhaps more typically, be subjective and have stated specifically that in the latter case axioms could be found from which could derive the desired numerical utility together with a number for the probabilities (cf. p. 19 of The Theory of Games and Economic Behavior). We did not carry this out; it was demonstrated by Pfanzagl ... with all the necessary rigor\".Morgenstern (1976, page 65)\n\nRamsey and Savage noted that the individual agent's probability distribution could be objectively studied in experiments. The role of judgment and disagreement in science has been recognized since Aristotle and even more clearly with Francis Bacon. The objectivity of science lies not in the psychology of individual scientists, but in the process of science and especially in statistical methods, as noted by C. S. Peirce. Recall that the objective methods for falsifying propositions about personal probabilities have been used for a half century, as noted previously. Procedures for testing hypotheses about probabilities (using finite samples) are due to Ramsey (1931) and de Finetti (1931, 1937, 1964, 1970). Both Bruno de Finetti and Frank P. Ramsey acknowledge their debts to pragmatic philosophy, particularly (for Ramsey) to Charles S. Peirce.\n\nThe \"Ramsey test\" for evaluating probability distributions is implementable in theory, and has kept experimental psychologists occupied for a half century.Davidson et al. (1957)\nThis work demonstrates that Bayesian-probability propositions can be falsified, and so meet an empirical criterion of Charles S. Peirce, whose work inspired Ramsey. (This falsifiability-criterion was popularized by Karl Popper.[http://plato.stanford.edu/entries/popper/#ProDem \"Karl Popper\" in Stanford Encyclopedia of Philosophy]Popper, Karl. (2002) [https://books.google.com/books?idT76Zd20IYlgC&printsec\nfrontcover&dqlogic+of+scientific+discovery&source\nbl&otseGfU8COmAA&sig\nW8TTt29x5sG8aCGpYuWnonDFH8M&hlen&ei\ncrC3TJKND46ssAPp06jkCA&saX&oi\nbook_result&ctresult&resnum\n3&ved0CCAQ6AEwAg#v\nonepage&qfalsifiability&f\nfalse The Logic of Scientific Discovery] 2nd Edition, Routledge  (Reprint of 1959 translation of 1935 original) Page 57.)\n\nModern work on the experimental evaluation of personal probabilities uses the randomization, blinding, and Boolean-decision procedures of the Peirce-Jastrow experiment.Peirce & Jastrow (1885)\n Since individuals act according to different probability judgments, these agents' probabilities are \"personal\" (but amenable to objective study).\n\nPersonal probabilities are problematic for science and for some applications where decision-makers lack the knowledge or time to specify an informed probability-distribution (on which they are prepared to act). To meet the needs of science and of human limitations, Bayesian statisticians have developed \"objective\" methods for specifying prior probabilities.\n\nIndeed, some Bayesians have argued the prior state of knowledge defines the (unique) prior probability-distribution for \"regular\" statistical problems; cf. well-posed problems. Finding the right method for constructing such \"objective\" priors (for appropriate classes of regular problems) has been the quest of statistical theorists from Laplace to John Maynard Keynes, Harold Jeffreys, and Edwin Thompson Jaynes. These theorists and their successors have suggested several methods for constructing \"objective\" priors (Unfortunately, it is not clear how to assess the relative \"objectivity\" of the priors proposed under these methods):\n* Maximum entropy\n* Transformation group analysis\n* Reference analysis\n\nEach of these methods contributes useful priors for \"regular\" one-parameter problems, and each prior can handle some challenging statistical models (with \"irregularity\" or several parameters). Each of these methods has been useful in Bayesian practice. Indeed, methods for constructing \"objective\" (alternatively, \"default\" or \"ignorance\") priors have been developed by avowed subjective (or \"personal\") Bayesians like James Berger (Duke University) and José-Miguel Bernardo (Universitat de València), simply because such priors are needed for Bayesian practice, particularly in science.Bernardo, J. M. (2005). [http://www.uv.es/~bernardo/RefAna.pdf Reference Analysis]. Handbook of Statistics 25 (D. K. Dey and C. R. Rao eds). Amsterdam: Elsevier, 17–90 The quest for \"the universal method for constructing priors\" continues to attract statistical theorists.\n\nThus, the Bayesian statistician needs either to use informed priors (using relevant expertise or previous data) or to choose among the competing methods for constructing \"objective\" priors.",
  "entityProperties" : [ {
    "name" : "title",
    "type" : "String",
    "values" : [ "Bayesian probability" ],
    "synthetic" : false
  }, {
    "name" : "url",
    "type" : "String",
    "values" : [ "http://en.wikipedia.org/?curid=4890" ],
    "synthetic" : false
  } ],
  "classifications" : [ "xml-export" ],
  "technicalAttributes" : {
    "technicalAttributes" : null,
    "aggregatedText" : "Bayesian probability is an interpretation of the concept of probability, in which, instead of frequency or propensity of some phenomenon, probability is interpreted as reasonable expectation representing a state of knowledgeJaynes, E.T. \"Bayesian Methods: General Background.\" In Maximum-Entropy and Bayesian Methods in Applied Statistics, by J. H. Justice (ed.). Cambridge: Cambridge Univ. Press, 1986 or as quantification of a personal belief.\n\nThe Bayesian interpretation of probability can be seen as an extension of propositional logic that enables reasoning with hypotheses, i.e., the propositions whose truth or falsity is uncertain. In the Bayesian view, a probability is assigned to a hypothesis, whereas under frequentist inference, a hypothesis is typically tested without being assigned a probability.\n\nBayesian probability belongs to the category of evidential probabilities; to evaluate the probability of a hypothesis, the Bayesian probabilist specifies some prior probability, which is then updated to a posterior probability in the light of new, relevant data (evidence).Paulos, John Allen. [https://www.nytimes.com/2011/08/07/books/review/the-theory-that-would-not-die-by-sharon-bertsch-mcgrayne-book-review.html?_r\n1&scp1&sq\nthomas%20bayes&st=cse The Mathematics of Changing Your Mind,] New York Times (US). August 5, 2011; retrieved 2011-08-06 The Bayesian interpretation provides a standard set of procedures and formulae to perform this calculation.\n\nThe term Bayesian derives from the 18th century mathematician and theologian Thomas Bayes, who provided the first mathematical treatment of a non-trivial problem of Bayesian inference.Stigler, Stephen M. (1986) The history of statistics. Harvard University Press. pg 131. Mathematician Pierre-Simon Laplace pioneered and popularised what is now called Bayesian probability.Stigler, Stephen M. (1986) The history of statistics., Harvard University press. pp. 97–98, 131.\n\nBroadly speaking, there are two views on Bayesian probability that interpret the probability concept in different ways. According to the objectivist view, probability is a reasonable expectation that represents the state of knowledge, can be interpreted as an extension of logic, and its rules can be justified by Cox's theorem.Cox, Richard T. Algebra of Probable Inference, The Johns Hopkins University Press, 2001 According to the subjectivist view, probability quantifies a personal belief, and its rules can be justified by requirements of rationality and coherence following from the Dutch book argument or from the decision theory and de Finetti's theorem.de Finetti, B. (1974) Theory of probability (2 vols.), J. Wiley & Sons, Inc., New York\n\nBayesian methodology\n\nBayesian methods are characterized by concepts and procedures as follows:\n* The use of random variables, or more generally unknown quantities, to model all sources of uncertainty in statistical models including uncertainty resulting from lack of information (see also aleatoric and epistemic uncertainty).\n* The need to determine the prior probability distribution taking into account the available (prior) information.\n* The sequential use of Bayes' formula: when more data become available, calculate the posterior distribution using Bayes' formula; subsequently, the posterior distribution becomes the next prior.\n* While for the frequentist a hypothesis is a proposition (which must be either true or false), so that the frequentist probability of a hypothesis is either 0 or 1, in Bayesian statistics the probability that can be assigned to a hypothesis can also be in a range from 0 to 1 if the truth value is uncertain.\n\nObjective and subjective Bayesian probabilities\n\nBroadly speaking, there are two interpretations on Bayesian probability. For objectivists, interpreting probability as extension of logic, probability quantifies the reasonable expectation everyone (even a \"robot\") sharing the same knowledge should share in accordance with the rules of Bayesian statistics, which can be justified by Cox's theorem. For subjectivists, probability corresponds to a personal belief. Rationality and coherence allow for substantial variation within the constraints they pose; the constraints are justified by the Dutch book argument or by the decision theory and de Finetti's theorem. The objective and subjective variants of Bayesian probability differ mainly in their interpretation and construction of the prior probability.\n\nHistory\n\nThe term Bayesian refers to Thomas Bayes (1702–1761), who proved a special case of what is now called Bayes' theorem in a paper titled \"An Essay towards solving a Problem in the Doctrine of Chances\".McGrayne, Sharon Bertsch. (2011).  In that special case, the prior and posterior distributions were Beta distributions and the data came from Bernoulli trials. It was Pierre-Simon Laplace (1749–1827) who introduced a general version of the theorem and used it to approach problems in celestial mechanics, medical statistics, reliability, and jurisprudence.Stigler, Stephen M. (1986) The history of statistics. Harvard University press. Chapter 3.  Early Bayesian inference, which used uniform priors following Laplace's principle of insufficient reason, was called \"inverse probability\" (because it infers backwards from observations to parameters, or from effects to causes).Fienberg, Stephen. E. (2006) [http://ba.stat.cmu.edu/journal/2006/vol01/issue01/fienberg.pdf When did Bayesian Inference become \"Bayesian\"?]  Bayesian Analysis, 1 (1), 1–40. See page 5. After the 1920s, \"inverse probability\" was largely supplanted by a collection of methods that came to be called frequentist statistics.\n\nIn the 20th century, the ideas of Laplace developed in two directions, giving rise to objective and subjective currents in Bayesian practice.\nHarold Jeffreys' Theory of Probability (first published in 1939) played an important role in the revival of the Bayesian view of probability, followed by works by Abraham Wald (1950) and Leonard J. Savage (1954). The adjective Bayesian itself dates to the 1950s;  the derived Bayesianism, neo-Bayesianism is of 1960s coinage.\n\"The works of Wald, Statistical Decision Functions (1950) and Savage, The Foundation of Statistics (1954) are commonly regarded starting points for current Bayesian approaches\";\n\"Recent developments of the so-called Bayesian approach to statistics\"\nMarshall Dees Harris, Legal-economic research, University of Iowa. Agricultural Law Center (1959), p. 125 (fn. 52); p. 126.\n\n\"This revolution, which may or may not succeed, is neo-Bayesianism. Jeffreys tried to introduce this approach, but did not succeed at the time in giving it general appeal.\"  Annals of the Computation Laboratory of Harvard University 31 (1962), p. 180.\n\"It is curious that even in its activities unrelated to ethics, humanity searches for a religion. At the present time, the religion being 'pushed' the hardest is Bayesianism.\"\nOscar Kempthorne, 'The Classical Problem of Inference—Goodness of Fit', Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability  (1967), [https://books.google.com/books?idIC4Ku_7dBFUC&pg\nPA235#vonepage&q&f\nfalse p. 235].\nIn the objectivist stream, the statistical analysis depends on only the model assumed and the data analysed. No subjective decisions need to be involved. In contrast, \"subjectivist\" statisticians deny the possibility of fully objective analysis for the general case.\n\nIn the 1980s there was a dramatic growth in research and applications of Bayesian methods, mostly attributed to the discovery of Markov chain Monte Carlo methods and the consequent removal of many of the computational problems, and to an increasing interest in nonstandard, complex applications.Wolpert, R.L. (2004) A conversation with James O. Berger, Statistical science, 9, 205–218 While frequentist statistics remains strong (as seen by the fact that most undergraduate teaching is still based on it Bernardo, José M. (2006) [http://www.ime.usp.br/~abe/ICOTS7/Proceedings/PDFs/InvitedPapers/3I2_BERN.pdf A Bayesian mathematical statistics primer]. ICOTS-7), Bayesian methods are widely accepted and used, e.g., in the field of machine learning.Bishop, C.M. Pattern Recognition and Machine Learning. Springer, 2007\n\nJustification of Bayesian probabilities\n\nThe use of Bayesian probabilities as the basis of Bayesian inference has been supported by several arguments, such as Cox axioms, the Dutch book argument, arguments based on decision theory and de Finetti's theorem.\n\nAxiomatic approach\n\nRichard T. Cox showed that Bayesian updating follows from several axioms, including two functional equations and a hypothesis of differentiability. The assumption of differentiability or even continuity is controversial; Halpern found a counterexample based on his observation that the Boolean algebra of statements may be finite.Halpern, J. A counterexample to theorems of Cox and Fine, Journal of Artificial Intelligence Research, 10: 67–85. Other axiomatizations have been suggested by various authors with the purpose of making the theory more rigorous.Dupré, Maurice J., Tipler, Frank J. [http://projecteuclid.org/download/pdf_1/euclid.ba/1340369856 New Axioms For Rigorous Bayesian Probability], Bayesian Analysis (2009), Number 3, pp. 599–606\n\nDutch book approach\n\nThe Dutch book argument was proposed by de Finetti; it is based on betting. A Dutch book is made when a clever gambler places a set of bets that guarantee a profit, no matter what the outcome of the bets. If a bookmaker follows the rules of the Bayesian calculus in the construction of his odds, a Dutch book cannot be made.\n\nHowever, Ian Hacking noted that traditional Dutch book arguments did not specify Bayesian updating: they left open the possibility that non-Bayesian updating rules could avoid Dutch books. For example, Hacking writesHacking (1967, Section 3, page 316), Hacking (1988, page 124) \"And neither the Dutch book argument, nor any other in the personalist arsenal of proofs of the probability axioms, entails the dynamic assumption. Not one entails Bayesianism. So the personalist requires the dynamic assumption to be Bayesian. It is true that in consistency a personalist could abandon the Bayesian model of learning from experience. Salt could lose its savour.\"\n\nIn fact, there are non-Bayesian updating rules that also avoid Dutch books (as discussed in the literature on \"probability kinematics\" following the publication of Richard C. Jeffreys' rule, which is itself regarded as Bayesian). The additional hypotheses sufficient to (uniquely) specify Bayesian updating are substantial and not universally seen as satisfactory.van Frassen, B. (1989) Laws and Symmetry, Oxford University Press. \n\nDecision theory approach\n\nA decision-theoretic justification of the use of Bayesian inference (and hence of Bayesian probabilities) was given by Abraham Wald, who proved that every admissible statistical procedure is either a Bayesian procedure or a limit of Bayesian procedures.Wald, Abraham. Statistical Decision Functions. Wiley 1950. Conversely, every Bayesian procedure is admissible.Bernardo, José M., Smith, Adrian F.M. Bayesian Theory. John Wiley 1994. .\n\nPersonal probabilities and objective methods for constructing priors\n\nFollowing the work on expected utility theory of Ramsey and von Neumann,  decision-theorists have accounted for rational behavior using a probability distribution for the agent. Johann Pfanzagl completed the Theory of Games and Economic Behavior by providing an axiomatization of subjective probability and utility, a task left uncompleted by von Neumann and Oskar Morgenstern: their original theory supposed that all the agents had the same probability distribution, as a convenience.Pfanzagl (1967, 1968) Pfanzagl's axiomatization was endorsed by Oskar Morgenstern: \"Von Neumann and I have anticipated\" the question whether probabilities \"might, perhaps more typically, be subjective and have stated specifically that in the latter case axioms could be found from which could derive the desired numerical utility together with a number for the probabilities (cf. p. 19 of The Theory of Games and Economic Behavior). We did not carry this out; it was demonstrated by Pfanzagl ... with all the necessary rigor\".Morgenstern (1976, page 65)\n\nRamsey and Savage noted that the individual agent's probability distribution could be objectively studied in experiments. The role of judgment and disagreement in science has been recognized since Aristotle and even more clearly with Francis Bacon. The objectivity of science lies not in the psychology of individual scientists, but in the process of science and especially in statistical methods, as noted by C. S. Peirce. Recall that the objective methods for falsifying propositions about personal probabilities have been used for a half century, as noted previously. Procedures for testing hypotheses about probabilities (using finite samples) are due to Ramsey (1931) and de Finetti (1931, 1937, 1964, 1970). Both Bruno de Finetti and Frank P. Ramsey acknowledge their debts to pragmatic philosophy, particularly (for Ramsey) to Charles S. Peirce.\n\nThe \"Ramsey test\" for evaluating probability distributions is implementable in theory, and has kept experimental psychologists occupied for a half century.Davidson et al. (1957)\nThis work demonstrates that Bayesian-probability propositions can be falsified, and so meet an empirical criterion of Charles S. Peirce, whose work inspired Ramsey. (This falsifiability-criterion was popularized by Karl Popper.[http://plato.stanford.edu/entries/popper/#ProDem \"Karl Popper\" in Stanford Encyclopedia of Philosophy]Popper, Karl. (2002) [https://books.google.com/books?idT76Zd20IYlgC&printsec\nfrontcover&dqlogic+of+scientific+discovery&source\nbl&otseGfU8COmAA&sig\nW8TTt29x5sG8aCGpYuWnonDFH8M&hlen&ei\ncrC3TJKND46ssAPp06jkCA&saX&oi\nbook_result&ctresult&resnum\n3&ved0CCAQ6AEwAg#v\nonepage&qfalsifiability&f\nfalse The Logic of Scientific Discovery] 2nd Edition, Routledge  (Reprint of 1959 translation of 1935 original) Page 57.)\n\nModern work on the experimental evaluation of personal probabilities uses the randomization, blinding, and Boolean-decision procedures of the Peirce-Jastrow experiment.Peirce & Jastrow (1885)\n Since individuals act according to different probability judgments, these agents' probabilities are \"personal\" (but amenable to objective study).\n\nPersonal probabilities are problematic for science and for some applications where decision-makers lack the knowledge or time to specify an informed probability-distribution (on which they are prepared to act). To meet the needs of science and of human limitations, Bayesian statisticians have developed \"objective\" methods for specifying prior probabilities.\n\nIndeed, some Bayesians have argued the prior state of knowledge defines the (unique) prior probability-distribution for \"regular\" statistical problems; cf. well-posed problems. Finding the right method for constructing such \"objective\" priors (for appropriate classes of regular problems) has been the quest of statistical theorists from Laplace to John Maynard Keynes, Harold Jeffreys, and Edwin Thompson Jaynes. These theorists and their successors have suggested several methods for constructing \"objective\" priors (Unfortunately, it is not clear how to assess the relative \"objectivity\" of the priors proposed under these methods):\n* Maximum entropy\n* Transformation group analysis\n* Reference analysis\n\nEach of these methods contributes useful priors for \"regular\" one-parameter problems, and each prior can handle some challenging statistical models (with \"irregularity\" or several parameters). Each of these methods has been useful in Bayesian practice. Indeed, methods for constructing \"objective\" (alternatively, \"default\" or \"ignorance\") priors have been developed by avowed subjective (or \"personal\") Bayesians like James Berger (Duke University) and José-Miguel Bernardo (Universitat de València), simply because such priors are needed for Bayesian practice, particularly in science.Bernardo, J. M. (2005). [http://www.uv.es/~bernardo/RefAna.pdf Reference Analysis]. Handbook of Statistics 25 (D. K. Dey and C. R. Rao eds). Amsterdam: Elsevier, 17–90 The quest for \"the universal method for constructing priors\" continues to attract statistical theorists.\n\nThus, the Bayesian statistician needs either to use informed priors (using relevant expertise or previous data) or to choose among the competing methods for constructing \"objective\" priors. Bayesian probability. http://en.wikipedia.org/?curid=4890."
  }
}
