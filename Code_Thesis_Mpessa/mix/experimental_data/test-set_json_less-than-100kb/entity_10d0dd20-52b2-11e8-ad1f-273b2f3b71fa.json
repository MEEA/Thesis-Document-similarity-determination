{
  "datasourceIdentifier" : "awesome wiki export",
  "backlink" : "http://en.wikipedia.org/?curid=13995",
  "eid" : "10d0dd20-52b2-11e8-ad1f-273b2f3b71fa",
  "loadTime" : 1525778537458,
  "textBody" : "In computer science, heapsort is a comparison-based sorting algorithm. Heapsort can be thought of as an improved selection sort: like that algorithm, it divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element and moving that to the sorted region. The improvement consists of the use of a heap data structure rather than a linear-time search to find the maximum.\n\nAlthough somewhat slower in practice on most machines than a well-implemented quicksort, it has the advantage of a more favorable worst-case big O notation| runtime.  Heapsort is an in-place algorithm, but it is not a stable sort.\n\nHeapsort was invented by J. W. J. Williams in 1964. This was also the birth of the heap, presented already by Williams as a useful data structure in its own right. In the same year, R. W. Floyd published an improved version that could sort an array in-place, continuing his earlier research into the treesort algorithm.\n\nOverview \n\nThe heapsort algorithm can be divided into two parts.\n\nIn the first step, a heap is built out of the data. The heap is often placed in an array with the layout of a complete binary tree. The complete binary tree maps the binary tree structure into the array indices; each array index represents a node; the index of the node's parent, left child branch, or right child branch are simple expressions.  For a zero-based array, the root node is stored at index 0; if i is the index of the current node, then\n\n  iParent(i)     = floor((i-1) / 2) where floor functions map a real number to the smallest leading integer.\n  iLeftChild(i)  = 2*i + 1\n  iRightChild(i) = 2*i + 2\n\nIn the second step, a sorted array is created by repeatedly removing the largest element from the heap (the root of the heap), and inserting it into the array. The heap is updated after each removal to maintain the heap property. Once all objects have been removed from the heap, the result is a sorted array.\n\nHeapsort can be performed in place. The array can be split into two parts, the sorted array and the heap. The storage of heaps as arrays is diagrammed here.  The heap's invariant is preserved after each extraction, so the only cost is that of extraction.\n\nAlgorithm \n\nThe heapsort algorithm involves preparing the list by first turning it into a max heap. The algorithm then repeatedly swaps the first value of the list with the last value, decreasing the range of values considered in the heap operation by one, and sifting the new first value into its position in the heap. This repeats until the range of considered values is one value in length.\n\nThe steps are:\n# Call the buildMaxHeap() function on the list. Also referred to as heapify(), this builds a heap from a list in O(n) operations.\n# Swap the first element of the list with the final element. Decrease the considered range of the list by one.\n# Call the siftDown() function on the list to sift the new first element to its appropriate index in the heap.\n# Go to step (2) unless the considered range of the list is one element.\nThe buildMaxHeap() operation is run once, and is  in performance. The siftDown() function is , and is called  times. Therefore, the performance of this algorithm is .\n\nPseudocode \n\nThe following is a simple way to implement the algorithm in pseudocode. Arrays are zero-based and swap is used to exchange two elements of the array. Movement 'down' means from the root towards the leaves, or from lower indices to higher. Note that during the sort, the largest element is at the root of the heap at a[0], while at the end of the sort, the largest element is in a[end].\n\n procedure heapsort(a, count) is\n     input: an unordered array a of length count\n  \n     (Build the heap in array a so that largest value is at the root)\n     heapify(a, count)\n \n     (The following loop maintains the invariants that a[0:end] is a heap and every element\n      beyond end is greater than everything before it (so a[end:count] is in sorted order))\n     end ← count - 1\n     while end > 0 do\n         (a[0] is the root and largest value. The swap moves it in front of the sorted elements.)\n         swap(a[end], a[0])\n         (the heap size is reduced by one)\n         end ← end - 1\n         (the swap ruined the heap property, so restore it)\n          siftDown(a, start, end)\n\nThe sorting routine uses two subroutines, heapify and siftDown. The former is the common in-place heap construction routine, while the latter is a common subroutine for implementing heapify.\n \n (Put elements of 'a' in heap order, in-place)\n procedure heapify(a, count) is\n     (start is assigned the index in 'a' of the last parent node)\n     (the last element in a 0-based array is at index count-1; find the parent of that element)\n     start ← iParent(count-1)\n     \n     while start ≥ 0 do\n         (sift down the node at index 'start' to the proper place such that all nodes below\n          the start index are in heap order)\n         siftDown(a, start, count - 1)\n         (go to the next parent node)\n         start ← start - 1\n     (after sifting down the root all nodes/elements are in heap order)\n \n (Repair the heap whose root element is at index 'start', assuming the heaps rooted at its children are valid)\n procedure siftDown(a, start, end) is\n     root ← start\n \n     while iLeftChild(root) ≤ end do    (While the root has at least one child)\n         child ← iLeftChild(root)   (Left child of root)\n         swap ← root                (Keeps track of child to swap with)\n \n         if a[swap] heapify procedure can be thought of as building a heap from the bottom up by successively sifting downward to establish the heap property. An alternative version (shown below) that builds the heap top-down and sifts upward may be simpler to understand. This siftUp version can be visualized as starting with an empty heap and successively inserting elements, whereas the siftDown version given above treats the entire input array as a full but \"broken\" heap and \"repairs\" it starting from the last non-trivial sub-heap (that is, the last parent node).\n\nAlso, the siftDown version of heapify has  time complexity, while the siftUp version given below has  time complexity due to its equivalence with inserting each element, one at a time, into an empty heap.\nThis may seem counter-intuitive since, at a glance, it is apparent that the former only makes half as many calls to its logarithmic-time sifting function as the latter; i.e., they seem to differ only by a constant factor, which never affects asymptotic analysis.\n\nTo grasp the intuition behind this difference in complexity, note that the number of swaps that may occur during any one siftUp call increases with the depth of the node on which the call is made. The crux is that there are many (exponentially many) more \"deep\" nodes than there are \"shallow\" nodes in a heap, so that siftUp may have its full logarithmic running-time on the approximately linear number of calls made on the nodes at or near the \"bottom\" of the heap. On the other hand, the number of swaps that may occur during any one siftDown call decreases as the depth of the node on which the call is made increases. Thus, when the siftDown heapify begins and is calling siftDown on the bottom and most numerous node-layers, each sifting call will incur, at most, a number of swaps equal to the \"height\" (from the bottom of the heap) of the node on which the sifting call is made. In other words, about half the calls to siftDown will have at most only one swap, then about a quarter of the calls will have at most two swaps, etc.\n\nThe heapsort algorithm itself has  time complexity using either version of heapify.\n\n  procedure heapify(a,count) is\n      (end is assigned the index of the first (left) child of the root)\n      end := 1\n      \n      while end  start\n          parent := iParent(child)\n          if a[parent] 2(N) − e2(N), where s2(N) is the number of 1 bits in the binary representation of N and e2(N) is number of trailing 0 bits.\n* The standard implementation of Floyd's heap-construction algorithm causes a large number of cache misses once the size of the data exceeds that of the CPU cache.  Much better performance on large data sets can be obtained by merging in depth-first order, combining subheaps as soon as possible, rather than combining all subheaps on one level before proceeding to the one above. [https://www.semanticscholar.org/paper/Performance-Engineering-Case-Study-Heap-Bojesen-Katajainen/6f4ada5912c1da64e16453d67ec99c970173fb5b Alternate PDF source].  See particularly Fig. 3.\n*Ternary heapsort\"Data Structures Using Pascal\", 1991, page 405, gives a ternary heapsort as a student exercise. \"Write a sorting routine similar to the heapsort except that it uses a ternary heap.\" uses a ternary heap instead of a binary heap; that is, each element in the heap has three children. It is more complicated to program, but does a constant number of times fewer swap and comparison operations.  This is because each sifting step in a ternary heap requires three comparisons and one swap, whereas in a binary heap two comparisons and one swap are required. Two levels in a ternary heap cover 9 elements, doing more work with the same number of comparisons as three levels in the binary heap, which only cover 8.\n*The smoothsort algorithm is a variation of heapsort developed by Edsger Dijkstra in 1981. Like heapsort, smoothsort's upper bound is Big O notation|. The advantage of smoothsort is that it comes closer to  time if the input is already sorted to some degree, whereas heapsort averages  regardless of the initial sorted state. Due to its complexity, smoothsort is rarely used.\n*Levcopoulos and Petersson. describe a variation of heapsort based on a Cartesian tree that does not add an element to the heap until smaller values on both sides of it have already been included in the sorted output. As they show, this modification can allow the algorithm to sort more quickly than  for inputs that are already nearly sorted.\n* Several variants such as weak heapsort require  comparisons in the worst case, close to the theoretical minimum, using one extra bit of state per node.  While this extra bit makes the algorithms not truly in-place, if space for it can be found inside the element, these algorithms are simple and efficient, but still slower than binary heaps if key comparisons are cheap enough (e.g. integer keys) that a constant factor does not matter.\n* Katajainen's \"ultimate heapsort\" requires no extra storage, performs  comparisons, and a similar number of element moves.  It is, however, even more complex and not justified unless comparisons are very expensive.\n\nBottom-up heapsort \n\nBottom-up heapsort was announced as beating quicksort (with median-of-three pivot selection) on arrays of size ≥16000. This version of heapsort keeps the linear-time heap-building phase, but changes the second phase, as follows.\n\nOrdinary heapsort extracts the top of the heap, , and fills the gap it leaves with , then sifts this latter element down the heap; but this element comes from the lowest level of the heap, meaning it is one of the smallest elements in the heap, so the sift-down will likely take many steps to move it back down. Each step of the sift-down requires two comparisons, to find the minimum of the new node and its two children.\n\nBottom-up heapsort instead finds the path of largest children to the leaf level of the tree (as if it were inserting −∞) using only one comparison per level.  Put another way, it finds the leaf which has the property that it and all of its ancestors are greater than their siblings.  (In the absence of equal keys, this leaf is unique.)  Then, from this leaf, it searches upward (using one comparison per level) for the correct position in that path to insert .  This is the same location as ordinary heapsort finds, and requires the same number of exchanges to perform the insert, but fewer comparisons are required to find that location.  Also available as \n\n function leafSearch(a, i, end) is\n     j ← i\n     while iLeftChild(j) ≤ end do\n         (Determine which of j's children is the greater)\n         if iRightChild(j) ≤ end and a[iRightChild(j)] > a[iLeftChild(j)] then\n             j ← iRightChild(j)\n         else\n             j ← iLeftChild(j)\n     return j\n\nThe return value of the leafSearch is used in the modified siftDown routine:\n\n procedure siftDown(a, i, end) is\n     j ← leafSearch(a, i, end)\n     while a[i] > a[j] do\n         j ← iParent(j)\n     x ← a[j]\n     a[j] ← a[i]\n     while j > i do\n         swap x, a[iParent(j)]\n         j ← iParent(j)\n\nBottom-up heapsort requires only  comparisons in the worst case and  on average.  For comparison, ordinary heapsort requires  comparisons worst-case and on average.\n\nA 2008 re-evaluation of this algorithm showed it to be no faster than ordinary heapsort for integer keys, though, presumably because modern branch prediction nullifies the cost of the predictable comparisons which bottom-up heapsort manages to avoid.  (It still has an advantage if comparisons are expensive.)\n\nA further refinement does a binary search in the path to the selected leaf, and sorts in a worst case of  comparisons, approaching the information-theoretic lower bound of  comparisons.\n\nA variant which uses two extra bits per internal node (n−1 bits total for an n-element heap) to cache information about which child is greater (two bits are required to store three cases: left, right, and unknown) uses less than  compares.\n\nComparison with other sorts \n\nHeapsort primarily competes with quicksort, another very efficient general purpose nearly-in-place comparison-based sort algorithm.\n\nQuicksort is typically somewhat faster due to some factors, but the worst-case running time for quicksort is , which is unacceptable for large data sets and can be deliberately triggered given enough knowledge of the implementation, creating a security risk. See quicksort for a detailed discussion of this problem and possible solutions.\n\nThus, because of the  upper bound on heapsort's running time and constant upper bound on its auxiliary storage, embedded systems with real-time constraints or systems concerned with security often use heapsort, such as the Linux kernel.https://github.com/torvalds/linux/blob/master/lib/sort.c Linux kernel source\n\nHeapsort also competes with merge sort, which has the same time bounds. Merge sort requires  auxiliary space, but heapsort requires only a constant amount. Heapsort typically runs faster in practice on machines with small or slow data caches, and does not require as much external memory. On the other hand, merge sort has several advantages over heapsort:\n* Merge sort on arrays has considerably better data cache performance, often outperforming heapsort on modern desktop computers because merge sort frequently accesses contiguous memory locations (good locality of reference); heapsort references are spread throughout the heap.\n* Heapsort is not a stable sort; merge sort is stable.\n* Merge sort parallelizes well and can achieve close to linear speedup with a trivial implementation; heapsort is not an obvious candidate for a parallel algorithm.\n* Merge sort can be adapted to operate on singly linked lists with  extra space. Heapsort can be adapted to operate on doubly linked lists with only  extra space overhead.\n* Merge sort is used in external sorting; heapsort is not. Locality of reference is the issue.\n\nIntrosort is an alternative to heapsort that combines quicksort and heapsort to retain advantages of both: worst case speed of heapsort and average speed of quicksort.\n\nExample \n\nLet { 6, 5, 3, 1, 8, 7, 2, 4 } be the list that we want to sort from the smallest to the largest. (NOTE, for 'Building the Heap' step: Larger nodes don't stay below smaller node parents. They are swapped with parents, and then recursively checked if another swap is needed, to keep larger numbers above smaller numbers on the heap binary tree.)\n\n1. Build the heap\n\n2. Sorting.\n\nNotes",
  "entityProperties" : [ {
    "name" : "title",
    "type" : "String",
    "values" : [ "Heapsort" ],
    "synthetic" : false
  }, {
    "name" : "url",
    "type" : "String",
    "values" : [ "http://en.wikipedia.org/?curid=13995" ],
    "synthetic" : false
  } ],
  "classifications" : [ "xml-export" ],
  "technicalAttributes" : {
    "technicalAttributes" : null,
    "aggregatedText" : "In computer science, heapsort is a comparison-based sorting algorithm. Heapsort can be thought of as an improved selection sort: like that algorithm, it divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element and moving that to the sorted region. The improvement consists of the use of a heap data structure rather than a linear-time search to find the maximum.\n\nAlthough somewhat slower in practice on most machines than a well-implemented quicksort, it has the advantage of a more favorable worst-case big O notation| runtime.  Heapsort is an in-place algorithm, but it is not a stable sort.\n\nHeapsort was invented by J. W. J. Williams in 1964. This was also the birth of the heap, presented already by Williams as a useful data structure in its own right. In the same year, R. W. Floyd published an improved version that could sort an array in-place, continuing his earlier research into the treesort algorithm.\n\nOverview \n\nThe heapsort algorithm can be divided into two parts.\n\nIn the first step, a heap is built out of the data. The heap is often placed in an array with the layout of a complete binary tree. The complete binary tree maps the binary tree structure into the array indices; each array index represents a node; the index of the node's parent, left child branch, or right child branch are simple expressions.  For a zero-based array, the root node is stored at index 0; if i is the index of the current node, then\n\n  iParent(i)     = floor((i-1) / 2) where floor functions map a real number to the smallest leading integer.\n  iLeftChild(i)  = 2*i + 1\n  iRightChild(i) = 2*i + 2\n\nIn the second step, a sorted array is created by repeatedly removing the largest element from the heap (the root of the heap), and inserting it into the array. The heap is updated after each removal to maintain the heap property. Once all objects have been removed from the heap, the result is a sorted array.\n\nHeapsort can be performed in place. The array can be split into two parts, the sorted array and the heap. The storage of heaps as arrays is diagrammed here.  The heap's invariant is preserved after each extraction, so the only cost is that of extraction.\n\nAlgorithm \n\nThe heapsort algorithm involves preparing the list by first turning it into a max heap. The algorithm then repeatedly swaps the first value of the list with the last value, decreasing the range of values considered in the heap operation by one, and sifting the new first value into its position in the heap. This repeats until the range of considered values is one value in length.\n\nThe steps are:\n# Call the buildMaxHeap() function on the list. Also referred to as heapify(), this builds a heap from a list in O(n) operations.\n# Swap the first element of the list with the final element. Decrease the considered range of the list by one.\n# Call the siftDown() function on the list to sift the new first element to its appropriate index in the heap.\n# Go to step (2) unless the considered range of the list is one element.\nThe buildMaxHeap() operation is run once, and is  in performance. The siftDown() function is , and is called  times. Therefore, the performance of this algorithm is .\n\nPseudocode \n\nThe following is a simple way to implement the algorithm in pseudocode. Arrays are zero-based and swap is used to exchange two elements of the array. Movement 'down' means from the root towards the leaves, or from lower indices to higher. Note that during the sort, the largest element is at the root of the heap at a[0], while at the end of the sort, the largest element is in a[end].\n\n procedure heapsort(a, count) is\n     input: an unordered array a of length count\n  \n     (Build the heap in array a so that largest value is at the root)\n     heapify(a, count)\n \n     (The following loop maintains the invariants that a[0:end] is a heap and every element\n      beyond end is greater than everything before it (so a[end:count] is in sorted order))\n     end ← count - 1\n     while end > 0 do\n         (a[0] is the root and largest value. The swap moves it in front of the sorted elements.)\n         swap(a[end], a[0])\n         (the heap size is reduced by one)\n         end ← end - 1\n         (the swap ruined the heap property, so restore it)\n          siftDown(a, start, end)\n\nThe sorting routine uses two subroutines, heapify and siftDown. The former is the common in-place heap construction routine, while the latter is a common subroutine for implementing heapify.\n \n (Put elements of 'a' in heap order, in-place)\n procedure heapify(a, count) is\n     (start is assigned the index in 'a' of the last parent node)\n     (the last element in a 0-based array is at index count-1; find the parent of that element)\n     start ← iParent(count-1)\n     \n     while start ≥ 0 do\n         (sift down the node at index 'start' to the proper place such that all nodes below\n          the start index are in heap order)\n         siftDown(a, start, count - 1)\n         (go to the next parent node)\n         start ← start - 1\n     (after sifting down the root all nodes/elements are in heap order)\n \n (Repair the heap whose root element is at index 'start', assuming the heaps rooted at its children are valid)\n procedure siftDown(a, start, end) is\n     root ← start\n \n     while iLeftChild(root) ≤ end do    (While the root has at least one child)\n         child ← iLeftChild(root)   (Left child of root)\n         swap ← root                (Keeps track of child to swap with)\n \n         if a[swap] heapify procedure can be thought of as building a heap from the bottom up by successively sifting downward to establish the heap property. An alternative version (shown below) that builds the heap top-down and sifts upward may be simpler to understand. This siftUp version can be visualized as starting with an empty heap and successively inserting elements, whereas the siftDown version given above treats the entire input array as a full but \"broken\" heap and \"repairs\" it starting from the last non-trivial sub-heap (that is, the last parent node).\n\nAlso, the siftDown version of heapify has  time complexity, while the siftUp version given below has  time complexity due to its equivalence with inserting each element, one at a time, into an empty heap.\nThis may seem counter-intuitive since, at a glance, it is apparent that the former only makes half as many calls to its logarithmic-time sifting function as the latter; i.e., they seem to differ only by a constant factor, which never affects asymptotic analysis.\n\nTo grasp the intuition behind this difference in complexity, note that the number of swaps that may occur during any one siftUp call increases with the depth of the node on which the call is made. The crux is that there are many (exponentially many) more \"deep\" nodes than there are \"shallow\" nodes in a heap, so that siftUp may have its full logarithmic running-time on the approximately linear number of calls made on the nodes at or near the \"bottom\" of the heap. On the other hand, the number of swaps that may occur during any one siftDown call decreases as the depth of the node on which the call is made increases. Thus, when the siftDown heapify begins and is calling siftDown on the bottom and most numerous node-layers, each sifting call will incur, at most, a number of swaps equal to the \"height\" (from the bottom of the heap) of the node on which the sifting call is made. In other words, about half the calls to siftDown will have at most only one swap, then about a quarter of the calls will have at most two swaps, etc.\n\nThe heapsort algorithm itself has  time complexity using either version of heapify.\n\n  procedure heapify(a,count) is\n      (end is assigned the index of the first (left) child of the root)\n      end := 1\n      \n      while end  start\n          parent := iParent(child)\n          if a[parent] 2(N) − e2(N), where s2(N) is the number of 1 bits in the binary representation of N and e2(N) is number of trailing 0 bits.\n* The standard implementation of Floyd's heap-construction algorithm causes a large number of cache misses once the size of the data exceeds that of the CPU cache.  Much better performance on large data sets can be obtained by merging in depth-first order, combining subheaps as soon as possible, rather than combining all subheaps on one level before proceeding to the one above. [https://www.semanticscholar.org/paper/Performance-Engineering-Case-Study-Heap-Bojesen-Katajainen/6f4ada5912c1da64e16453d67ec99c970173fb5b Alternate PDF source].  See particularly Fig. 3.\n*Ternary heapsort\"Data Structures Using Pascal\", 1991, page 405, gives a ternary heapsort as a student exercise. \"Write a sorting routine similar to the heapsort except that it uses a ternary heap.\" uses a ternary heap instead of a binary heap; that is, each element in the heap has three children. It is more complicated to program, but does a constant number of times fewer swap and comparison operations.  This is because each sifting step in a ternary heap requires three comparisons and one swap, whereas in a binary heap two comparisons and one swap are required. Two levels in a ternary heap cover 9 elements, doing more work with the same number of comparisons as three levels in the binary heap, which only cover 8.\n*The smoothsort algorithm is a variation of heapsort developed by Edsger Dijkstra in 1981. Like heapsort, smoothsort's upper bound is Big O notation|. The advantage of smoothsort is that it comes closer to  time if the input is already sorted to some degree, whereas heapsort averages  regardless of the initial sorted state. Due to its complexity, smoothsort is rarely used.\n*Levcopoulos and Petersson. describe a variation of heapsort based on a Cartesian tree that does not add an element to the heap until smaller values on both sides of it have already been included in the sorted output. As they show, this modification can allow the algorithm to sort more quickly than  for inputs that are already nearly sorted.\n* Several variants such as weak heapsort require  comparisons in the worst case, close to the theoretical minimum, using one extra bit of state per node.  While this extra bit makes the algorithms not truly in-place, if space for it can be found inside the element, these algorithms are simple and efficient, but still slower than binary heaps if key comparisons are cheap enough (e.g. integer keys) that a constant factor does not matter.\n* Katajainen's \"ultimate heapsort\" requires no extra storage, performs  comparisons, and a similar number of element moves.  It is, however, even more complex and not justified unless comparisons are very expensive.\n\nBottom-up heapsort \n\nBottom-up heapsort was announced as beating quicksort (with median-of-three pivot selection) on arrays of size ≥16000. This version of heapsort keeps the linear-time heap-building phase, but changes the second phase, as follows.\n\nOrdinary heapsort extracts the top of the heap, , and fills the gap it leaves with , then sifts this latter element down the heap; but this element comes from the lowest level of the heap, meaning it is one of the smallest elements in the heap, so the sift-down will likely take many steps to move it back down. Each step of the sift-down requires two comparisons, to find the minimum of the new node and its two children.\n\nBottom-up heapsort instead finds the path of largest children to the leaf level of the tree (as if it were inserting −∞) using only one comparison per level.  Put another way, it finds the leaf which has the property that it and all of its ancestors are greater than their siblings.  (In the absence of equal keys, this leaf is unique.)  Then, from this leaf, it searches upward (using one comparison per level) for the correct position in that path to insert .  This is the same location as ordinary heapsort finds, and requires the same number of exchanges to perform the insert, but fewer comparisons are required to find that location.  Also available as \n\n function leafSearch(a, i, end) is\n     j ← i\n     while iLeftChild(j) ≤ end do\n         (Determine which of j's children is the greater)\n         if iRightChild(j) ≤ end and a[iRightChild(j)] > a[iLeftChild(j)] then\n             j ← iRightChild(j)\n         else\n             j ← iLeftChild(j)\n     return j\n\nThe return value of the leafSearch is used in the modified siftDown routine:\n\n procedure siftDown(a, i, end) is\n     j ← leafSearch(a, i, end)\n     while a[i] > a[j] do\n         j ← iParent(j)\n     x ← a[j]\n     a[j] ← a[i]\n     while j > i do\n         swap x, a[iParent(j)]\n         j ← iParent(j)\n\nBottom-up heapsort requires only  comparisons in the worst case and  on average.  For comparison, ordinary heapsort requires  comparisons worst-case and on average.\n\nA 2008 re-evaluation of this algorithm showed it to be no faster than ordinary heapsort for integer keys, though, presumably because modern branch prediction nullifies the cost of the predictable comparisons which bottom-up heapsort manages to avoid.  (It still has an advantage if comparisons are expensive.)\n\nA further refinement does a binary search in the path to the selected leaf, and sorts in a worst case of  comparisons, approaching the information-theoretic lower bound of  comparisons.\n\nA variant which uses two extra bits per internal node (n−1 bits total for an n-element heap) to cache information about which child is greater (two bits are required to store three cases: left, right, and unknown) uses less than  compares.\n\nComparison with other sorts \n\nHeapsort primarily competes with quicksort, another very efficient general purpose nearly-in-place comparison-based sort algorithm.\n\nQuicksort is typically somewhat faster due to some factors, but the worst-case running time for quicksort is , which is unacceptable for large data sets and can be deliberately triggered given enough knowledge of the implementation, creating a security risk. See quicksort for a detailed discussion of this problem and possible solutions.\n\nThus, because of the  upper bound on heapsort's running time and constant upper bound on its auxiliary storage, embedded systems with real-time constraints or systems concerned with security often use heapsort, such as the Linux kernel.https://github.com/torvalds/linux/blob/master/lib/sort.c Linux kernel source\n\nHeapsort also competes with merge sort, which has the same time bounds. Merge sort requires  auxiliary space, but heapsort requires only a constant amount. Heapsort typically runs faster in practice on machines with small or slow data caches, and does not require as much external memory. On the other hand, merge sort has several advantages over heapsort:\n* Merge sort on arrays has considerably better data cache performance, often outperforming heapsort on modern desktop computers because merge sort frequently accesses contiguous memory locations (good locality of reference); heapsort references are spread throughout the heap.\n* Heapsort is not a stable sort; merge sort is stable.\n* Merge sort parallelizes well and can achieve close to linear speedup with a trivial implementation; heapsort is not an obvious candidate for a parallel algorithm.\n* Merge sort can be adapted to operate on singly linked lists with  extra space. Heapsort can be adapted to operate on doubly linked lists with only  extra space overhead.\n* Merge sort is used in external sorting; heapsort is not. Locality of reference is the issue.\n\nIntrosort is an alternative to heapsort that combines quicksort and heapsort to retain advantages of both: worst case speed of heapsort and average speed of quicksort.\n\nExample \n\nLet { 6, 5, 3, 1, 8, 7, 2, 4 } be the list that we want to sort from the smallest to the largest. (NOTE, for 'Building the Heap' step: Larger nodes don't stay below smaller node parents. They are swapped with parents, and then recursively checked if another swap is needed, to keep larger numbers above smaller numbers on the heap binary tree.)\n\n1. Build the heap\n\n2. Sorting.\n\nNotes. Heapsort. http://en.wikipedia.org/?curid=13995."
  }
}
