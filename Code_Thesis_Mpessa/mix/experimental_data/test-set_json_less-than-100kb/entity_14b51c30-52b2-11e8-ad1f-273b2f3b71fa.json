{
  "datasourceIdentifier" : "awesome wiki export",
  "backlink" : "http://en.wikipedia.org/?curid=14463",
  "eid" : "14b51c30-52b2-11e8-ad1f-273b2f3b71fa",
  "loadTime" : 1525778543987,
  "textBody" : "In mathematics, the harmonic mean (sometimes called the subcontrary mean) is one of several kinds of average, and in particular one of the Pythagorean means. Typically, it is appropriate for situations when the average of rates is desired.\nThe harmonic mean can be expressed as the reciprocal of the arithmetic mean of the reciprocals of the given set of observations. As a simple example, the harmonic mean of 1, 4, and 4 is \\left(\\frac{1^{-1}+4^{-1}+4^{-1}}{3}\\right)^{-1} \\frac{3}{\\frac1{1}+\\frac1{4}+\\frac1{4}} \n \\frac{3}{1.5} = 2\\,.\n\nDefinition\n\nThe harmonic mean H of the positive real numbers \nx_1, x_2, \\ldots, x_n is defined to be\n\nH \\frac{n}{\\frac1{x_1} + \\frac1{x_2} + \\cdots + \\frac1{x_n}} \n \\frac{n}{\\sum\\limits_{i1}^n \\frac1{x_i}} \n \\left(\\frac{\\sum\\limits_{i=1}^n x_i^{-1}}{n}\\right)^{-1}.\n\nThe third formula in the above equation expresses the harmonic mean as the reciprocal of the arithmetic mean of the reciprocals.\n\nFrom the following formula:\n\nH \\frac{n\\cdot \\prod\\limits_{j\n1}^n x_j}{ \\sum\\limits_{i1}^n \\left\\{\\frac{1}{x_i}{\\prod\\limits_{j\n1}^n x_j}\\right\\}}.\n\nit is more apparent that the harmonic mean is related to the arithmetic and geometric means. It is the reciprocal dual of the arithmetic mean for positive inputs:\n\n1/H(1/x_1 \\ldots 1/x_n) = A(x_1 \\ldots x_n)\n\nThe harmonic mean is a Schur-concave function, and dominated by the minimum of its arguments, in the sense that for any positive set of arguments, \n\\min(x_1 \\ldots x_n) \\le H(x_1 \\ldots x_n) \\le n \\min(x_1 \\ldots x_n). Thus, the harmonic mean cannot be made arbitrarily large by changing some values to bigger ones (while having at least one value unchanged).\n\nRelationship with other means\n\nThe harmonic mean is one of the three Pythagorean means. For all positive data sets containing at least one pair of nonequal values, the harmonic mean is always the least of the three means,Da-Feng Xia, Sen-Lin Xu, and Feng Qi, \"A proof of the arithmetic mean-geometric mean-harmonic mean inequalities\", RGMIA Research Report Collection, vol. 2, no. 1, 1999, http://ajmaa.org/RGMIA/papers/v2n1/v2n1-10.pdf while the arithmetic mean is always the greatest of the three and the geometric mean is always in between. (If all values in a nonempty dataset are equal, the three means are always equal to one another; e.g., the harmonic, geometric, and arithmetic means of {2, 2, 2} are all 2.)\n\nIt is the special case M−1 of the power mean:\n\nH(x_1, x_2, \\ldots, x_n) M_{-1}(x_1, x_2, \\ldots, x_n) \n \\frac{n}{x_1^{-1} + x_2^{-1} + \\cdots + x_n^{-1}}\n\nSince the harmonic mean of a list of numbers tends strongly toward the least elements of the list, it tends (compared to the arithmetic mean) to mitigate the impact of large outliers and aggravate the impact of small ones.\n\nThe arithmetic mean is often mistakenly used in places calling for the harmonic mean.*Statistical Analysis, Ya-lun Chou, Holt International, 1969,  In the speed example below for instance, the arithmetic mean of 50 is incorrect, and too big.\n\nThe harmonic mean is related to the other Pythagorean means, as seen in the third formula in the above equation. This can be seen by interpreting the denominator to be the arithmetic mean of the product of numbers n times but each time omitting the j-th term. That is, for the first term, we multiply all n numbers except the first; for the second, we multiply all n numbers except the second; and so on. The numerator, excluding the n, which goes with the arithmetic mean, is the geometric mean to the power n. Thus the n-th harmonic mean is related to the n-th geometric and arithmetic means. The general formula is\n\nH(x_1, \\ldots , x_n)\\frac{(G(x_1, \\ldots , x_n))^n}{A(x_2x_3 \\cdots x_n, x_1x_3 \\cdots x_n, \\ldots , x_1x_2 \\cdots x_{n-1})} \n \\frac{(G(x_1, \\ldots , x_n))^n}{A\\left(\\frac{1}{x_1}{\\prod\\limits_{i1}^n x_i}, \\frac{1}{x_2}{\\prod\\limits_{i\n1}^n x_i}, \\ldots , \\frac{1}{x_n}{\\prod\\limits_{i=1}^n x_i}\\right)} .\n\nIf a set of non-identical numbers is subjected to a mean-preserving spread — that is, two or more elements of the set are \"spread apart\" from each other while leaving the arithmetic mean unchanged — then the harmonic mean always decreases.Mitchell, Douglas W., \"More on spreads and non-arithmetic means,\" The Mathematical Gazette 88, March 2004, 142–144.\n\nHarmonic mean of two or three numbers\n\nTwo numbers\n\nFor the special case of just two numbers, x_1 and x_2, the harmonic mean can be written\nH = \\frac{2x_1 x_2}{x_1 + x_2}.\n\nIn this special case, the harmonic mean is related to the arithmetic mean A \\frac{x_1 + x_2}{2} and the geometric mean G \n \\sqrt{x_1 x_2}, by\nH \\frac{G^2}{A}\nG\\cdot\\left(\\frac{G}{A}\\right).\n\nSince \\tfrac{G}{A}\\le1 by the inequality of arithmetic and geometric means, this shows for the n 2 case that H ≤ G (a property that in fact holds for all n). It also follows that G \n \\sqrt{AH}, meaning the two numbers' geometric mean equals the geometric mean of their arithmetic and harmonic means.\n\nThree numbers\n\nThree positive numbers H, G, and A are respectively the harmonic, geometric, and arithmetic means of three positive numbers if and only ifInequalities proposed in “Crux Mathematicorum”, [http://www.imomath.com/othercomp/Journ/ineq.pdf]. the following inequality holds\n\n\\frac{A^3}{G^3}+\\frac{G^3}{H^3}+1 \\le \\frac3{4} \\left(1+\\frac{A}{H}\\right)^2.\n\nWeighted harmonic mean\n\nIf a set of weights w_1, ..., w_n is associated to the dataset x_1, ..., x_n, the weighted harmonic mean is defined by\nH \\frac{\\sum\\limits_{i\n1}^n w_i}{\\sum\\limits_{i=1}^n \\frac{w_i}{x_i}}\n \\left( \\frac{\\sum\\limits_{i\n1}^n w_i x_i^{-1}}{\\sum\\limits_{i=1}^n w_i} \\right)^{-1}.\nThe unweighted harmonic mean can be regarded as the special case where all of the weights are equal.\n\nExamples\n\nIn physics\n\nIn certain situations, especially many situations involving rates and ratios, the harmonic mean provides the truest average. For instance, if a vehicle travels a certain distance d at a speed x (e.g., 60 kilometres per hour - ) and then the same distance again at a speed y (e.g., 40 ), then its average speed is the harmonic mean of x and y (48 ), and its total travel time is the same as if it had traveled the whole distance at that average speed. This can be proven as followshttps://learningpundits.com/module-view/48-averages/1-tips-on-averages/:\n\nAverage speed for the entire journey  = Total distance traveled/ Total time taken\n= 2d/ (d/x + d/y)\n= 2d/ [(yd + xd)/xy]\n= 2dxy/[d(x+y)]\n= 2xy/ x+y (Harmonic mean of x and y)       \n\nHowever, if the vehicle travels for a certain amount of time at a speed x and then the same amount of time at a speed y, then its average speed is the arithmetic mean of x and y, which in the above example is 50 kilometres per hour. The same principle applies to more than two segments: given a series of sub-trips at different speeds, if each sub-trip covers the same distance, then the average speed is the harmonic mean of all the sub-trip speeds; and if each sub-trip takes the same amount of time, then the average speed is the arithmetic mean of all the sub-trip speeds. (If neither is the case, then a weighted harmonic mean or weighted arithmetic mean is needed. For the arithmetic mean, the speed of each portion of the trip is weighted by the duration of that portion, while for the harmonic mean, the corresponding weight is the distance.  In both cases, the resulting formula reduces to dividing the total distance by the total time.)\n\nHowever one may avoid the use of the harmonic mean for the case of \"weighting by distance\". Pose the problem as finding \"slowness\" of the trip where \"slowness\" (in hours per kilometre) is the inverse of speed. When trip slowness is found, invert it so as to find the \"true\" average trip speed. For each trip segment i, the slowness si=1/speedi. Then take the weighted arithmetic mean of the si's weighted by their respective distances (optionally with the weights normalized so they sum to 1 by dividing them by trip length). This gives the true average slowness (in time per kilometre). It turns out that this procedure, which can be done with no knowledge of the harmonic mean, amounts to the same mathematical operations as one would use in solving this problem by using the harmonic mean. Thus it illustrates why the harmonic mean works in this case.\n\nSimilarly, if one wishes to estimate the density of an alloy given the densities of its constituent elements and their mass fractions (or, equivalently, percentages by mass), then the predicted density of the alloy (exclusive of typically minor volume changes due to atom packing effects) is the weighted harmonic mean of the individual densities, weighted by mass, rather than the weighted arithmetic mean as one might at first expect. To use the weighted arithmetic mean, the densities would have to be weighted by volume. Applying dimensional analysis to the problem while labelling the mass units by element and making sure that only like element-masses cancel, makes this clear.\n\nIf one connects two electrical resistors in parallel, one having resistance x (e.g., 60 Ω) and one having resistance y (e.g., 40 Ω), then the effect is the same as if one had used two resistors with the same resistance, both equal to the harmonic mean of x and y (48 Ω): the equivalent resistance, in either case, is 24 Ω (one-half of the harmonic mean). This same principle applies to capacitors in series or to inductors in parallel.\n\nHowever, if one connects the resistors in series, then the average resistance is the arithmetic mean of x and y (with total resistance equal to the sum of x and y). As with the previous example, the same principle applies when more than two resistors are connected, provided that all are in parallel or all are in series.\n\nThe \"conductivity effective mass\" of a semiconductor is also defined as the harmonic mean of the effective masses along the three crystallographic directions.http://ecee.colorado.edu/~bart/book/effmass.htm\n\nIn finance\n\nThe weighted harmonic mean is the preferable method for averaging multiples, such as the price–earnings ratio (P/E), in which price is in the numerator. If these ratios are averaged using a weighted arithmetic mean (a common error), high data points are given greater weights than low data points. The weighted harmonic mean, on the other hand, gives equal weight to each data point. The simple weighted arithmetic mean when applied to non-price normalized ratios such as the P/E is biased upwards and cannot be numerically justified, since it is based on equalized earnings; just as vehicles speeds cannot be averaged for a roundtrip journey.\n\nFor example, consider two firms, one with a market capitalization of $150 billion and earnings of $5 billion (P/E of 30) and one with a market capitalization of $1 billion and earnings of $1 million (P/E of 1000). Consider an index made of the two stocks, with 30% invested in the first and 70% invested in the second. We want to calculate the P/E ratio of this index.\n\nUsing the weighted arithmetic mean (incorrect):  P/E  0.3*30 + 0.7*1000 \n 709 \n\nUsing the weighted harmonic mean (correct):  P/E =  \\frac  {0.3+0.7}{0.3/30 + 0.7/1000} \\approx 93.46 \n\nThus, the correct P/E of 93.46 of this index can only be found using the weighted harmonic mean, while the weighted arithmetic mean will significantly overestimate it.\n\nIn geometry\n\nIn any triangle, the radius of the incircle is one-third of the harmonic mean of the altitudes.\n\nFor any point P on the minor arc BC of the circumcircle of an equilateral triangle ABC, with distances q and t from B and C respectively, and with the intersection of PA and BC being at a distance y from point P, we have that y is half the harmonic mean of q and t.\n\nIn a right triangle with legs a and b and altitude h from the hypotenuse to the right angle,  is half the harmonic mean of  and .Voles, Roger, \"Integer solutions of a^{-2}+b^{-2}=d^{-2},\" Mathematical Gazette 83, July 1999, 269–271.Richinick, Jennifer, \"The upside-down Pythagorean Theorem,\" Mathematical Gazette 92, July 2008, 313–;317.\n\nLet t and s (t > s) be the sides of the two inscribed squares in a right triangle with hypotenuse c. Then  equals half the harmonic mean of  and .\n\nLet a trapezoid have vertices A, B, C, and D in sequence and have parallel sides AB and CD. Let E be the intersection of the diagonals, and let F be on side DA and G be on side BC such that FEG is parallel to AB and CD. Then FG is the harmonic mean of AB and DC. (This is provable using similar triangles.)\n\nIn the crossed ladders problem, two ladders lie oppositely across an alley, each with feet at the base of one sidewall, with one leaning against a wall at height A and the other leaning against the opposite wall at height B, as shown. The ladders cross at a height of h above the alley floor. Then h is half the harmonic mean of A and B. This result still holds if the walls are slanted but still parallel and the \"heights\" A, B, and h are measured as distances from the floor along lines parallel to the walls.\n\nIn an ellipse, the semi-latus rectum (the distance from a focus to the ellipse along a line parallel to the minor axis) is the harmonic mean of the maximum and minimum distances of the ellipse from a focus.\n\nIn other sciences \n\nIn computer science, specifically information retrieval and machine learning, the harmonic mean of the precision (true positives per predicted positive) and the recall (true positives per real positive) is often used as an aggregated performance score for the evaluation of algorithms and systems: the F-score (or F-measure). This is used in information retrieval because only the positive class is of relevance, while number of negatives, in general, is large and unknown. It is thus a trade-off as to whether the correct positive predictions should be measured in relation to the number of predicted positives or the number of real positives, so it is measured versus a putative number of positives that is an arithmetic mean of the two possible denominators.\n\nAn interesting consequence arises from basic algebra in problems where people or systems work together. As an example, if a gas-powered pump can drain a pool in 4 hours and a battery-powered pump can drain the same pool in 6 hours, then it will take both pumps , which is equal to 2.4 hours, to drain the pool together. Interestingly, this is one-half of the harmonic mean of 6 and 4: . That is the appropriate average for the two types of pump is the harmonic mean, and with one pair of pumps (two pumps), it takes half this harmonic mean time, while with two pairs of pumps (four pumps) it would take a quarter of this harmonic mean time.\n\nIn hydrology, the harmonic mean is similarly used to average hydraulic conductivity values for a flow that is perpendicular to layers (e.g., geologic or soil) - flow parallel to layers uses the arithmetic mean. This apparent difference in averaging is explained by the fact that hydrology uses conductivity, which is the inverse of resistivity.\n\nIn sabermetrics, a player's Power–speed number is the harmonic mean of their home run and stolen base totals.\n\nIn population genetics, the harmonic mean is used when calculating the effects of fluctuations in generation size on the effective breeding population. This is to take into account the fact that a very small generation is effectively like a bottleneck and means that a very small number of individuals are contributing disproportionately to the gene pool, which can result in higher levels of inbreeding.\n\nWhen considering fuel economy in automobiles two measures are commonly used – miles per gallon (mpg), and litres per 100 km. As the dimensions of these quantities are the inverse of each other (one is distance per volume, the other volume per distance) when taking the mean value of the fuel economy of a range of cars one measure will produce the harmonic mean of the other – i.e., converting the mean value of fuel economy expressed in litres per 100 km to miles per gallon will produce the harmonic mean of the fuel economy expressed in miles-per-gallon.\n\nIn chemistry and nuclear physics the average mass per particle of a mixture consisting of different species (e.g., molecules or isotopes) is given by the harmonic mean of the individual species' masses weighted by their respective mass fraction.\n\nBeta distribution\n\n \n\n \n\n \n\n \n\n \nThe harmonic mean of a beta distribution with shape parameters α and β is:\n \n H = \\frac{ \\alpha - 1 }{ \\alpha + \\beta - 1 } \\text{ conditional on } \\alpha > 1 \\, \\, \\& \\, \\, \\beta > 0 \n \nThe harmonic mean with α  H = \\frac{ \\alpha - 1 }{ 2 \\alpha -1 }\n \nshowing that for α β the harmonic mean ranges from 0 for α \n β 1, to 1/2 for α \n β → ∞.\n \nThe following are the limits with one parameter finite (non zero) and the other parameter approaching these limits:\n \n \\lim_{ \\alpha \\to 0 } H = \\text{ undefined } \n \n \\lim_{ \\alpha \\to 1 } H \\lim_{ \\beta \\to \\infty } H  \n 0 \n \n \\lim_{ \\beta \\to 0 } H \\lim_{ \\alpha \\to \\infty } H \n 1 \n \nWith the geometric mean the harmonic mean may be useful in maximum likelihood estimation in the four parameter case.\n\nA second harmonic mean (H1 - X) also exists for this distribution\n \nH_{ 1 - X } =  \\frac{ \\beta - 1 }{ \\alpha + \\beta - 1 } \\text{ conditional on } \\beta > 1 \\, \\, \\& \\, \\, \\alpha > 0\n \nThis harmonic mean with β  H_{ 1 - X } = \\frac{ \\beta - 1 }{ 2 \\beta - 1 } \n \nshowing that for α β the harmonic mean ranges from 0, for α \n β 1, to 1/2, for α \n β → ∞.\n \nThe following are the limits with one parameter finite (non zero) and the other approaching these limits:\n \n \\lim_{ \\beta \\to 0 } H_{ 1 - X } = \\text{ undefined } \n \n \\lim_{ \\beta\\to 1} H_{ 1 - X } \\lim_{ \\alpha \\to \\infty } H_{ 1 - X } \n 0 \n \n \\lim_{ \\alpha \\to 0} H_{ 1 - X } \\lim_{ \\beta \\to \\infty } H_{ 1 - X } \n 1 \n \nAlthough both harmonic means are asymmetric, when α = β the two means are equal.\n\nLognormal distribution\n\n \nThe harmonic mean ( H ) of a lognormal distribution isAitchison J, Brown JAC (1969). The lognormal distribution with special reference to its uses in economics. Cambridge University Press, New York\n\n H = \\exp \\left( \\mu -\\frac{ 1 }{ 2 } \\sigma^2 \\right) ,\n\nwhere μ is the arithmetic mean and σ2 is the variance of the distribution.\n \nThe harmonic and arithmetic means are related by\n \n \\frac{ \\mu }{ H } = 1 + C_v \\, ,\n\nwhere Cv is the coefficient of variation.\n \nThe geometric ( G ), arithmetic and harmonic means are related byRossman LA (1990) Design stream flows based on harmonic means. J Hydr Eng ASCE 116(7) 946–950\n\n H \\mu = G^2 .\n\nPareto distribution\n\n \nThe harmonic mean of type 1 Pareto distribution isJohnson NL, Kotz S, Balakrishnan N (1994) Continuous univariate distributions Vol 1. Wiley Series in Probability and Statistics.\n \n H = k \\left( 1 + \\frac{ 1 }{ \\alpha } \\right) \n \nwhere k is the scale parameter and α is the shape parameter.\n\nStatistics\n\n \nFor a random sample, the harmonic mean is calculated as above. Both the mean and the variance may be infinite (if it includes at least one term of the form 1/0).\n\nSample distributions of mean and variance\n\n \nThe mean of the sample m is  asymptotically distributed normally with variance s2.\n \n s^2 =  \\frac  { m [ \\operatorname{E}( 1 / x - 1 ) ] }{ m^2 n } \n\nThe variance of the mean itself isZelen M (1972) Length-biased sampling and biomedical problems. In: Biometric Society Meeting, Dallas, Texas\n \n \\operatorname{Var}\\left( \\frac { 1 } { x } \\right) = \\frac { m \\left[ \\operatorname{E}( 1 / x - 1 ) \\right] } { n m^2 } \n \nwhere m is the arithmetic mean of the reciprocals, x are the variates, n is the population size and E is the expectation operator. \n\nDelta method\n\n \nAssuming that the variance is not infinite and that the central limit theorem applies to the sample than using the delta method, the variance is\n \n \\operatorname{ Var }( H ) = \\frac { 1 }{ n }\\frac{ s^2 } { m^4 } \n \nwhere H is the harmonic mean, m is the arithmetic mean of the reciprocals\n \n m = \\frac{ 1 } { n } \\sum{ \\frac{ 1 } { x } } .\n \ns2 is the variance of the reciprocals of the data\n \n s^2 = \\operatorname{Var}\\left( \\frac { 1 } { x } \\right) \n \nand n is the number of data points in the sample.\n \nJackknife method\n\n \nA jackknife method of estimating the variance is possible if the mean is known.Lam FC (1985) Estimate of variance for harmonic mean half lives. J Pharm Sci 74(2) 229-231 This method is the usual 'delete 1' rather than the 'delete m' version.\n \nThis method first requires the computation of the mean of the sample (m)\n \n m = \\frac{ n }{ \\sum { \\frac{ 1 }{ x } } }\n \nwhere x are the sample values.\n \nA series of value wi is then computed where\n \n w_i = \\frac{ n - 1 }{ \\sum_{j \\neq i} { \\frac{ 1 }{ x } } }.\n \nThe mean (h) of the wi is then taken:\n \n h = \\frac{ 1 }{ n } \\sum{ w_i } \n \nThe variance of the mean is\n \n \\frac{ n - 1 }{ n } \\sum{ ( m - h ) }^2  .\n \nSignificance testing and confidence intervals for the mean can then be estimated with the t test.\n \nSize biased sampling\n\n \nAssume a random variate has a distribution f( x ). Assume also that the likelihood of a variate being chosen is proportional to its value. This is known as length based or size biased sampling.\n \nLet μ be the mean of the population. Then the probability density function f*( x ) of the size biased population is\n \n f^*(x) = \\frac{ x f( x ) }{ \\mu } \n \nThe expectation of this length biased distribution E*( x ) is\n \n \\operatorname{E}^*( x ) = \\mu \\left[ 1 + \\frac{ \\sigma^2 }{ \\mu^2 } \\right] \n \nwhere σ2 is the variance.\n \nThe expectation of the harmonic mean is the same as the non-length biased version E( x )\n \n \\operatorname{E}^*\\left( \\frac{ 1 }{ x } \\right) = \\operatorname{E}\\left( \\frac{ 1 }{ x } \\right) \n \nThe problem of length biased sampling arises in a number of areas including textile manufactureCox DR (1969) Some sampling problems in technology. In: New developments in survey sampling. U.L. Johnson, H Smith eds. New York: Wiley Interscience pedigree analysisDavidov O, Zelen M (2001) Referent sampling, family history and relative risk: the role of length‐biased sampling. Biostat 2(2): 173-181 doi: 10.1093/biostatistics/2.2.173 and survival analysisZelen M, Feinleib M (1969) On the theory of screening for chronic diseases. Biometrika 56: 601-614\n \nAkman et al have developed a test for the detection of length based bias in samples.Akman O, Gamage J, Jannot J, Juliano S, Thurman A, Whitman D (2007) A simple test for detection of length-biased sampling. J Biostats 1 (2) 189-195\n \nShifted variables\n\n \nIf X is a positive random variable and q > 0 then for all ε > 0Chuen-Teck See, Chen J (2008) Convex functions of random variables. J Inequal Pure Appl Math 9 (3) Art 80\n \n \\operatorname{Var}\\left[ \\frac{ 1 }{( X + \\epsilon )^q } \\right] \n \nMoments\n\n \nAssuming that X and E(X) are > 0 then\n \n \\operatorname{E}\\left[ \\frac{ 1 }{ X } \\right] \\ge \\frac{ 1 }{ \\operatorname{E}( X ) }\n \nThis follows from Jensen's inequality.\n \nGurland has shown thatGurland J (1967) An inequality satisfied by the expectation of the reciprocal of a random variable. The American Statistician. 21 (2) 24 for a distribution that takes only positive values, for any n > 0\n \n \\operatorname{E}( X^{ -1 } ) \\ge \\frac{ \\operatorname{E}( X^{ n - 1 } ) }{ \\operatorname{E}( X^n ) } .\n \nUnder some conditionsSung SH (2010) On inverse moments for a class of nonnegative random variables. J Inequal Applic doi:10.1155/2010/823767\n \n \\operatorname{E}( a + X )^{ -n }  \\sim \\operatorname{E}( a + X^{ -n } ) \n \nwhere ~ means approximately.\n\nSampling properties\n\nAssuming that the variates (x) are drawn from a lognormal distribution there are several possible estimators for H:\n\n H_1 = \\frac{ n }{ \\sum( \\frac{ 1 }{ x } ) } \n \n H_2 = \\frac{ [ \\exp( \\frac{ 1 }{ n } \\sum \\log_e( x ) ) ]^2 }{ \\frac{ 1 }{ n } \\sum(  x  ) } \n \n H_3 = \\exp \\left( m - \\frac{ 1 }{ 2 } s^2 \\right) \n \nwhere\n\n m = \\frac{ 1 }{ n } \\sum \\log_e( x )\n \n s^2 = \\frac{ 1 }{ n } \\sum ( \\log_e( x ) - m )^2 \n \nOf these H3 is probably the best estimator for samples of 25 or more.Stedinger JR (1980) Fitting lognormal distributions to hydrologic data. Water Resour Res 16(3) 481–490\n \nBias and variance estimators\n\n \nA first order approximation to the bias and variance of H1 areLimbrunner JF, Vogel RM, Brown LC (2000) Estimation of harmonic mean of a lognormal variable. J Hydrol Eng 5(1) 59-66 [http://engineering.tufts.edu/cee/people/vogel/publications/estimation-harmonic.pdf]\n \n \\operatorname{bias}[ H_1 ] = \\frac{ H C_v }{ n }\n \n \\operatorname{Var}[ H_1 ] = \\frac{ H^2 C_v }{ n }\n \nwhere Cv is the coefficient of variation.\n \nSimilarly a first order approximation to the bias and variance of H3 are\n \n \\frac{ H \\log_e( 1 + C_v ) }{ 2n } \\left[ 1 + \\frac{ 1 + C_v^2 }{ 2 } \\right] \n \n \\frac{ H \\log_e( 1 + C_v ) }{ n } \\left[ 1 + \\frac{ 1 + C_v^2 }{ 4 } \\right] \n \nIt has been found in numerical experiments that H3 is generally a superior estimator of the harmonic mean than H1. H2 produces estimates that are largely similar to H1.\n\nNotes\n\n \nThe Environmental Protection Agency recommend the use of the harmonic mean in setting maximum toxin levels in water.EPA (1991) Technical support document for water quality-based toxics control. EPA/505/2-90-001. Office of Water\n \nIn geophysical reservoir engineering studies, the harmonic mean is widely used.Muskat M (1937) The flow of homogeneous fluids through porous media. McGraw-Hill, New York",
  "entityProperties" : [ {
    "name" : "title",
    "type" : "String",
    "values" : [ "Harmonic mean" ],
    "synthetic" : false
  }, {
    "name" : "url",
    "type" : "String",
    "values" : [ "http://en.wikipedia.org/?curid=14463" ],
    "synthetic" : false
  } ],
  "classifications" : [ "xml-export" ],
  "technicalAttributes" : {
    "technicalAttributes" : null,
    "aggregatedText" : "In mathematics, the harmonic mean (sometimes called the subcontrary mean) is one of several kinds of average, and in particular one of the Pythagorean means. Typically, it is appropriate for situations when the average of rates is desired.\nThe harmonic mean can be expressed as the reciprocal of the arithmetic mean of the reciprocals of the given set of observations. As a simple example, the harmonic mean of 1, 4, and 4 is \\left(\\frac{1^{-1}+4^{-1}+4^{-1}}{3}\\right)^{-1} \\frac{3}{\\frac1{1}+\\frac1{4}+\\frac1{4}} \n \\frac{3}{1.5} = 2\\,.\n\nDefinition\n\nThe harmonic mean H of the positive real numbers \nx_1, x_2, \\ldots, x_n is defined to be\n\nH \\frac{n}{\\frac1{x_1} + \\frac1{x_2} + \\cdots + \\frac1{x_n}} \n \\frac{n}{\\sum\\limits_{i1}^n \\frac1{x_i}} \n \\left(\\frac{\\sum\\limits_{i=1}^n x_i^{-1}}{n}\\right)^{-1}.\n\nThe third formula in the above equation expresses the harmonic mean as the reciprocal of the arithmetic mean of the reciprocals.\n\nFrom the following formula:\n\nH \\frac{n\\cdot \\prod\\limits_{j\n1}^n x_j}{ \\sum\\limits_{i1}^n \\left\\{\\frac{1}{x_i}{\\prod\\limits_{j\n1}^n x_j}\\right\\}}.\n\nit is more apparent that the harmonic mean is related to the arithmetic and geometric means. It is the reciprocal dual of the arithmetic mean for positive inputs:\n\n1/H(1/x_1 \\ldots 1/x_n) = A(x_1 \\ldots x_n)\n\nThe harmonic mean is a Schur-concave function, and dominated by the minimum of its arguments, in the sense that for any positive set of arguments, \n\\min(x_1 \\ldots x_n) \\le H(x_1 \\ldots x_n) \\le n \\min(x_1 \\ldots x_n). Thus, the harmonic mean cannot be made arbitrarily large by changing some values to bigger ones (while having at least one value unchanged).\n\nRelationship with other means\n\nThe harmonic mean is one of the three Pythagorean means. For all positive data sets containing at least one pair of nonequal values, the harmonic mean is always the least of the three means,Da-Feng Xia, Sen-Lin Xu, and Feng Qi, \"A proof of the arithmetic mean-geometric mean-harmonic mean inequalities\", RGMIA Research Report Collection, vol. 2, no. 1, 1999, http://ajmaa.org/RGMIA/papers/v2n1/v2n1-10.pdf while the arithmetic mean is always the greatest of the three and the geometric mean is always in between. (If all values in a nonempty dataset are equal, the three means are always equal to one another; e.g., the harmonic, geometric, and arithmetic means of {2, 2, 2} are all 2.)\n\nIt is the special case M−1 of the power mean:\n\nH(x_1, x_2, \\ldots, x_n) M_{-1}(x_1, x_2, \\ldots, x_n) \n \\frac{n}{x_1^{-1} + x_2^{-1} + \\cdots + x_n^{-1}}\n\nSince the harmonic mean of a list of numbers tends strongly toward the least elements of the list, it tends (compared to the arithmetic mean) to mitigate the impact of large outliers and aggravate the impact of small ones.\n\nThe arithmetic mean is often mistakenly used in places calling for the harmonic mean.*Statistical Analysis, Ya-lun Chou, Holt International, 1969,  In the speed example below for instance, the arithmetic mean of 50 is incorrect, and too big.\n\nThe harmonic mean is related to the other Pythagorean means, as seen in the third formula in the above equation. This can be seen by interpreting the denominator to be the arithmetic mean of the product of numbers n times but each time omitting the j-th term. That is, for the first term, we multiply all n numbers except the first; for the second, we multiply all n numbers except the second; and so on. The numerator, excluding the n, which goes with the arithmetic mean, is the geometric mean to the power n. Thus the n-th harmonic mean is related to the n-th geometric and arithmetic means. The general formula is\n\nH(x_1, \\ldots , x_n)\\frac{(G(x_1, \\ldots , x_n))^n}{A(x_2x_3 \\cdots x_n, x_1x_3 \\cdots x_n, \\ldots , x_1x_2 \\cdots x_{n-1})} \n \\frac{(G(x_1, \\ldots , x_n))^n}{A\\left(\\frac{1}{x_1}{\\prod\\limits_{i1}^n x_i}, \\frac{1}{x_2}{\\prod\\limits_{i\n1}^n x_i}, \\ldots , \\frac{1}{x_n}{\\prod\\limits_{i=1}^n x_i}\\right)} .\n\nIf a set of non-identical numbers is subjected to a mean-preserving spread — that is, two or more elements of the set are \"spread apart\" from each other while leaving the arithmetic mean unchanged — then the harmonic mean always decreases.Mitchell, Douglas W., \"More on spreads and non-arithmetic means,\" The Mathematical Gazette 88, March 2004, 142–144.\n\nHarmonic mean of two or three numbers\n\nTwo numbers\n\nFor the special case of just two numbers, x_1 and x_2, the harmonic mean can be written\nH = \\frac{2x_1 x_2}{x_1 + x_2}.\n\nIn this special case, the harmonic mean is related to the arithmetic mean A \\frac{x_1 + x_2}{2} and the geometric mean G \n \\sqrt{x_1 x_2}, by\nH \\frac{G^2}{A}\nG\\cdot\\left(\\frac{G}{A}\\right).\n\nSince \\tfrac{G}{A}\\le1 by the inequality of arithmetic and geometric means, this shows for the n 2 case that H ≤ G (a property that in fact holds for all n). It also follows that G \n \\sqrt{AH}, meaning the two numbers' geometric mean equals the geometric mean of their arithmetic and harmonic means.\n\nThree numbers\n\nThree positive numbers H, G, and A are respectively the harmonic, geometric, and arithmetic means of three positive numbers if and only ifInequalities proposed in “Crux Mathematicorum”, [http://www.imomath.com/othercomp/Journ/ineq.pdf]. the following inequality holds\n\n\\frac{A^3}{G^3}+\\frac{G^3}{H^3}+1 \\le \\frac3{4} \\left(1+\\frac{A}{H}\\right)^2.\n\nWeighted harmonic mean\n\nIf a set of weights w_1, ..., w_n is associated to the dataset x_1, ..., x_n, the weighted harmonic mean is defined by\nH \\frac{\\sum\\limits_{i\n1}^n w_i}{\\sum\\limits_{i=1}^n \\frac{w_i}{x_i}}\n \\left( \\frac{\\sum\\limits_{i\n1}^n w_i x_i^{-1}}{\\sum\\limits_{i=1}^n w_i} \\right)^{-1}.\nThe unweighted harmonic mean can be regarded as the special case where all of the weights are equal.\n\nExamples\n\nIn physics\n\nIn certain situations, especially many situations involving rates and ratios, the harmonic mean provides the truest average. For instance, if a vehicle travels a certain distance d at a speed x (e.g., 60 kilometres per hour - ) and then the same distance again at a speed y (e.g., 40 ), then its average speed is the harmonic mean of x and y (48 ), and its total travel time is the same as if it had traveled the whole distance at that average speed. This can be proven as followshttps://learningpundits.com/module-view/48-averages/1-tips-on-averages/:\n\nAverage speed for the entire journey  = Total distance traveled/ Total time taken\n= 2d/ (d/x + d/y)\n= 2d/ [(yd + xd)/xy]\n= 2dxy/[d(x+y)]\n= 2xy/ x+y (Harmonic mean of x and y)       \n\nHowever, if the vehicle travels for a certain amount of time at a speed x and then the same amount of time at a speed y, then its average speed is the arithmetic mean of x and y, which in the above example is 50 kilometres per hour. The same principle applies to more than two segments: given a series of sub-trips at different speeds, if each sub-trip covers the same distance, then the average speed is the harmonic mean of all the sub-trip speeds; and if each sub-trip takes the same amount of time, then the average speed is the arithmetic mean of all the sub-trip speeds. (If neither is the case, then a weighted harmonic mean or weighted arithmetic mean is needed. For the arithmetic mean, the speed of each portion of the trip is weighted by the duration of that portion, while for the harmonic mean, the corresponding weight is the distance.  In both cases, the resulting formula reduces to dividing the total distance by the total time.)\n\nHowever one may avoid the use of the harmonic mean for the case of \"weighting by distance\". Pose the problem as finding \"slowness\" of the trip where \"slowness\" (in hours per kilometre) is the inverse of speed. When trip slowness is found, invert it so as to find the \"true\" average trip speed. For each trip segment i, the slowness si=1/speedi. Then take the weighted arithmetic mean of the si's weighted by their respective distances (optionally with the weights normalized so they sum to 1 by dividing them by trip length). This gives the true average slowness (in time per kilometre). It turns out that this procedure, which can be done with no knowledge of the harmonic mean, amounts to the same mathematical operations as one would use in solving this problem by using the harmonic mean. Thus it illustrates why the harmonic mean works in this case.\n\nSimilarly, if one wishes to estimate the density of an alloy given the densities of its constituent elements and their mass fractions (or, equivalently, percentages by mass), then the predicted density of the alloy (exclusive of typically minor volume changes due to atom packing effects) is the weighted harmonic mean of the individual densities, weighted by mass, rather than the weighted arithmetic mean as one might at first expect. To use the weighted arithmetic mean, the densities would have to be weighted by volume. Applying dimensional analysis to the problem while labelling the mass units by element and making sure that only like element-masses cancel, makes this clear.\n\nIf one connects two electrical resistors in parallel, one having resistance x (e.g., 60 Ω) and one having resistance y (e.g., 40 Ω), then the effect is the same as if one had used two resistors with the same resistance, both equal to the harmonic mean of x and y (48 Ω): the equivalent resistance, in either case, is 24 Ω (one-half of the harmonic mean). This same principle applies to capacitors in series or to inductors in parallel.\n\nHowever, if one connects the resistors in series, then the average resistance is the arithmetic mean of x and y (with total resistance equal to the sum of x and y). As with the previous example, the same principle applies when more than two resistors are connected, provided that all are in parallel or all are in series.\n\nThe \"conductivity effective mass\" of a semiconductor is also defined as the harmonic mean of the effective masses along the three crystallographic directions.http://ecee.colorado.edu/~bart/book/effmass.htm\n\nIn finance\n\nThe weighted harmonic mean is the preferable method for averaging multiples, such as the price–earnings ratio (P/E), in which price is in the numerator. If these ratios are averaged using a weighted arithmetic mean (a common error), high data points are given greater weights than low data points. The weighted harmonic mean, on the other hand, gives equal weight to each data point. The simple weighted arithmetic mean when applied to non-price normalized ratios such as the P/E is biased upwards and cannot be numerically justified, since it is based on equalized earnings; just as vehicles speeds cannot be averaged for a roundtrip journey.\n\nFor example, consider two firms, one with a market capitalization of $150 billion and earnings of $5 billion (P/E of 30) and one with a market capitalization of $1 billion and earnings of $1 million (P/E of 1000). Consider an index made of the two stocks, with 30% invested in the first and 70% invested in the second. We want to calculate the P/E ratio of this index.\n\nUsing the weighted arithmetic mean (incorrect):  P/E  0.3*30 + 0.7*1000 \n 709 \n\nUsing the weighted harmonic mean (correct):  P/E =  \\frac  {0.3+0.7}{0.3/30 + 0.7/1000} \\approx 93.46 \n\nThus, the correct P/E of 93.46 of this index can only be found using the weighted harmonic mean, while the weighted arithmetic mean will significantly overestimate it.\n\nIn geometry\n\nIn any triangle, the radius of the incircle is one-third of the harmonic mean of the altitudes.\n\nFor any point P on the minor arc BC of the circumcircle of an equilateral triangle ABC, with distances q and t from B and C respectively, and with the intersection of PA and BC being at a distance y from point P, we have that y is half the harmonic mean of q and t.\n\nIn a right triangle with legs a and b and altitude h from the hypotenuse to the right angle,  is half the harmonic mean of  and .Voles, Roger, \"Integer solutions of a^{-2}+b^{-2}=d^{-2},\" Mathematical Gazette 83, July 1999, 269–271.Richinick, Jennifer, \"The upside-down Pythagorean Theorem,\" Mathematical Gazette 92, July 2008, 313–;317.\n\nLet t and s (t > s) be the sides of the two inscribed squares in a right triangle with hypotenuse c. Then  equals half the harmonic mean of  and .\n\nLet a trapezoid have vertices A, B, C, and D in sequence and have parallel sides AB and CD. Let E be the intersection of the diagonals, and let F be on side DA and G be on side BC such that FEG is parallel to AB and CD. Then FG is the harmonic mean of AB and DC. (This is provable using similar triangles.)\n\nIn the crossed ladders problem, two ladders lie oppositely across an alley, each with feet at the base of one sidewall, with one leaning against a wall at height A and the other leaning against the opposite wall at height B, as shown. The ladders cross at a height of h above the alley floor. Then h is half the harmonic mean of A and B. This result still holds if the walls are slanted but still parallel and the \"heights\" A, B, and h are measured as distances from the floor along lines parallel to the walls.\n\nIn an ellipse, the semi-latus rectum (the distance from a focus to the ellipse along a line parallel to the minor axis) is the harmonic mean of the maximum and minimum distances of the ellipse from a focus.\n\nIn other sciences \n\nIn computer science, specifically information retrieval and machine learning, the harmonic mean of the precision (true positives per predicted positive) and the recall (true positives per real positive) is often used as an aggregated performance score for the evaluation of algorithms and systems: the F-score (or F-measure). This is used in information retrieval because only the positive class is of relevance, while number of negatives, in general, is large and unknown. It is thus a trade-off as to whether the correct positive predictions should be measured in relation to the number of predicted positives or the number of real positives, so it is measured versus a putative number of positives that is an arithmetic mean of the two possible denominators.\n\nAn interesting consequence arises from basic algebra in problems where people or systems work together. As an example, if a gas-powered pump can drain a pool in 4 hours and a battery-powered pump can drain the same pool in 6 hours, then it will take both pumps , which is equal to 2.4 hours, to drain the pool together. Interestingly, this is one-half of the harmonic mean of 6 and 4: . That is the appropriate average for the two types of pump is the harmonic mean, and with one pair of pumps (two pumps), it takes half this harmonic mean time, while with two pairs of pumps (four pumps) it would take a quarter of this harmonic mean time.\n\nIn hydrology, the harmonic mean is similarly used to average hydraulic conductivity values for a flow that is perpendicular to layers (e.g., geologic or soil) - flow parallel to layers uses the arithmetic mean. This apparent difference in averaging is explained by the fact that hydrology uses conductivity, which is the inverse of resistivity.\n\nIn sabermetrics, a player's Power–speed number is the harmonic mean of their home run and stolen base totals.\n\nIn population genetics, the harmonic mean is used when calculating the effects of fluctuations in generation size on the effective breeding population. This is to take into account the fact that a very small generation is effectively like a bottleneck and means that a very small number of individuals are contributing disproportionately to the gene pool, which can result in higher levels of inbreeding.\n\nWhen considering fuel economy in automobiles two measures are commonly used – miles per gallon (mpg), and litres per 100 km. As the dimensions of these quantities are the inverse of each other (one is distance per volume, the other volume per distance) when taking the mean value of the fuel economy of a range of cars one measure will produce the harmonic mean of the other – i.e., converting the mean value of fuel economy expressed in litres per 100 km to miles per gallon will produce the harmonic mean of the fuel economy expressed in miles-per-gallon.\n\nIn chemistry and nuclear physics the average mass per particle of a mixture consisting of different species (e.g., molecules or isotopes) is given by the harmonic mean of the individual species' masses weighted by their respective mass fraction.\n\nBeta distribution\n\n \n\n \n\n \n\n \n\n \nThe harmonic mean of a beta distribution with shape parameters α and β is:\n \n H = \\frac{ \\alpha - 1 }{ \\alpha + \\beta - 1 } \\text{ conditional on } \\alpha > 1 \\, \\, \\& \\, \\, \\beta > 0 \n \nThe harmonic mean with α  H = \\frac{ \\alpha - 1 }{ 2 \\alpha -1 }\n \nshowing that for α β the harmonic mean ranges from 0 for α \n β 1, to 1/2 for α \n β → ∞.\n \nThe following are the limits with one parameter finite (non zero) and the other parameter approaching these limits:\n \n \\lim_{ \\alpha \\to 0 } H = \\text{ undefined } \n \n \\lim_{ \\alpha \\to 1 } H \\lim_{ \\beta \\to \\infty } H  \n 0 \n \n \\lim_{ \\beta \\to 0 } H \\lim_{ \\alpha \\to \\infty } H \n 1 \n \nWith the geometric mean the harmonic mean may be useful in maximum likelihood estimation in the four parameter case.\n\nA second harmonic mean (H1 - X) also exists for this distribution\n \nH_{ 1 - X } =  \\frac{ \\beta - 1 }{ \\alpha + \\beta - 1 } \\text{ conditional on } \\beta > 1 \\, \\, \\& \\, \\, \\alpha > 0\n \nThis harmonic mean with β  H_{ 1 - X } = \\frac{ \\beta - 1 }{ 2 \\beta - 1 } \n \nshowing that for α β the harmonic mean ranges from 0, for α \n β 1, to 1/2, for α \n β → ∞.\n \nThe following are the limits with one parameter finite (non zero) and the other approaching these limits:\n \n \\lim_{ \\beta \\to 0 } H_{ 1 - X } = \\text{ undefined } \n \n \\lim_{ \\beta\\to 1} H_{ 1 - X } \\lim_{ \\alpha \\to \\infty } H_{ 1 - X } \n 0 \n \n \\lim_{ \\alpha \\to 0} H_{ 1 - X } \\lim_{ \\beta \\to \\infty } H_{ 1 - X } \n 1 \n \nAlthough both harmonic means are asymmetric, when α = β the two means are equal.\n\nLognormal distribution\n\n \nThe harmonic mean ( H ) of a lognormal distribution isAitchison J, Brown JAC (1969). The lognormal distribution with special reference to its uses in economics. Cambridge University Press, New York\n\n H = \\exp \\left( \\mu -\\frac{ 1 }{ 2 } \\sigma^2 \\right) ,\n\nwhere μ is the arithmetic mean and σ2 is the variance of the distribution.\n \nThe harmonic and arithmetic means are related by\n \n \\frac{ \\mu }{ H } = 1 + C_v \\, ,\n\nwhere Cv is the coefficient of variation.\n \nThe geometric ( G ), arithmetic and harmonic means are related byRossman LA (1990) Design stream flows based on harmonic means. J Hydr Eng ASCE 116(7) 946–950\n\n H \\mu = G^2 .\n\nPareto distribution\n\n \nThe harmonic mean of type 1 Pareto distribution isJohnson NL, Kotz S, Balakrishnan N (1994) Continuous univariate distributions Vol 1. Wiley Series in Probability and Statistics.\n \n H = k \\left( 1 + \\frac{ 1 }{ \\alpha } \\right) \n \nwhere k is the scale parameter and α is the shape parameter.\n\nStatistics\n\n \nFor a random sample, the harmonic mean is calculated as above. Both the mean and the variance may be infinite (if it includes at least one term of the form 1/0).\n\nSample distributions of mean and variance\n\n \nThe mean of the sample m is  asymptotically distributed normally with variance s2.\n \n s^2 =  \\frac  { m [ \\operatorname{E}( 1 / x - 1 ) ] }{ m^2 n } \n\nThe variance of the mean itself isZelen M (1972) Length-biased sampling and biomedical problems. In: Biometric Society Meeting, Dallas, Texas\n \n \\operatorname{Var}\\left( \\frac { 1 } { x } \\right) = \\frac { m \\left[ \\operatorname{E}( 1 / x - 1 ) \\right] } { n m^2 } \n \nwhere m is the arithmetic mean of the reciprocals, x are the variates, n is the population size and E is the expectation operator. \n\nDelta method\n\n \nAssuming that the variance is not infinite and that the central limit theorem applies to the sample than using the delta method, the variance is\n \n \\operatorname{ Var }( H ) = \\frac { 1 }{ n }\\frac{ s^2 } { m^4 } \n \nwhere H is the harmonic mean, m is the arithmetic mean of the reciprocals\n \n m = \\frac{ 1 } { n } \\sum{ \\frac{ 1 } { x } } .\n \ns2 is the variance of the reciprocals of the data\n \n s^2 = \\operatorname{Var}\\left( \\frac { 1 } { x } \\right) \n \nand n is the number of data points in the sample.\n \nJackknife method\n\n \nA jackknife method of estimating the variance is possible if the mean is known.Lam FC (1985) Estimate of variance for harmonic mean half lives. J Pharm Sci 74(2) 229-231 This method is the usual 'delete 1' rather than the 'delete m' version.\n \nThis method first requires the computation of the mean of the sample (m)\n \n m = \\frac{ n }{ \\sum { \\frac{ 1 }{ x } } }\n \nwhere x are the sample values.\n \nA series of value wi is then computed where\n \n w_i = \\frac{ n - 1 }{ \\sum_{j \\neq i} { \\frac{ 1 }{ x } } }.\n \nThe mean (h) of the wi is then taken:\n \n h = \\frac{ 1 }{ n } \\sum{ w_i } \n \nThe variance of the mean is\n \n \\frac{ n - 1 }{ n } \\sum{ ( m - h ) }^2  .\n \nSignificance testing and confidence intervals for the mean can then be estimated with the t test.\n \nSize biased sampling\n\n \nAssume a random variate has a distribution f( x ). Assume also that the likelihood of a variate being chosen is proportional to its value. This is known as length based or size biased sampling.\n \nLet μ be the mean of the population. Then the probability density function f*( x ) of the size biased population is\n \n f^*(x) = \\frac{ x f( x ) }{ \\mu } \n \nThe expectation of this length biased distribution E*( x ) is\n \n \\operatorname{E}^*( x ) = \\mu \\left[ 1 + \\frac{ \\sigma^2 }{ \\mu^2 } \\right] \n \nwhere σ2 is the variance.\n \nThe expectation of the harmonic mean is the same as the non-length biased version E( x )\n \n \\operatorname{E}^*\\left( \\frac{ 1 }{ x } \\right) = \\operatorname{E}\\left( \\frac{ 1 }{ x } \\right) \n \nThe problem of length biased sampling arises in a number of areas including textile manufactureCox DR (1969) Some sampling problems in technology. In: New developments in survey sampling. U.L. Johnson, H Smith eds. New York: Wiley Interscience pedigree analysisDavidov O, Zelen M (2001) Referent sampling, family history and relative risk: the role of length‐biased sampling. Biostat 2(2): 173-181 doi: 10.1093/biostatistics/2.2.173 and survival analysisZelen M, Feinleib M (1969) On the theory of screening for chronic diseases. Biometrika 56: 601-614\n \nAkman et al have developed a test for the detection of length based bias in samples.Akman O, Gamage J, Jannot J, Juliano S, Thurman A, Whitman D (2007) A simple test for detection of length-biased sampling. J Biostats 1 (2) 189-195\n \nShifted variables\n\n \nIf X is a positive random variable and q > 0 then for all ε > 0Chuen-Teck See, Chen J (2008) Convex functions of random variables. J Inequal Pure Appl Math 9 (3) Art 80\n \n \\operatorname{Var}\\left[ \\frac{ 1 }{( X + \\epsilon )^q } \\right] \n \nMoments\n\n \nAssuming that X and E(X) are > 0 then\n \n \\operatorname{E}\\left[ \\frac{ 1 }{ X } \\right] \\ge \\frac{ 1 }{ \\operatorname{E}( X ) }\n \nThis follows from Jensen's inequality.\n \nGurland has shown thatGurland J (1967) An inequality satisfied by the expectation of the reciprocal of a random variable. The American Statistician. 21 (2) 24 for a distribution that takes only positive values, for any n > 0\n \n \\operatorname{E}( X^{ -1 } ) \\ge \\frac{ \\operatorname{E}( X^{ n - 1 } ) }{ \\operatorname{E}( X^n ) } .\n \nUnder some conditionsSung SH (2010) On inverse moments for a class of nonnegative random variables. J Inequal Applic doi:10.1155/2010/823767\n \n \\operatorname{E}( a + X )^{ -n }  \\sim \\operatorname{E}( a + X^{ -n } ) \n \nwhere ~ means approximately.\n\nSampling properties\n\nAssuming that the variates (x) are drawn from a lognormal distribution there are several possible estimators for H:\n\n H_1 = \\frac{ n }{ \\sum( \\frac{ 1 }{ x } ) } \n \n H_2 = \\frac{ [ \\exp( \\frac{ 1 }{ n } \\sum \\log_e( x ) ) ]^2 }{ \\frac{ 1 }{ n } \\sum(  x  ) } \n \n H_3 = \\exp \\left( m - \\frac{ 1 }{ 2 } s^2 \\right) \n \nwhere\n\n m = \\frac{ 1 }{ n } \\sum \\log_e( x )\n \n s^2 = \\frac{ 1 }{ n } \\sum ( \\log_e( x ) - m )^2 \n \nOf these H3 is probably the best estimator for samples of 25 or more.Stedinger JR (1980) Fitting lognormal distributions to hydrologic data. Water Resour Res 16(3) 481–490\n \nBias and variance estimators\n\n \nA first order approximation to the bias and variance of H1 areLimbrunner JF, Vogel RM, Brown LC (2000) Estimation of harmonic mean of a lognormal variable. J Hydrol Eng 5(1) 59-66 [http://engineering.tufts.edu/cee/people/vogel/publications/estimation-harmonic.pdf]\n \n \\operatorname{bias}[ H_1 ] = \\frac{ H C_v }{ n }\n \n \\operatorname{Var}[ H_1 ] = \\frac{ H^2 C_v }{ n }\n \nwhere Cv is the coefficient of variation.\n \nSimilarly a first order approximation to the bias and variance of H3 are\n \n \\frac{ H \\log_e( 1 + C_v ) }{ 2n } \\left[ 1 + \\frac{ 1 + C_v^2 }{ 2 } \\right] \n \n \\frac{ H \\log_e( 1 + C_v ) }{ n } \\left[ 1 + \\frac{ 1 + C_v^2 }{ 4 } \\right] \n \nIt has been found in numerical experiments that H3 is generally a superior estimator of the harmonic mean than H1. H2 produces estimates that are largely similar to H1.\n\nNotes\n\n \nThe Environmental Protection Agency recommend the use of the harmonic mean in setting maximum toxin levels in water.EPA (1991) Technical support document for water quality-based toxics control. EPA/505/2-90-001. Office of Water\n \nIn geophysical reservoir engineering studies, the harmonic mean is widely used.Muskat M (1937) The flow of homogeneous fluids through porous media. McGraw-Hill, New York. Harmonic mean. http://en.wikipedia.org/?curid=14463."
  }
}
