{
  "datasourceIdentifier" : "awesome wiki export",
  "backlink" : "http://en.wikipedia.org/?curid=17905",
  "eid" : "2f400060-52b2-11e8-ad1f-273b2f3b71fa",
  "loadTime" : 1525778588518,
  "textBody" : "In statistics,\nthe likelihood principle is that, given a statistical model, all the evidence in a sample relevant to model parameters is contained in the likelihood function.\n\nA likelihood function arises from a probability density function considered as a function of its distributional parameterization argument. For example, consider a model which gives the probability density function ƒX(x | θ) of observable random variable X as a function of a parameter θ. Then for a specific value x of X, the function \\mathcal L(θ | x) = ƒX(x | θ) is a likelihood function of θ: it gives a measure of how \"likely\" any particular value of θ is, if we know that X has the value x. The density function may be a density with respect to counting measure, i.e. a probability mass function.\n\nTwo likelihood functions are equivalent if one is a scalar multiple of the other. The likelihood principle is this: all information from the data that is relevant to inferences about the value of the model parameters is in the equivalence class to which the likelihood function belongs. The strong likelihood principle applies this same criterion to cases such as sequential experiments where the sample of data that is available results from applying a stopping rule to the observations earlier in the experiment.Dodge, Y. (2003) The Oxford Dictionary of Statistical Terms. OUP. \n\nExample\n\nSuppose\n\n*X is the number of successes in twelve independent Bernoulli trials with probability θ of success on each trial, and\n*Y is the number of independent Bernoulli trials needed to get three successes, again with probability θ (= 1/2 for a coin-toss) of success on each trial.\n\nThen the observation that X = 3 induces the likelihood function\n\n\\mathcal L(\\theta  \\mid X3) \n \\binom{12} 3 \\theta^3(1-\\theta)^9 = 220\\theta^3(1-\\theta)^9, \n\nwhile the observation that Y = 12 induces the likelihood function\n\n\\mathcal L(\\theta \\mid Y12) \n \\binom{11} 2 \\theta^3(1-\\theta)^9 = 55\\theta^3(1-\\theta)^9.\n\nThe likelihood principle says that, as the data are the same in both cases, the inferences drawn about the value of θ should also be the same. In addition, all the inferential content in the data about the value of θ is contained in the two likelihoods, and is the same if they are proportional to one another. This is the case in the above example, reflecting the fact that the difference between observing X 3 and observing Y \n 12 lies not in the actual data, but merely in the design of the experiment. Specifically, in one case, one has decided in advance to try twelve times; in the other, to keep trying until three successes are observed.  The inference about θ should be the same, and this is reflected in the fact that the two likelihoods are proportional to each other.\n\nThis is not always the case, however. The use of frequentist methods involving p-values leads to different inferences for the two cases above, showing that the outcome of frequentist methods depends on the experimental procedure, and thus violates the likelihood principle.\n\nThe law of likelihood\n\nA related concept is the law of likelihood, the notion that the extent to which the evidence supports one parameter value or hypothesis against another is equal to the ratio of their likelihoods.\nThat is,\n\\Lambda {\\mathcal L(a\\mid X\nx) \\over \\mathcal L(b\\mid Xx)} \n {P(Xx\\mid a) \\over P(X\nx\\mid b)}\nis the degree to which the observation x supports parameter value or hypothesis a against b. If this ratio is 1, the evidence is indifferent; if greater than 1, the evidence supports the value a against b; or if less, then vice versa.  The use of Bayes factors can extend this by taking account of the complexity of different hypotheses.\n\nCombining the likelihood principle with the law of likelihood yields the consequence that the parameter value which maximizes the likelihood function is the value which is most strongly supported by the evidence. This is the basis for the widely used method of maximum likelihood.\n\nHistory \n\nThe likelihood principle was first identified by that name in print in 1962\n(Barnard et al., Birnbaum, and Savage et al.),\nbut arguments for the same principle, unnamed, and the use of the principle in applications goes back to the works of R.A. Fisher in the 1920s. \nThe law of likelihood was identified by that name by I. Hacking (1965).\nMore recently the likelihood principle as a general principle of inference has been championed by A. W. F. Edwards. The likelihood principle has been applied to the philosophy of science by R. Royall.Royall, Richard (1997) Statistical Evidence: A likelihood paradigm. Chapman and Hall, Boca Raton. \n\nBirnbaum proved that the likelihood principle follows from two more primitive and seemingly reasonable principles, the conditionality principle and the sufficiency principle. The conditionality principle says that if an experiment is chosen by a random process independent of the states of nature \\theta, then only the experiment actually performed is relevant to inferences about \\theta. The sufficiency principle says that if T(X) is a sufficient statistic for \\theta, and if in two experiments with data x_1 and x_2 we have  T(x_1)=T(x_2), then the evidence about \\theta given by the two experiments is the same.\n\nArguments for and against \n\nSome widely used methods of conventional statistics, for example many significance tests, are not consistent with the likelihood principle.\n\nLet us briefly consider some of the arguments for and against the likelihood principle.\n\nThe original Birnbaum argument\n\nBirnbaum's proof of the likelihood principle has been disputed by philosophers of science, including Deborah MayoMayo, D. (2010) [http://www.phil.vt.edu/dmayo/personal_website/ch%207%20mayo%20birnbaum%20proof.pdf \"An Error in the Argument from Conditionality and Sufficiency to the Likelihood Principle\"] in Error and Inference: Recent Exchanges on Experimental Reasoning, Reliability and the Objectivity and Rationality of Science (D Mayo and A. Spanos eds.), Cambridge: Cambridge University Press: 305-314.Mayo, Deborah (2014), \"[https://projecteuclid.org/euclid.ss/1408368565 On the Birnbaum Argument for the Strong Likelihood Principle]\", Statistical Science, 29: 227-266 (with Discussion). and statisticians including Michael Evans.Evans, Michael (2013) [https://arxiv.org/abs/1302.5468 What does the proof of Birnbaum's theorem prove?] On the other hand, a new proof of the likelihood principle has been provided by Greg Gandenberger.Gandenberger, Greg (2014), \"A new proof of the likelihood principle\", British Journal for the Philosophy of Science, 66: 475-503; .\n\nExperimental design arguments on the likelihood principle\n\nUnrealized events play a role in some common statistical methods. For example, the result of a significance test depends on the p-value, the probability of a result as extreme or more extreme than the observation, and that probability may depend on the design of the experiment. To the extent that the likelihood principle is accepted, such methods are therefore denied.\n\nSome classical significance tests are not based on the likelihood.  A commonly cited example is the optional stopping problem. Suppose I tell you that I tossed a coin 12 times and in the process observed 3 heads. You might make some inference about the probability of heads and whether the coin was fair. Suppose now I tell that I tossed the coin until I observed 3 heads, and I tossed it 12 times. Will you now make some different inference?\n\nThe likelihood function is the same in both cases: it is proportional to\np^3 (1-p)^9.\n\nAccording to the likelihood principle, the inference should be the same in either case.\n\nSuppose a number of scientists are assessing the probability of a certain outcome (which we shall call 'success') in experimental trials. Conventional wisdom suggests that if there is no bias towards success or failure then the success probability would be one half.  Adam, a scientist, conducted 12 trials and obtains 3 successes and 9 failures.  Then he left the lab.\n\nBill, a colleague in the same lab, continued Adam's  work and published Adam's results, along with a significance test. He tested the null hypothesis that p, the success probability, is equal to a half, versus p 0 is true, is\n\n\\left({12 \\choose 9}+{12 \\choose 10}+{12 \\choose 11}+{12 \\choose 12}\\right)\\left({1 \\over 2}\\right)^{12}\n\nwhich is 299/4096 = 7.3%. Thus the null hypothesis is not rejected at the 5% significance level.\n\nCharlotte, another scientist, reads Bill's paper and writes a letter, saying that it is possible that Adam kept trying until he obtained 3 successes, in which case the probability of needing to conduct 12 or more experiments is given by\n\n1-\\left({10 \\choose 2}\\left({1 \\over 2}\\right)^{11}+{9 \\choose 2}\\left({1 \\over 2} \\right)^{10}+\\cdots +{2 \\choose 2}\\left({1 \\over 2}\\right)^3 \\right)\n\nwhich is 134/4096 = 3.27%. Now the result is statistically significant at the 5% level. Note that there is no contradiction among these two results; both computations are correct.\n\nTo these scientists, whether a result is significant or not depends on the design of the experiment, not on the likelihood (in the sense of the likelihood function) of the parameter value being 1/2.\n\nResults of this kind are considered by some as arguments against the likelihood principle. For others it exemplifies the value of the likelihood principle and is an argument against significance tests.\n\nSimilar themes appear when comparing Fisher's exact test with Pearson's chi-squared test.\n\nThe voltmeter story\n\nAn argument in favor of the likelihood principle is given by Edwards in his book Likelihood. He cites the following story from J.W. Pratt, slightly condensed here. Note that the likelihood function depends only on what actually happened, and not on what could have happened.\n\nAn engineer draws a random sample of electron tubes and measures their voltage. The measurements range from 75 to 99 volts. A statistician computes the sample mean and a confidence interval for the true mean. Later the statistician discovers that the voltmeter reads only as far as 100, so the population appears to be 'censored'. This necessitates a new analysis, if the statistician is orthodox. However, the engineer says he has another meter reading to 1000 volts, which he would have used if any voltage had been over 100. This is a relief to the statistician, because it means the population was effectively uncensored after all. But, the next day the engineer informs the statistician that this second meter was not working at the time of the measuring. The statistician ascertains that the engineer would not have held up the measurements until the meter was fixed, and informs him that new measurements are required. The engineer is astounded. \"Next you'll be asking about my oscilloscope\".\n\nThis story can be translated to Adam's stopping rule above, as follows. Adam stopped immediately after 3 successes, because his boss Bill had instructed him to do so. Adam did not die. After the publication of the statistical analysis by Bill, Adam discovers that he has missed a second instruction from Bill to conduct 12 trials instead, and that Bill's paper is based on this second instruction. Adam is very glad that he got his 3 successes after exactly 12 trials, and explains to his friend Charlotte that by coincidence he executed the second instruction. Later, he is astonished to hear about Charlotte's letter explaining that now the result is significant.",
  "entityProperties" : [ {
    "name" : "title",
    "type" : "String",
    "values" : [ "Likelihood principle" ],
    "synthetic" : false
  }, {
    "name" : "url",
    "type" : "String",
    "values" : [ "http://en.wikipedia.org/?curid=17905" ],
    "synthetic" : false
  } ],
  "classifications" : [ "xml-export" ],
  "technicalAttributes" : {
    "technicalAttributes" : null,
    "aggregatedText" : "In statistics,\nthe likelihood principle is that, given a statistical model, all the evidence in a sample relevant to model parameters is contained in the likelihood function.\n\nA likelihood function arises from a probability density function considered as a function of its distributional parameterization argument. For example, consider a model which gives the probability density function ƒX(x | θ) of observable random variable X as a function of a parameter θ. Then for a specific value x of X, the function \\mathcal L(θ | x) = ƒX(x | θ) is a likelihood function of θ: it gives a measure of how \"likely\" any particular value of θ is, if we know that X has the value x. The density function may be a density with respect to counting measure, i.e. a probability mass function.\n\nTwo likelihood functions are equivalent if one is a scalar multiple of the other. The likelihood principle is this: all information from the data that is relevant to inferences about the value of the model parameters is in the equivalence class to which the likelihood function belongs. The strong likelihood principle applies this same criterion to cases such as sequential experiments where the sample of data that is available results from applying a stopping rule to the observations earlier in the experiment.Dodge, Y. (2003) The Oxford Dictionary of Statistical Terms. OUP. \n\nExample\n\nSuppose\n\n*X is the number of successes in twelve independent Bernoulli trials with probability θ of success on each trial, and\n*Y is the number of independent Bernoulli trials needed to get three successes, again with probability θ (= 1/2 for a coin-toss) of success on each trial.\n\nThen the observation that X = 3 induces the likelihood function\n\n\\mathcal L(\\theta  \\mid X3) \n \\binom{12} 3 \\theta^3(1-\\theta)^9 = 220\\theta^3(1-\\theta)^9, \n\nwhile the observation that Y = 12 induces the likelihood function\n\n\\mathcal L(\\theta \\mid Y12) \n \\binom{11} 2 \\theta^3(1-\\theta)^9 = 55\\theta^3(1-\\theta)^9.\n\nThe likelihood principle says that, as the data are the same in both cases, the inferences drawn about the value of θ should also be the same. In addition, all the inferential content in the data about the value of θ is contained in the two likelihoods, and is the same if they are proportional to one another. This is the case in the above example, reflecting the fact that the difference between observing X 3 and observing Y \n 12 lies not in the actual data, but merely in the design of the experiment. Specifically, in one case, one has decided in advance to try twelve times; in the other, to keep trying until three successes are observed.  The inference about θ should be the same, and this is reflected in the fact that the two likelihoods are proportional to each other.\n\nThis is not always the case, however. The use of frequentist methods involving p-values leads to different inferences for the two cases above, showing that the outcome of frequentist methods depends on the experimental procedure, and thus violates the likelihood principle.\n\nThe law of likelihood\n\nA related concept is the law of likelihood, the notion that the extent to which the evidence supports one parameter value or hypothesis against another is equal to the ratio of their likelihoods.\nThat is,\n\\Lambda {\\mathcal L(a\\mid X\nx) \\over \\mathcal L(b\\mid Xx)} \n {P(Xx\\mid a) \\over P(X\nx\\mid b)}\nis the degree to which the observation x supports parameter value or hypothesis a against b. If this ratio is 1, the evidence is indifferent; if greater than 1, the evidence supports the value a against b; or if less, then vice versa.  The use of Bayes factors can extend this by taking account of the complexity of different hypotheses.\n\nCombining the likelihood principle with the law of likelihood yields the consequence that the parameter value which maximizes the likelihood function is the value which is most strongly supported by the evidence. This is the basis for the widely used method of maximum likelihood.\n\nHistory \n\nThe likelihood principle was first identified by that name in print in 1962\n(Barnard et al., Birnbaum, and Savage et al.),\nbut arguments for the same principle, unnamed, and the use of the principle in applications goes back to the works of R.A. Fisher in the 1920s. \nThe law of likelihood was identified by that name by I. Hacking (1965).\nMore recently the likelihood principle as a general principle of inference has been championed by A. W. F. Edwards. The likelihood principle has been applied to the philosophy of science by R. Royall.Royall, Richard (1997) Statistical Evidence: A likelihood paradigm. Chapman and Hall, Boca Raton. \n\nBirnbaum proved that the likelihood principle follows from two more primitive and seemingly reasonable principles, the conditionality principle and the sufficiency principle. The conditionality principle says that if an experiment is chosen by a random process independent of the states of nature \\theta, then only the experiment actually performed is relevant to inferences about \\theta. The sufficiency principle says that if T(X) is a sufficient statistic for \\theta, and if in two experiments with data x_1 and x_2 we have  T(x_1)=T(x_2), then the evidence about \\theta given by the two experiments is the same.\n\nArguments for and against \n\nSome widely used methods of conventional statistics, for example many significance tests, are not consistent with the likelihood principle.\n\nLet us briefly consider some of the arguments for and against the likelihood principle.\n\nThe original Birnbaum argument\n\nBirnbaum's proof of the likelihood principle has been disputed by philosophers of science, including Deborah MayoMayo, D. (2010) [http://www.phil.vt.edu/dmayo/personal_website/ch%207%20mayo%20birnbaum%20proof.pdf \"An Error in the Argument from Conditionality and Sufficiency to the Likelihood Principle\"] in Error and Inference: Recent Exchanges on Experimental Reasoning, Reliability and the Objectivity and Rationality of Science (D Mayo and A. Spanos eds.), Cambridge: Cambridge University Press: 305-314.Mayo, Deborah (2014), \"[https://projecteuclid.org/euclid.ss/1408368565 On the Birnbaum Argument for the Strong Likelihood Principle]\", Statistical Science, 29: 227-266 (with Discussion). and statisticians including Michael Evans.Evans, Michael (2013) [https://arxiv.org/abs/1302.5468 What does the proof of Birnbaum's theorem prove?] On the other hand, a new proof of the likelihood principle has been provided by Greg Gandenberger.Gandenberger, Greg (2014), \"A new proof of the likelihood principle\", British Journal for the Philosophy of Science, 66: 475-503; .\n\nExperimental design arguments on the likelihood principle\n\nUnrealized events play a role in some common statistical methods. For example, the result of a significance test depends on the p-value, the probability of a result as extreme or more extreme than the observation, and that probability may depend on the design of the experiment. To the extent that the likelihood principle is accepted, such methods are therefore denied.\n\nSome classical significance tests are not based on the likelihood.  A commonly cited example is the optional stopping problem. Suppose I tell you that I tossed a coin 12 times and in the process observed 3 heads. You might make some inference about the probability of heads and whether the coin was fair. Suppose now I tell that I tossed the coin until I observed 3 heads, and I tossed it 12 times. Will you now make some different inference?\n\nThe likelihood function is the same in both cases: it is proportional to\np^3 (1-p)^9.\n\nAccording to the likelihood principle, the inference should be the same in either case.\n\nSuppose a number of scientists are assessing the probability of a certain outcome (which we shall call 'success') in experimental trials. Conventional wisdom suggests that if there is no bias towards success or failure then the success probability would be one half.  Adam, a scientist, conducted 12 trials and obtains 3 successes and 9 failures.  Then he left the lab.\n\nBill, a colleague in the same lab, continued Adam's  work and published Adam's results, along with a significance test. He tested the null hypothesis that p, the success probability, is equal to a half, versus p 0 is true, is\n\n\\left({12 \\choose 9}+{12 \\choose 10}+{12 \\choose 11}+{12 \\choose 12}\\right)\\left({1 \\over 2}\\right)^{12}\n\nwhich is 299/4096 = 7.3%. Thus the null hypothesis is not rejected at the 5% significance level.\n\nCharlotte, another scientist, reads Bill's paper and writes a letter, saying that it is possible that Adam kept trying until he obtained 3 successes, in which case the probability of needing to conduct 12 or more experiments is given by\n\n1-\\left({10 \\choose 2}\\left({1 \\over 2}\\right)^{11}+{9 \\choose 2}\\left({1 \\over 2} \\right)^{10}+\\cdots +{2 \\choose 2}\\left({1 \\over 2}\\right)^3 \\right)\n\nwhich is 134/4096 = 3.27%. Now the result is statistically significant at the 5% level. Note that there is no contradiction among these two results; both computations are correct.\n\nTo these scientists, whether a result is significant or not depends on the design of the experiment, not on the likelihood (in the sense of the likelihood function) of the parameter value being 1/2.\n\nResults of this kind are considered by some as arguments against the likelihood principle. For others it exemplifies the value of the likelihood principle and is an argument against significance tests.\n\nSimilar themes appear when comparing Fisher's exact test with Pearson's chi-squared test.\n\nThe voltmeter story\n\nAn argument in favor of the likelihood principle is given by Edwards in his book Likelihood. He cites the following story from J.W. Pratt, slightly condensed here. Note that the likelihood function depends only on what actually happened, and not on what could have happened.\n\nAn engineer draws a random sample of electron tubes and measures their voltage. The measurements range from 75 to 99 volts. A statistician computes the sample mean and a confidence interval for the true mean. Later the statistician discovers that the voltmeter reads only as far as 100, so the population appears to be 'censored'. This necessitates a new analysis, if the statistician is orthodox. However, the engineer says he has another meter reading to 1000 volts, which he would have used if any voltage had been over 100. This is a relief to the statistician, because it means the population was effectively uncensored after all. But, the next day the engineer informs the statistician that this second meter was not working at the time of the measuring. The statistician ascertains that the engineer would not have held up the measurements until the meter was fixed, and informs him that new measurements are required. The engineer is astounded. \"Next you'll be asking about my oscilloscope\".\n\nThis story can be translated to Adam's stopping rule above, as follows. Adam stopped immediately after 3 successes, because his boss Bill had instructed him to do so. Adam did not die. After the publication of the statistical analysis by Bill, Adam discovers that he has missed a second instruction from Bill to conduct 12 trials instead, and that Bill's paper is based on this second instruction. Adam is very glad that he got his 3 successes after exactly 12 trials, and explains to his friend Charlotte that by coincidence he executed the second instruction. Later, he is astonished to hear about Charlotte's letter explaining that now the result is significant. Likelihood principle. http://en.wikipedia.org/?curid=17905."
  }
}
