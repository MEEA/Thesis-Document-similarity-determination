{
  "datasourceIdentifier" : "awesome wiki export",
  "backlink" : "http://en.wikipedia.org/?curid=13266",
  "eid" : "07937240-52b2-11e8-ad1f-273b2f3b71fa",
  "loadTime" : 1525778521956,
  "textBody" : "A histogram is an accurate representation of the distribution of numerical data. It is an estimate of the probability distribution of a continuous variable (quantitative variable) and was first introduced by Karl Pearson. It is a kind of bar graph. To construct a histogram, the first step is to \"bin\" the range of values—that is, divide the entire range of values into a series of intervals—and then count how many values fall into each interval.  The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are often (but are not required to be) of equal size.\n\nIf the bins are of equal size, a rectangle is erected over the bin with height proportional to the frequency—the number of cases in each bin. A histogram may also be normalized to display \"relative\" frequencies. It then shows the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1.\n\nHowever, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. The vertical axis is then not the frequency but frequency density—the number of cases per unit of the variable on the horizontal axis. Examples of variable bin width are displayed on Census bureau data below.\n\nAs the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous.Charles Stangor (2011) \"Research Methods For The Behavioral Sciences\". Wadsworth, Cengage Learning. .\n\nHistograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot.\n\nA histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes.\n\nAnother alternative is the average shifted histogram,\nwhich is fast to compute and gives a smooth curve estimate of the density without using kernels.\n\nThe histogram is one of the seven basic tools of quality control.\n\nHistograms are sometimes confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.\n\nEtymology\n\nThe etymology of the word histogram is uncertain. Sometimes it is said to be derived from the Ancient Greek  (histos) – \"anything set upright\" (as the masts of a ship, the bar of a loom, or the vertical bars of a histogram);  and  (gramma) – \"drawing, record, writing\". It is also said that Karl Pearson, who introduced the term in 1891, derived the name from \"historical diagram\".\n\nExamples\n\nThis is the data for the histogram to the right, using 500 items:\n\nThe words used to describe the patterns in a histogram are: \"symmetric\", \"skewed left\" or \"right\", \"unimodal\", \"bimodal\" or \"multimodal\".\n\nSymmetric-histogram.png|Symmetric, unimodal\nSkewed-right.png|Skewed right\nSkewed-left.png|Skewed left\nBimodal-histogram.png|Bimodal\nMultimodal.png|Multimodal\nSymmetric2.png|Symmetric\n\nIt is a good idea to plot the data using several different bin widths to learn more about it. Here is an example on tips given in a restaurant.\n\nTips-histogram1.png|Tips using a $1 bin width, skewed right, unimodal\nTips-histogram2.png|Tips using a 10c bin width, still skewed right, multimodal with modes at $ and 50c amounts, indicates rounding, also some outliers\n\nHere are a couple more examples:\n\nHousingprice.png|Prices of houses sold in Ames in 2009 exhibits some right skew\nTennis-aces.png|Aces by players in a grand slam tennis tournament, facetted by gender. There are more aces in the men's game.\n\nThe U.S. Census Bureau found that there were 124 million people who work outside of their homes.[https://www.census.gov/prod/2004pubs/c2kbr-33.pdf US 2000 census].  Using their data on the time occupied by travel to work, the table below shows the absolute number of people who responded with travel times \"at least 30 but less than 35 minutes\" is higher than the numbers for the categories above and below it. This is likely due to people rounding their reported journey time. The problem of reporting values as somewhat arbitrarily rounded numbers is a common phenomenon when collecting data from people.\n\nThis histogram shows the number of cases per unit interval as the height of each block, so that the area of each block is equal to the number of people in the survey who fall into its category. The area under the curve represents the total number of cases (124 million). This type of histogram shows absolute numbers, with Q in thousands.\n\nThis histogram differs from the first only in the vertical scale.  The area of each block is the fraction of the total that each category represents, and the total area of all the bars is equal to 1 (the fraction meaning \"all\"). The curve displayed is a simple density estimate. This version shows proportions, and is also known as a unit area histogram.\n\nIn other words, a histogram represents a frequency distribution by means of rectangles whose widths represent class intervals and whose areas are proportional to the corresponding frequencies: the height of each is the average frequency density for the interval. The intervals are placed together in order to show that the data represented by the histogram, while exclusive, is also contiguous. (E.g., in a histogram it is possible to have two connecting intervals of 10.5–20.5 and 20.5–33.5, but not two connecting intervals of 10.5–20.5 and 22.5–32.5.  Empty intervals are represented as empty and not skipped.)Dean, S., & Illowsky, B. (2009, February 19). Descriptive Statistics: Histogram. Retrieved from the Connexions Web site: http://cnx.org/content/m16298/1.11/\n\nMathematical definition\n\nIn a more general mathematical sense, a histogram is a function mi that counts the number of observations that fall into each of the disjoint categories (known as bins), whereas the graph of a histogram is merely one way to represent a histogram. Thus, if we let n be the total number of observations and k be the total number of bins, the histogram mi meets the following conditions:\n\nn \\sum_{i\n1}^k{m_i}.\n\nCumulative histogram\n\nA cumulative histogram is a mapping that counts the cumulative number of observations in all of the bins up to the specified bin. That is, the cumulative histogram Mi of a histogram mj is defined as:\n\nM_i \\sum_{j\n1}^i{m_j}.\n\nNumber of bins and width\n\nThere is no \"best\" number of bins, and different bin sizes can reveal different features of the data.  Grouping data is at least as old as Graunt's work in the 17th century, but no systematic guidelines were given until Sturges's work in 1926.\n\nUsing wider bins where the density of the underlying data points is low reduces noise due to sampling randomness; using narrower bins where the density is high (so the signal drowns the noise) gives greater precision to the density estimation.  Thus varying the bin-width within a histogram can be beneficial.  Nonetheless, equal-width bins are widely used.\n\nSome theoreticians have attempted to determine an optimal number of bins, but these methods generally make strong assumptions about the shape of the distribution.  Depending on the actual data distribution and the goals of the analysis, different bin widths may be appropriate, so experimentation is usually needed to determine an appropriate width. There are, however, various useful guidelines and rules of thumb.e.g. § 5.6 \"Density Estimation\", W. N. Venables and B. D. Ripley, Modern Applied Statistics with S (2002), Springer, 4th edition. .\n\nThe number of bins k can be assigned directly or can be calculated from a suggested bin width h as:\nk = \\left \\lceil \\frac{\\max x - \\min x}{h} \\right \\rceil.\n\nThe braces indicate the ceiling function.\n\nSquare-root choice \n\nk = \\sqrt{n}, \\, \n\nwhich takes the square root of the number of data points in the sample (used by Excel histograms and many others).\n\nSturges' formula \n\nSturges' formula is derived from a binomial distribution and implicitly assumes an approximately normal distribution.\n\nk = \\lceil \\log_2 n \\rceil+ 1 , \\, \n\nIt implicitly bases the bin sizes on the range of the data and can perform poorly if n k = \\lceil 2 n^{1/3}\\rceil,\n\nThe Rice Rule Online Statistics Education: A Multimedia Course of Study (http://onlinestatbook.com/). Project Leader: David M. Lane, Rice University (chapter 2 \"Graphing Distributions\", section \"Histograms\") is presented as a simple alternative to Sturges's rule.\n\nDoane's formula \n\nDoane's formulaDoane DP (1976) Aesthetic frequency classification. American Statistician, 30: 181–183 is a modification of Sturges' formula which attempts to improve its performance with non-normal data.\n\n k = 1 + \\log_2( n ) + \\log_2 \\left( 1 +  \\frac { |g_1| }{\\sigma_{g_1}} \\right) \n\nwhere g_1 is the estimated 3rd-moment-skewness of the distribution and\n\n \\sigma_{g_1} = \\sqrt { \\frac { 6(n-2) }{ (n+1)(n+3) } }  \n\nScott's normal reference rule \n\nh = \\frac{3.5 \\hat \\sigma}{n^{1/3}},\n\nwhere \\hat \\sigma is the sample standard deviation. Scott's normal reference rule is optimal for random samples of normally distributed data, in the sense that it minimizes the integrated mean squared error of the density estimate.\n\nFreedman–Diaconis' choice \n\nThe Freedman–Diaconis rule is:\n\nh = 2 \\frac{\\operatorname{IQR}(x)}{n^{1/3}},\n\nwhich is based on the interquartile range, denoted by IQR. It replaces 3.5σ of Scott's rule with 2 IQR, which is less sensitive than the standard deviation to outliers in data.\n\n Minimizing cross-validation estimated squared error  \n\nThis approach of minimizing integrated mean squared error from Scott's rule can be generalized beyond normal distributions, by using leave-one out cross validation:\n\n\\underset{h}{\\operatorname{arg\\,min}} \\hat{J}(h) = \\underset{h}{\\operatorname{arg\\,min}} \\left( \\frac{2}{(n-1)h} - \\frac{n+1}{n^2(n-1)h} \\sum_k N_k^2 \\right)\n\nHere, N_k is the number of datapoints in the kth bin, and choosing the value of h that minimizes J will minimize integrated mean squared error.\n\nShimazaki and Shinomoto's choice \n\nThe choice is based on minimization of an estimated L2 risk function\n\n \\underset{h}{\\operatorname{arg\\,min}} \\frac{ 2 \\bar{m} - v } {h^2} \n\nwhere \\textstyle \\bar{m} and \\textstyle v are mean and biased variance of a histogram with bin-width \\textstyle h, \\textstyle \\bar{m}\\frac{1}{k} \\sum_{i\n1}^{k}  m_i and \\textstyle v\\frac{1}{k} \\sum_{i\n1}^{k} (m_i - \\bar{m})^2 .\n\nRemark \n\nA good reason why the number of bins should be proportional to n^{1/3} is the following: suppose that the data are obtained as n independent realizations of a bounded probability distribution with smooth density. Then the histogram remains equally \"rugged\" as n tends to infinity. If s is the \"width\" of the distribution (e. g., the standard deviation or the inter-quartile range), then the number of units in a bin (the frequency) is of order n h/s and the relative standard error is of order \\sqrt{s/(n h)}. Comparing to the next bin, the relative change of the frequency is of order h/s provided that the derivative of the density is non-zero. These two are of the same order if h is of order s/n^{1/3}, so that k is of order n^{1/3} . This simple cubic root choice can also be applied to bins with non-constant width.",
  "entityProperties" : [ {
    "name" : "title",
    "type" : "String",
    "values" : [ "Histogram" ],
    "synthetic" : false
  }, {
    "name" : "url",
    "type" : "String",
    "values" : [ "http://en.wikipedia.org/?curid=13266" ],
    "synthetic" : false
  } ],
  "classifications" : [ "xml-export" ],
  "technicalAttributes" : {
    "technicalAttributes" : null,
    "aggregatedText" : "A histogram is an accurate representation of the distribution of numerical data. It is an estimate of the probability distribution of a continuous variable (quantitative variable) and was first introduced by Karl Pearson. It is a kind of bar graph. To construct a histogram, the first step is to \"bin\" the range of values—that is, divide the entire range of values into a series of intervals—and then count how many values fall into each interval.  The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are often (but are not required to be) of equal size.\n\nIf the bins are of equal size, a rectangle is erected over the bin with height proportional to the frequency—the number of cases in each bin. A histogram may also be normalized to display \"relative\" frequencies. It then shows the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1.\n\nHowever, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. The vertical axis is then not the frequency but frequency density—the number of cases per unit of the variable on the horizontal axis. Examples of variable bin width are displayed on Census bureau data below.\n\nAs the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous.Charles Stangor (2011) \"Research Methods For The Behavioral Sciences\". Wadsworth, Cengage Learning. .\n\nHistograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot.\n\nA histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes.\n\nAnother alternative is the average shifted histogram,\nwhich is fast to compute and gives a smooth curve estimate of the density without using kernels.\n\nThe histogram is one of the seven basic tools of quality control.\n\nHistograms are sometimes confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.\n\nEtymology\n\nThe etymology of the word histogram is uncertain. Sometimes it is said to be derived from the Ancient Greek  (histos) – \"anything set upright\" (as the masts of a ship, the bar of a loom, or the vertical bars of a histogram);  and  (gramma) – \"drawing, record, writing\". It is also said that Karl Pearson, who introduced the term in 1891, derived the name from \"historical diagram\".\n\nExamples\n\nThis is the data for the histogram to the right, using 500 items:\n\nThe words used to describe the patterns in a histogram are: \"symmetric\", \"skewed left\" or \"right\", \"unimodal\", \"bimodal\" or \"multimodal\".\n\nSymmetric-histogram.png|Symmetric, unimodal\nSkewed-right.png|Skewed right\nSkewed-left.png|Skewed left\nBimodal-histogram.png|Bimodal\nMultimodal.png|Multimodal\nSymmetric2.png|Symmetric\n\nIt is a good idea to plot the data using several different bin widths to learn more about it. Here is an example on tips given in a restaurant.\n\nTips-histogram1.png|Tips using a $1 bin width, skewed right, unimodal\nTips-histogram2.png|Tips using a 10c bin width, still skewed right, multimodal with modes at $ and 50c amounts, indicates rounding, also some outliers\n\nHere are a couple more examples:\n\nHousingprice.png|Prices of houses sold in Ames in 2009 exhibits some right skew\nTennis-aces.png|Aces by players in a grand slam tennis tournament, facetted by gender. There are more aces in the men's game.\n\nThe U.S. Census Bureau found that there were 124 million people who work outside of their homes.[https://www.census.gov/prod/2004pubs/c2kbr-33.pdf US 2000 census].  Using their data on the time occupied by travel to work, the table below shows the absolute number of people who responded with travel times \"at least 30 but less than 35 minutes\" is higher than the numbers for the categories above and below it. This is likely due to people rounding their reported journey time. The problem of reporting values as somewhat arbitrarily rounded numbers is a common phenomenon when collecting data from people.\n\nThis histogram shows the number of cases per unit interval as the height of each block, so that the area of each block is equal to the number of people in the survey who fall into its category. The area under the curve represents the total number of cases (124 million). This type of histogram shows absolute numbers, with Q in thousands.\n\nThis histogram differs from the first only in the vertical scale.  The area of each block is the fraction of the total that each category represents, and the total area of all the bars is equal to 1 (the fraction meaning \"all\"). The curve displayed is a simple density estimate. This version shows proportions, and is also known as a unit area histogram.\n\nIn other words, a histogram represents a frequency distribution by means of rectangles whose widths represent class intervals and whose areas are proportional to the corresponding frequencies: the height of each is the average frequency density for the interval. The intervals are placed together in order to show that the data represented by the histogram, while exclusive, is also contiguous. (E.g., in a histogram it is possible to have two connecting intervals of 10.5–20.5 and 20.5–33.5, but not two connecting intervals of 10.5–20.5 and 22.5–32.5.  Empty intervals are represented as empty and not skipped.)Dean, S., & Illowsky, B. (2009, February 19). Descriptive Statistics: Histogram. Retrieved from the Connexions Web site: http://cnx.org/content/m16298/1.11/\n\nMathematical definition\n\nIn a more general mathematical sense, a histogram is a function mi that counts the number of observations that fall into each of the disjoint categories (known as bins), whereas the graph of a histogram is merely one way to represent a histogram. Thus, if we let n be the total number of observations and k be the total number of bins, the histogram mi meets the following conditions:\n\nn \\sum_{i\n1}^k{m_i}.\n\nCumulative histogram\n\nA cumulative histogram is a mapping that counts the cumulative number of observations in all of the bins up to the specified bin. That is, the cumulative histogram Mi of a histogram mj is defined as:\n\nM_i \\sum_{j\n1}^i{m_j}.\n\nNumber of bins and width\n\nThere is no \"best\" number of bins, and different bin sizes can reveal different features of the data.  Grouping data is at least as old as Graunt's work in the 17th century, but no systematic guidelines were given until Sturges's work in 1926.\n\nUsing wider bins where the density of the underlying data points is low reduces noise due to sampling randomness; using narrower bins where the density is high (so the signal drowns the noise) gives greater precision to the density estimation.  Thus varying the bin-width within a histogram can be beneficial.  Nonetheless, equal-width bins are widely used.\n\nSome theoreticians have attempted to determine an optimal number of bins, but these methods generally make strong assumptions about the shape of the distribution.  Depending on the actual data distribution and the goals of the analysis, different bin widths may be appropriate, so experimentation is usually needed to determine an appropriate width. There are, however, various useful guidelines and rules of thumb.e.g. § 5.6 \"Density Estimation\", W. N. Venables and B. D. Ripley, Modern Applied Statistics with S (2002), Springer, 4th edition. .\n\nThe number of bins k can be assigned directly or can be calculated from a suggested bin width h as:\nk = \\left \\lceil \\frac{\\max x - \\min x}{h} \\right \\rceil.\n\nThe braces indicate the ceiling function.\n\nSquare-root choice \n\nk = \\sqrt{n}, \\, \n\nwhich takes the square root of the number of data points in the sample (used by Excel histograms and many others).\n\nSturges' formula \n\nSturges' formula is derived from a binomial distribution and implicitly assumes an approximately normal distribution.\n\nk = \\lceil \\log_2 n \\rceil+ 1 , \\, \n\nIt implicitly bases the bin sizes on the range of the data and can perform poorly if n k = \\lceil 2 n^{1/3}\\rceil,\n\nThe Rice Rule Online Statistics Education: A Multimedia Course of Study (http://onlinestatbook.com/). Project Leader: David M. Lane, Rice University (chapter 2 \"Graphing Distributions\", section \"Histograms\") is presented as a simple alternative to Sturges's rule.\n\nDoane's formula \n\nDoane's formulaDoane DP (1976) Aesthetic frequency classification. American Statistician, 30: 181–183 is a modification of Sturges' formula which attempts to improve its performance with non-normal data.\n\n k = 1 + \\log_2( n ) + \\log_2 \\left( 1 +  \\frac { |g_1| }{\\sigma_{g_1}} \\right) \n\nwhere g_1 is the estimated 3rd-moment-skewness of the distribution and\n\n \\sigma_{g_1} = \\sqrt { \\frac { 6(n-2) }{ (n+1)(n+3) } }  \n\nScott's normal reference rule \n\nh = \\frac{3.5 \\hat \\sigma}{n^{1/3}},\n\nwhere \\hat \\sigma is the sample standard deviation. Scott's normal reference rule is optimal for random samples of normally distributed data, in the sense that it minimizes the integrated mean squared error of the density estimate.\n\nFreedman–Diaconis' choice \n\nThe Freedman–Diaconis rule is:\n\nh = 2 \\frac{\\operatorname{IQR}(x)}{n^{1/3}},\n\nwhich is based on the interquartile range, denoted by IQR. It replaces 3.5σ of Scott's rule with 2 IQR, which is less sensitive than the standard deviation to outliers in data.\n\n Minimizing cross-validation estimated squared error  \n\nThis approach of minimizing integrated mean squared error from Scott's rule can be generalized beyond normal distributions, by using leave-one out cross validation:\n\n\\underset{h}{\\operatorname{arg\\,min}} \\hat{J}(h) = \\underset{h}{\\operatorname{arg\\,min}} \\left( \\frac{2}{(n-1)h} - \\frac{n+1}{n^2(n-1)h} \\sum_k N_k^2 \\right)\n\nHere, N_k is the number of datapoints in the kth bin, and choosing the value of h that minimizes J will minimize integrated mean squared error.\n\nShimazaki and Shinomoto's choice \n\nThe choice is based on minimization of an estimated L2 risk function\n\n \\underset{h}{\\operatorname{arg\\,min}} \\frac{ 2 \\bar{m} - v } {h^2} \n\nwhere \\textstyle \\bar{m} and \\textstyle v are mean and biased variance of a histogram with bin-width \\textstyle h, \\textstyle \\bar{m}\\frac{1}{k} \\sum_{i\n1}^{k}  m_i and \\textstyle v\\frac{1}{k} \\sum_{i\n1}^{k} (m_i - \\bar{m})^2 .\n\nRemark \n\nA good reason why the number of bins should be proportional to n^{1/3} is the following: suppose that the data are obtained as n independent realizations of a bounded probability distribution with smooth density. Then the histogram remains equally \"rugged\" as n tends to infinity. If s is the \"width\" of the distribution (e. g., the standard deviation or the inter-quartile range), then the number of units in a bin (the frequency) is of order n h/s and the relative standard error is of order \\sqrt{s/(n h)}. Comparing to the next bin, the relative change of the frequency is of order h/s provided that the derivative of the density is non-zero. These two are of the same order if h is of order s/n^{1/3}, so that k is of order n^{1/3} . This simple cubic root choice can also be applied to bins with non-constant width. Histogram. http://en.wikipedia.org/?curid=13266."
  }
}
