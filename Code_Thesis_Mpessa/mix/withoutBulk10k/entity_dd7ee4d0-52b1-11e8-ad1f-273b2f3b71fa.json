{
  "datasourceIdentifier" : "awesome wiki export",
  "backlink" : "http://en.wikipedia.org/?curid=6759",
  "eid" : "dd7ee4d0-52b1-11e8-ad1f-273b2f3b71fa",
  "loadTime" : 1525778451357,
  "textBody" : "In formal language theory, a context-free grammar (CFG) is a certain type of formal grammar:  a set of production rules that describe all possible strings in a given formal language. Production rules are simple replacements. For example, the rule\n\nA\\ \\to\\ \\alpha\n\nreplaces A with \\alpha. There can be multiple replacement rules for any given value. For example,\n\nA\\ \\to\\ \\alpha\n\nA\\ \\to\\ \\beta\n\nmeans that A can be replaced with either \\alpha or \\beta.\n\nIn context-free grammars, all rules are one-to-one, one-to-many, or one-to-none. These rules can be applied regardless of context. The left-hand side of the production rule is always a nonterminal symbol. This means that the symbol does not appear in the resulting formal language. So in our case, our language contains the letters \\alpha and \\beta but not A.Stephen Scheinberg, Note on the Boolean Properties of Context-Free Languages, Information and Control, 3, 372–375 (1960).\n\nRules can also be applied in reverse to check if a string is grammatically correct according to the grammar.\n\nHere is an example context-free grammar that describes all two-letter strings containing the letters \\alpha and \\beta.\n\nS\\ \\to\\ AA\n\nA\\ \\to\\ \\alpha\n\nA\\ \\to\\ \\beta\n\nIf we start with the nonterminal symbol S then we can use the rule \nS\\ \\to\\ AA to turn S into AA. We can then apply one of the two later rules. For example, if we apply A\\ \\to\\ \\beta to the first A we get \\beta A. If we then apply A\\ \\to\\ \\alpha to the second A we get \\beta\\alpha. Since both \\alpha and \\beta are terminal symbols, and in context-free grammars terminal symbols never appear on the left hand side of a production rule, there are no more rules that can be applied. This same process can be used, applying the second two rules in different orders in order to get all possible strings within our simple context-free grammar.\n\nLanguages generated by context-free grammars are known as context-free languages (CFL). Different context-free grammars can generate the same context-free language. It is important to distinguish the properties of the language (intrinsic properties) from the properties of a particular grammar (extrinsic properties). The language equality question (do two given context-free grammars generate the same language?) is undecidable.\n\nContext-free grammars arise in linguistics where they are used to describe the structure of sentences and words in a natural language, and they were in fact invented by the linguist Noam Chomsky for this purpose, but have not really lived up to their original expectation. By contrast, in computer science, as the use of recursively-defined concepts increased, they were used more and more. In an early application, grammars are used to describe the structure of programming languages. In a newer application, they are used in an essential part of the Extensible Markup Language (XML) called the Document Type Definition.Introduction to Automata Theory, Languages, and Computation, John E. Hopcroft, Rajeev Motwani, Jeffrey D. Ullman, Addison Wesley, 2001, p.191\n\nIn linguistics, some authors use the term phrase structure grammar to refer to context-free grammars, whereby phrase-structure grammars are distinct from dependency grammars. In computer science, a popular notation for context-free grammars is Backus–Naur form, or BNF.\n\nBackground \n\nSince the time of Pāṇini, at least, linguists have described the grammars of languages in terms of their block structure, and described how sentences are recursively built up from smaller phrases, and eventually individual words or word elements. An essential property of these block structures is that logical units never overlap. For example, the sentence:\nJohn, whose blue car was in the garage, walked to the grocery store.\ncan be logically parenthesized as follows:\n(John, ((whose blue car) (was (in the garage))), (walked (to (the grocery store)))).\nA context-free grammar provides a simple and mathematically precise mechanism for describing the methods by which phrases in some natural language are built from smaller blocks, capturing the \"block structure\" of sentences in a natural way. Its simplicity makes the formalism amenable to rigorous mathematical study. Important features of natural language syntax such as agreement and reference are not part of the context-free grammar, but the basic recursive structure of sentences, the way in which clauses nest inside other clauses, and the way in which lists of adjectives and adverbs are swallowed by nouns and verbs, is described exactly.\n\nThe formalism of context-free grammars was developed in the mid-1950s by Noam Chomsky,, p. 106. and also their classification as a special type of formal grammar (which he called phrase-structure grammars). What Chomsky called a phrase structure grammar is also known now as a constituency grammar, whereby constituency grammars stand in contrast to dependency grammars. In Chomsky's generative grammar framework, the syntax of natural language was described by context-free rules combined with transformation rules.\n\nBlock structure was introduced into computer programming languages by the Algol project (1957–1960), which, as a consequence, also featured a context-free grammar to describe the resulting Algol syntax. This became a standard feature of computer languages, and the notation for grammars used in concrete descriptions of computer languages came to be known as Backus–Naur form, after two members of the Algol language design committee. The \"block structure\" aspect that context-free grammars capture is so fundamental to grammar that the terms syntax and grammar are often identified with context-free grammar rules, especially in computer science. Formal constraints not captured by the grammar are then considered to be part of the \"semantics\" of the language.\n\nContext-free grammars are simple enough to allow the construction of efficient parsing algorithms that, for a given string, determine whether and how it can be generated from the grammar. An Earley parser is an example of such an algorithm, while the widely used LR and LL parsers are simpler algorithms that deal only with more restrictive subsets of context-free grammars.\n\nFormal definitions \n\nA context-free grammar  is defined by the 4-tuple:The notation here is that of , p. 94.  (p. 79) define context-free grammars as 4-tuples in the same way, but with different variable names.\n\nG = (V, \\Sigma, R, S)\nwhere\n#  is a finite set; each element  v\\in V is called a nonterminal character or a variable. Each variable represents a different type of phrase or clause in the sentence. Variables are also sometimes called syntactic categories. Each variable defines a sub-language of the language defined by .\n#  is a finite set of terminals, disjoint from , which make up the actual content of the sentence. The set of terminals is the alphabet of the language defined by the grammar .\n#  is a finite relation from  to (V\\cup\\Sigma)^{*}, where the asterisk represents the Kleene star operation. The members of  are called the (rewrite) rules or productions of the grammar. (also commonly symbolized by a )\n#  is the start variable (or start symbol), used to represent the whole sentence (or program). It must be an element of .\n\nProduction rule notation \n\nA production rule in  is formalized mathematically as a pair (\\alpha, \\beta)\\in R, where \\alpha \\in V is a nonterminal and \\beta \\in (V\\cup\\Sigma)^{*} is a string of variables and/or terminals; rather than using ordered pair notation, production rules are usually written using an arrow operator with  as its left hand side and  as its right hand side:\n\\alpha\\rightarrow\\beta.\n\nIt is allowed for  to be the empty string, and in this case it is customary to denote it by ε. The form \\alpha\\rightarrow\\varepsilon is called an -production., pp. 90–92.\n\nIt is common to list all right-hand sides for the same left-hand side on the same line, using | (the pipe symbol) to separate them. Rules \\alpha\\rightarrow \\beta_1 and \\alpha\\rightarrow\\beta_2 can hence be written as \\alpha\\rightarrow\\beta_1\\mid\\beta_2. In this case, \\beta_1 and \\beta_2 is called the first and second alternative, respectively.\n\nRule application \n\nFor any strings u, v\\in (V\\cup\\Sigma)^{*}, we say  directly yields , written as u\\Rightarrow v\\,, if \\exists (\\alpha, \\beta)\\in R with \\alpha \\in V and u_{1}, u_{2}\\in (V\\cup\\Sigma)^{*} such that u\\,u_{1}\\alpha u_{2} and v\\,\nu_{1}\\beta u_{2}. Thus,  is a result of applying the rule (\\alpha, \\beta) to .\n\nRepetitive rule application \n\nFor any strings u, v\\in (V\\cup\\Sigma)^{*},  we say  yields , written as u\\stackrel{*}{\\Rightarrow} v (or u\\Rightarrow\\Rightarrow v\\, in some textbooks), if \\exists k\\geq 1\\, \\exists \\, u_{1}, \\cdots, u_{k}\\in (V\\cup\\Sigma)^{*} such that u \\, u_{1} \\Rightarrow u_{2} \\Rightarrow \\cdots \\Rightarrow u_{k} \\, \n v. In this case, if k\\geq 2 (i.e., u \\neq v), the relation u\\stackrel{+}{\\Rightarrow} v holds. In other words, (\\stackrel{*}{\\Rightarrow}) and (\\stackrel{+}{\\Rightarrow}) are the reflexive transitive closure (allowing a word to yield itself) and the transitive closure (requiring at least one step) of (\\Rightarrow), respectively.\n\nContext-free language \n\nThe language of a grammar G = (V, \\Sigma, R, S) is the set\nL(G) = \\{ w\\in\\Sigma^{*} : S\\stackrel{*}{\\Rightarrow} w\\}\n\nA language  is said to be a context-free language (CFL), if there exists a CFG , such that L\\,=\\,L(G).\n\nProper CFGs \n\nA context-free grammar is said to be proper,. if it has\n* no unreachable symbols: \\forall N \\in V: \\exists \\alpha,\\beta \\in (V\\cup\\Sigma)^*: S \\stackrel{*}{\\Rightarrow} \\alpha{N}\\beta\n* no unproductive symbols: \\forall N \\in V: \\exists w \\in \\Sigma^*: N \\stackrel{*}{\\Rightarrow} w\n* no ε-productions: \\neg\\exists N \\in V: (N, \\varepsilon) \\in R\n* no cycles: \\neg\\exists N \\in V: N \\stackrel{+}{\\Rightarrow} N\nEvery context-free grammar can be effectively transformed into a weakly equivalent one without unreachable symbols,Hopcroft & Ullman (1979), p.88, Lemma 4.1 a weakly equivalent one without unproductive symbols,Hopcroft & Ullman (1979), p.89, Lemma 4.2 and a weakly equivalent one without cycles.This is a consequence of the unit-production elimination theorem in Hopcroft & Ullman (1979), p.91, Theorem 4.4\nEvery context-free grammar not producing ε can be effectively transformed into a weakly equivalent one without ε-productions;Hopcroft & Ullman (1979), p.91, Theorem 4.4 altogether, every such grammar can be effectively transformed into a weakly equivalent proper CFG.\n\nExample \n\nThe grammar G = (\\{S\\}, \\{a, b\\}, P, S), with productions\n \n,\n,\n,\n \nis context-free. It is not proper since it includes an ε-production. A typical derivation in this grammar is\n.\nThis makes it clear that \nL(G) = \\{ww^R:w\\in\\{a,b\\}^*\\}. \nThe language is context-free, however, it can be proved that it is not regular.\n\nExamples \n\nWell-formed parentheses \n\nThe canonical example of a context-free grammar is parenthesis matching, which is representative of the general case. There are two terminal symbols \"(\" and \")\" and one nonterminal symbol S. The production rules are\n\nS → SS\nS → (S)\nS → ()\n\nThe first rule allows the S symbol to multiply; the second rule allows the S symbol to become enclosed by matching parentheses; and the third rule terminates the recursion.\n\nWell-formed nested parentheses and square brackets \n\nA second canonical example is two different kinds of matching nested parentheses, described by the productions:\n\nS → SS\nS → ()\nS → (S)\nS → []\nS → [S]\n\nwith terminal symbols [ ] ( ) and nonterminal S.\n\nThe following sequence can be derived in that grammar:\n([ [ [ ()() [ ][ ] ] ]([ ]) ])\n\nHowever, there is no context-free grammar for generating all sequences of two different types of parentheses, each separately balanced disregarding the other, but where the two types need not nest inside one another, for example:\n\n[ ( ] )\n\nor\n\n[ [ [ [(((( ] ] ] ]))))(([ ))(([ ))([ )( ])( ])( ])\n\nA regular grammar \n\nEvery regular grammar is context-free, but not all context-free grammars are regular. The following context-free grammar, however, is also regular.\n\nS → a\nS → aS\nS → bS\n\nThe terminals here are a and b, while the only nonterminal is S.\nThe language described is all nonempty strings of as and bs that end in a.\n\nThis grammar is regular: no rule has more than one nonterminal in its right-hand side, and each of these nonterminals is at the same end of the right-hand side.\n\nEvery regular grammar corresponds directly to a nondeterministic finite automaton, so we know that this is a regular language.\n\nUsing pipe symbols, the grammar above can be described more tersely as follows:\n\nS → a | aS | bS\n\nMatching pairs \n\nIn a context-free grammar, we can pair up characters the way we do with brackets. The simplest example:\n\nS → aSb\nS → ab\n\nThis grammar generates the language  \\{ a^n b^n : n \\ge 1 \\} , which is not regular (according to the pumping lemma for regular languages).\n\nThe special character ε stands for the empty string. By changing the above grammar to\nS → aSb | ε\nwe obtain a grammar generating the language  \\{ a^n b^n : n \\ge 0 \\}  instead. This differs only in that it contains the empty string while the original grammar did not.\n\nAlgebraic expressions \n\nHere is a context-free grammar for syntactically correct infix algebraic expressions in the variables x, y and z:\n# S → x\n# S → y\n# S → z\n# S → S + S\n# S → S - S\n# S → S * S\n# S → S / S\n# S → ( S )\n\nThis grammar can, for example, generate the string\n\n( x + y ) * x - z * y / ( x + x )\n\nas follows:\n\nS (the start symbol)\n→ S - S (by rule 5)\n→ S * S - S (by rule 6, applied to the leftmost S)\n→ S * S - S / S (by rule 7, applied to the rightmost S)\n→ ( S ) * S - S / S (by rule 8, applied to the leftmost S)\n→ ( S ) * S - S / ( S ) (by rule 8, applied to the rightmost S)\n→ ( S + S ) * S - S / ( S ) (etc.)\n→ ( S + S ) * S - S * S / ( S )\n→ ( S + S ) * S - S * S / ( S + S )\n→ ( x + S ) * S - S * S / ( S + S )\n→ ( x + y ) * S - S * S / ( S + S )\n→ ( x + y ) * x - S * y / ( S + S )\n→ ( x + y ) * x - S * y / ( x + S )\n→ ( x + y ) * x - z * y / ( x + S )\n→ ( x + y ) * x - z * y / ( x + x )\n\nNote that many choices were made underway as to which rewrite was going to be performed next.\nThese choices look quite arbitrary. As a matter of fact, they are, in the sense that the string finally generated is always the same.  For example, the second and third rewrites\n\n→ S * S - S (by rule 6, applied to the leftmost S)\n→ S * S - S / S (by rule 7, applied to the rightmost S)\n\ncould be done in the opposite order:\n\n→ S - S / S (by rule 7, applied to the rightmost S)\n→ S * S - S / S (by rule 6, applied to the leftmost S)\n\nAlso, many choices were made on which rule to apply to each selected S.\nChanging the choices made and not only the order they were made in usually affects which terminal string comes out at the end.\n\nLet's look at this in more detail.  Consider the parse tree of this derivation:\n \n\nStarting at the top, step by step, an S in the tree is expanded, until no more unexpanded Ses (nonterminals) remain.\nPicking a different order of expansion will produce a different derivation, but the same parse tree.\nThe parse tree will only change if we pick a different rule to apply at some position in the tree.\n\nBut can a different parse tree still produce the same terminal string,\nwhich is ( x + y ) * x - z * y / ( x + x ) in this case?\nYes, for this particular grammar, this is possible.\nGrammars with this property are called ambiguous.\n\nFor example, x + y * z can be produced with these two different parse trees:\n \n\nHowever, the language described by this grammar is not inherently ambiguous:\nan alternative, unambiguous grammar can be given for the language, for example:\n\nT → x\nT → y\nT → z\nS → S + T\nS → S - T\nS → S * T\nS → S / T\nT → ( S )\nS → T\n\n(once again picking S as the start symbol). This alternative grammar will produce x + y * z with a parse tree similar to the left one above, i.e. implicitly assuming the association (x + y) * z, which is not according to standard operator precedence. More elaborate, unambiguous and context-free grammars can be constructed that produce parse trees that obey all desired operator precedence and associativity rules.\n\nFurther examples \n\nExample 1 \n\nA context-free grammar for the language consisting of all strings over {a,b} containing an unequal number of a's and b's:\nS → U | V\nU → TaU | TaT | UaT\nV → TbV | TbT | VbT\nT → aTbT | bTaT | ε \nHere, the nonterminal T can generate all strings with the same number of a's as b's, the nonterminal U generates all strings with more a's than b's and the nonterminal V generates all strings with fewer a's than b's. Omitting the third alternative in the rule for U and V doesn't restrict the grammar's language.\n\nExample 2 \n\nAnother example of a non-regular language is  \\{ b^n a^m b^{2n} : n \\ge 0, m \\ge 0 \\} . It is context-free as it can be generated by the following context-free grammar:\nS → bSbb | A\nA → aA | ε\n\nOther examples \n\nThe formation rules for the terms and formulas of formal logic fit the definition of context-free grammar, except that the set of symbols may be infinite and there may be more than one start symbol.\n\nDerivations and syntax trees \n\nA derivation of a string for a grammar is a sequence of grammar rule applications that transform the start symbol into the string.\nA derivation proves that the string belongs to the grammar's language.\n\nA derivation is fully determined by giving, for each step:\n* the rule applied in that step\n* the occurrence of its left-hand side to which it is applied\nFor clarity, the intermediate string is usually given as well.\n\nFor instance, with the grammar:\n\n  (1)  S → S + S\n  (2)  S → 1\n  (3)  S → a\n\nthe string\n\n 1 + 1 + a\n\ncan be derived with the derivation:\n\n S\n     → (rule 1 on the first S)\n  S+S\n     → (rule 1 on the second S)\n  S+S+S\n     → (rule 2 on the second S)\n  S+1+S\n     → (rule 3 on the third S)\n  S+1+a\n     → (rule 2 on the first S)\n  1+1+a\n\nOften, a strategy is followed that deterministically determines the next nonterminal to rewrite:\n* in a leftmost derivation, it is always the leftmost nonterminal;\n* in a rightmost derivation, it is always the rightmost nonterminal.\nGiven such a strategy, a derivation is completely determined by the sequence of rules applied.  For instance, the leftmost derivation\n\n S\n     → (rule 1 on the first S)\n  S+S\n     → (rule 2 on the first S)\n  1+S\n     → (rule 1 on the first S)\n  1+S+S\n     → (rule 2 on the first S)\n  1+1+S\n     → (rule 3 on the first S)\n  1+1+a\n\ncan be summarized as\n\n rule 1, rule 2, rule 1, rule 2, rule 3\n\nThe distinction between leftmost derivation and rightmost derivation is important because in most parsers the transformation of the input is defined by giving a piece of code for every grammar rule that is executed whenever the rule is applied. Therefore, it is important to know whether the parser determines a leftmost or a rightmost derivation because this determines the order in which the pieces of code will be executed. See for an example LL parsers and LR parsers.\n\nA derivation also imposes in some sense a hierarchical structure on the string that is derived. For example, if the string \"1 + 1 + a\" is derived according to the leftmost derivation:\n\nS → S + S (1)\n   → 1 + S (2)\n   → 1 + S + S (1)\n   → 1 + 1 + S (2)\n   → 1 + 1 + a (3)\n\nthe structure of the string would be:\n\n{ { 1 }S + { { 1 }S + { a }S }S }S\nwhere { ... }S indicates a substring recognized as belonging to S. This hierarchy can also be seen as a tree:\n\nThis tree is called a parse tree or \"concrete syntax tree\" of the string, by contrast with the abstract syntax tree. In this case the presented leftmost and the rightmost derivations define the same parse tree; however, there is another (rightmost) derivation of the same string\n\nS → S + S (1)\n   → S + a (3)\n   → S + S + a (1)\n   → S + 1 + a (2)\n   → 1 + 1 + a (2)\n\nand this defines the following parse tree:\n\nNote however that both parse trees can be obtained by both leftmost and rightmost derivations.  For example, the last tree can be obtained with the leftmost derivation as follows:\n\nS → S + S (1)\n   → S + S + S (1)\n   → 1 + S + S (2)\n   → 1 + 1 + S (2)\n   → 1 + 1 + a (3)\n\nIf a string in the language of the grammar has more than one parsing tree, then the grammar is said to be an ambiguous grammar. Such grammars are usually hard to parse because the parser cannot always decide which grammar rule it has to apply. Usually, ambiguity is a feature of the grammar, not the language, and an unambiguous grammar can be found that generates the same context-free language. However, there are certain languages that can only be generated by ambiguous grammars; such languages are called inherently ambiguous languages.\n\nNormal forms \n\nEvery context-free grammar that does not generate the empty string can be transformed into one in which there is no ε-production (that is, a rule that has the empty string as a product). If a grammar does generate the empty string, it will be necessary to include the rule S \\rarr \\epsilon, but there need be no other ε-rule. Every context-free grammar with no ε-production has an equivalent grammar in Chomsky normal form, and a grammar in Greibach normal form. \"Equivalent\" here means that the two grammars generate the same language.\n\nThe especially simple form of production rules in Chomsky normal form grammars has both theoretical and practical implications. For instance, given a context-free grammar, one can use the Chomsky normal form to construct a polynomial-time algorithm that decides whether a given string is in the language represented by that grammar or not (the CYK algorithm).\n\nClosure properties\n\nContext-free languages are closed under the various operations, that is, if the languages K and L are \ncontext-free, so is the result of the following operations:\n* union K ∪ L; concatenation K ∘ L; Kleene star L*Hopcroft & Ullman (1979), p.131, Theorem 6.1\n* substitution (in particular homomorphism)Hopcroft & Ullman (1979), p.131-132, Theorem 6.2\n* inverse homomorphismHopcroft & Ullman (1979), p.132-134, Theorem 6.3\n* intersection with a regular languageHopcroft & Ullman (1979), p.135-136, Theorem 6.5\n\nThey are not closed under general intersection (hence neither under complementation) and set difference.Hopcroft & Ullman (1979), p.134-135, Theorem 6.4\n\nDecidable problems\n\nThere are algorithms to decide whether a context-free language is empty and whether it is finite.Hopcroft & Ullman (1979), p.137-138, Theorem 6.6\n\nUndecidable problems \n\nSome questions that are undecidable for wider classes of grammars become decidable for context-free grammars; e.g. the emptiness problem (whether the grammar generates any terminal strings at all), is undecidable for context-sensitive grammars, but decidable for context-free grammars.\n\nHowever, many problems are undecidable even for context-free grammars. Examples are:\n\nUniversality \n\nGiven a CFG, does it generate the language of all strings over the alphabet of terminal symbols used in its rules?, Theorem 5.10, p. 181.\n\nA reduction can be demonstrated to this problem from the well-known undecidable problem of determining whether a Turing machine accepts a particular input (the halting problem). The reduction uses the concept of a computation history, a string describing an entire computation of a Turing machine. A CFG can be constructed that generates all strings that are not accepting computation histories for a particular Turing machine on a particular input, and thus it will accept all strings only if the machine doesn't accept that input.\n\nLanguage equality \n\nGiven two CFGs, do they generate the same language?, p. 281.\n\nThe undecidability of this problem is a direct consequence of the previous: it is impossible to even decide whether a CFG is equivalent to the trivial CFG defining the language of all strings.\n\nLanguage inclusion \n\nGiven two CFGs, can the first one generate all strings that the second one can generate?\n\nIf this problem was decidable, then language equality could be decided too: two CFGs G1 and G2 generate the same language if L(G1) is a subset of L(G2) and L(G2) is a subset of L(G1).\n\nBeing in a lower or higher level of the Chomsky hierarchy \n\nUsing Greibach's theorem, it can be shown that the two following problems are undecidable:\n\n* Given a context-sensitive grammar, does it describe a context-free language?\n* Given a context-free grammar, does it describe a regular language?.\n\nGrammar ambiguity \n\nGiven a CFG, is it ambiguous?\n\nThe undecidability of this problem follows from the fact that if an algorithm to determine ambiguity existed, the Post correspondence problem could be decided, which is known to be undecidable.\n\nLanguage disjointness \n\nGiven two CFGs, is there any string derivable from both grammars?\n\nIf this problem was decidable, the undecidable Post correspondence problem could be decided, too: given strings \\alpha_1, \\ldots, \\alpha_N, \\beta_1, \\ldots, \\beta_N over some alphabet \\{a_1, \\ldots, a_k\\}, let the grammar  consist of the rule\nS \\to \\alpha_1 S \\beta_1^{rev} | \\cdots | \\alpha_N S \\beta_N^{rev} | b;\nwhere \\beta_i^{rev} denotes the reversed string \\beta_i and b doesn't occur among the a_i; and let grammar  consist of  the rule\nT \\to a_1 T a_1 | \\cdots | a_k T a_k | b;\nThen the Post problem given by \\alpha_1, \\ldots, \\alpha_N, \\beta_1, \\ldots, \\beta_N has a solution if and only if  and  share a derivable string.\n\nExtensions \n\nAn obvious way to extend the context-free grammar formalism is to allow nonterminals to have arguments, the values of which are passed along within the rules. This allows natural language features such as agreement and reference, and programming language analogs such as the correct use and definition of identifiers, to be expressed in a natural way. E.g. we can now easily express that in English sentences, the subject and verb must agree in number. In computer science, examples of this approach include affix grammars, attribute grammars, indexed grammars, and Van Wijngaarden two-level grammars. Similar extensions exist in linguistics.\n\nAn extended context-free grammar (or regular right part grammar) is one in which the right-hand side of the production rules is allowed to be a regular expression over the grammar's terminals and nonterminals. Extended context-free grammars describe exactly the context-free languages.\n\nAnother extension is to allow additional terminal symbols to appear at the left-hand side of rules, constraining their application. This produces the formalism of context-sensitive grammars.\n\nSubclasses \n\nThere are a number of important subclasses of the context-free grammars:\n\n* LR(k) grammars (also known as deterministic context-free grammars) allow parsing (string recognition) with deterministic pushdown automata (PDA), but they can only describe deterministic context-free languages.\n* Simple LR, Look-Ahead LR grammars are subclasses that allow further simplification of parsing. SLR and LALR are recognized using the same PDA as LR, but with simpler tables, in most cases.\n* LL(k) and LL(*) grammars allow parsing by direct construction of a leftmost derivation as described above, and describe even fewer languages.\n* Simple grammars are a subclass of the LL(1) grammars mostly interesting for its theoretical property that language equality of simple grammars is decidable, while language inclusion is not.\n* Bracketed grammars have the property that the terminal symbols are divided into left and right bracket pairs that always match up in rules.\n* Linear grammars have no rules with more than one nonterminal on the right-hand side.\n* Regular grammars are a subclass of the linear grammars and describe the regular languages, i.e. they correspond to finite automata and regular expressions.\n\nLR parsing extends LL parsing to support a larger range of grammars; in turn, generalized LR parsing extends LR parsing to support arbitrary context-free grammars.  On LL grammars and LR grammars, it essentially performs LL parsing and LR parsing, respectively, while on nondeterministic grammars, it is as efficient as can be expected.  Although GLR parsing was developed in the 1980s, many new language definitions and parser generators continue to be based on LL, LALR or LR parsing up to the present day.\n\nLinguistic applications\n\nChomsky initially hoped to overcome the limitations of context-free grammars by adding transformation rules.\n\nSuch rules are another standard device in traditional linguistics; e.g. passivization in English. Much of generative grammar has been devoted to finding ways of refining the descriptive mechanisms of phrase-structure grammar and transformation rules such that exactly the kinds of things can be expressed that natural language actually allows. Allowing arbitrary transformations doesn't meet that goal: they are much too powerful, being Turing complete unless significant restrictions are added (e.g. no transformations that introduce and then rewrite symbols in a context-free fashion).\n\nChomsky's general position regarding the non-context-freeness of natural language has held up since then,. although his specific examples regarding the inadequacy of context-free grammars in terms of their weak generative capacity were later disproved..\nGerald Gazdar and Geoffrey Pullum have argued that despite a few non-context-free constructions in natural language (such as cross-serial dependencies in Swiss German and reduplication in Bambara.), the vast majority of forms in natural language are indeed context-free.",
  "entityProperties" : [ {
    "name" : "title",
    "type" : "String",
    "values" : [ "Context-free grammar" ],
    "synthetic" : false
  }, {
    "name" : "url",
    "type" : "String",
    "values" : [ "http://en.wikipedia.org/?curid=6759" ],
    "synthetic" : false
  } ],
  "classifications" : [ "xml-export" ],
  "technicalAttributes" : {
    "technicalAttributes" : null,
    "aggregatedText" : "In formal language theory, a context-free grammar (CFG) is a certain type of formal grammar:  a set of production rules that describe all possible strings in a given formal language. Production rules are simple replacements. For example, the rule\n\nA\\ \\to\\ \\alpha\n\nreplaces A with \\alpha. There can be multiple replacement rules for any given value. For example,\n\nA\\ \\to\\ \\alpha\n\nA\\ \\to\\ \\beta\n\nmeans that A can be replaced with either \\alpha or \\beta.\n\nIn context-free grammars, all rules are one-to-one, one-to-many, or one-to-none. These rules can be applied regardless of context. The left-hand side of the production rule is always a nonterminal symbol. This means that the symbol does not appear in the resulting formal language. So in our case, our language contains the letters \\alpha and \\beta but not A.Stephen Scheinberg, Note on the Boolean Properties of Context-Free Languages, Information and Control, 3, 372–375 (1960).\n\nRules can also be applied in reverse to check if a string is grammatically correct according to the grammar.\n\nHere is an example context-free grammar that describes all two-letter strings containing the letters \\alpha and \\beta.\n\nS\\ \\to\\ AA\n\nA\\ \\to\\ \\alpha\n\nA\\ \\to\\ \\beta\n\nIf we start with the nonterminal symbol S then we can use the rule \nS\\ \\to\\ AA to turn S into AA. We can then apply one of the two later rules. For example, if we apply A\\ \\to\\ \\beta to the first A we get \\beta A. If we then apply A\\ \\to\\ \\alpha to the second A we get \\beta\\alpha. Since both \\alpha and \\beta are terminal symbols, and in context-free grammars terminal symbols never appear on the left hand side of a production rule, there are no more rules that can be applied. This same process can be used, applying the second two rules in different orders in order to get all possible strings within our simple context-free grammar.\n\nLanguages generated by context-free grammars are known as context-free languages (CFL). Different context-free grammars can generate the same context-free language. It is important to distinguish the properties of the language (intrinsic properties) from the properties of a particular grammar (extrinsic properties). The language equality question (do two given context-free grammars generate the same language?) is undecidable.\n\nContext-free grammars arise in linguistics where they are used to describe the structure of sentences and words in a natural language, and they were in fact invented by the linguist Noam Chomsky for this purpose, but have not really lived up to their original expectation. By contrast, in computer science, as the use of recursively-defined concepts increased, they were used more and more. In an early application, grammars are used to describe the structure of programming languages. In a newer application, they are used in an essential part of the Extensible Markup Language (XML) called the Document Type Definition.Introduction to Automata Theory, Languages, and Computation, John E. Hopcroft, Rajeev Motwani, Jeffrey D. Ullman, Addison Wesley, 2001, p.191\n\nIn linguistics, some authors use the term phrase structure grammar to refer to context-free grammars, whereby phrase-structure grammars are distinct from dependency grammars. In computer science, a popular notation for context-free grammars is Backus–Naur form, or BNF.\n\nBackground \n\nSince the time of Pāṇini, at least, linguists have described the grammars of languages in terms of their block structure, and described how sentences are recursively built up from smaller phrases, and eventually individual words or word elements. An essential property of these block structures is that logical units never overlap. For example, the sentence:\nJohn, whose blue car was in the garage, walked to the grocery store.\ncan be logically parenthesized as follows:\n(John, ((whose blue car) (was (in the garage))), (walked (to (the grocery store)))).\nA context-free grammar provides a simple and mathematically precise mechanism for describing the methods by which phrases in some natural language are built from smaller blocks, capturing the \"block structure\" of sentences in a natural way. Its simplicity makes the formalism amenable to rigorous mathematical study. Important features of natural language syntax such as agreement and reference are not part of the context-free grammar, but the basic recursive structure of sentences, the way in which clauses nest inside other clauses, and the way in which lists of adjectives and adverbs are swallowed by nouns and verbs, is described exactly.\n\nThe formalism of context-free grammars was developed in the mid-1950s by Noam Chomsky,, p. 106. and also their classification as a special type of formal grammar (which he called phrase-structure grammars). What Chomsky called a phrase structure grammar is also known now as a constituency grammar, whereby constituency grammars stand in contrast to dependency grammars. In Chomsky's generative grammar framework, the syntax of natural language was described by context-free rules combined with transformation rules.\n\nBlock structure was introduced into computer programming languages by the Algol project (1957–1960), which, as a consequence, also featured a context-free grammar to describe the resulting Algol syntax. This became a standard feature of computer languages, and the notation for grammars used in concrete descriptions of computer languages came to be known as Backus–Naur form, after two members of the Algol language design committee. The \"block structure\" aspect that context-free grammars capture is so fundamental to grammar that the terms syntax and grammar are often identified with context-free grammar rules, especially in computer science. Formal constraints not captured by the grammar are then considered to be part of the \"semantics\" of the language.\n\nContext-free grammars are simple enough to allow the construction of efficient parsing algorithms that, for a given string, determine whether and how it can be generated from the grammar. An Earley parser is an example of such an algorithm, while the widely used LR and LL parsers are simpler algorithms that deal only with more restrictive subsets of context-free grammars.\n\nFormal definitions \n\nA context-free grammar  is defined by the 4-tuple:The notation here is that of , p. 94.  (p. 79) define context-free grammars as 4-tuples in the same way, but with different variable names.\n\nG = (V, \\Sigma, R, S)\nwhere\n#  is a finite set; each element  v\\in V is called a nonterminal character or a variable. Each variable represents a different type of phrase or clause in the sentence. Variables are also sometimes called syntactic categories. Each variable defines a sub-language of the language defined by .\n#  is a finite set of terminals, disjoint from , which make up the actual content of the sentence. The set of terminals is the alphabet of the language defined by the grammar .\n#  is a finite relation from  to (V\\cup\\Sigma)^{*}, where the asterisk represents the Kleene star operation. The members of  are called the (rewrite) rules or productions of the grammar. (also commonly symbolized by a )\n#  is the start variable (or start symbol), used to represent the whole sentence (or program). It must be an element of .\n\nProduction rule notation \n\nA production rule in  is formalized mathematically as a pair (\\alpha, \\beta)\\in R, where \\alpha \\in V is a nonterminal and \\beta \\in (V\\cup\\Sigma)^{*} is a string of variables and/or terminals; rather than using ordered pair notation, production rules are usually written using an arrow operator with  as its left hand side and  as its right hand side:\n\\alpha\\rightarrow\\beta.\n\nIt is allowed for  to be the empty string, and in this case it is customary to denote it by ε. The form \\alpha\\rightarrow\\varepsilon is called an -production., pp. 90–92.\n\nIt is common to list all right-hand sides for the same left-hand side on the same line, using | (the pipe symbol) to separate them. Rules \\alpha\\rightarrow \\beta_1 and \\alpha\\rightarrow\\beta_2 can hence be written as \\alpha\\rightarrow\\beta_1\\mid\\beta_2. In this case, \\beta_1 and \\beta_2 is called the first and second alternative, respectively.\n\nRule application \n\nFor any strings u, v\\in (V\\cup\\Sigma)^{*}, we say  directly yields , written as u\\Rightarrow v\\,, if \\exists (\\alpha, \\beta)\\in R with \\alpha \\in V and u_{1}, u_{2}\\in (V\\cup\\Sigma)^{*} such that u\\,u_{1}\\alpha u_{2} and v\\,\nu_{1}\\beta u_{2}. Thus,  is a result of applying the rule (\\alpha, \\beta) to .\n\nRepetitive rule application \n\nFor any strings u, v\\in (V\\cup\\Sigma)^{*},  we say  yields , written as u\\stackrel{*}{\\Rightarrow} v (or u\\Rightarrow\\Rightarrow v\\, in some textbooks), if \\exists k\\geq 1\\, \\exists \\, u_{1}, \\cdots, u_{k}\\in (V\\cup\\Sigma)^{*} such that u \\, u_{1} \\Rightarrow u_{2} \\Rightarrow \\cdots \\Rightarrow u_{k} \\, \n v. In this case, if k\\geq 2 (i.e., u \\neq v), the relation u\\stackrel{+}{\\Rightarrow} v holds. In other words, (\\stackrel{*}{\\Rightarrow}) and (\\stackrel{+}{\\Rightarrow}) are the reflexive transitive closure (allowing a word to yield itself) and the transitive closure (requiring at least one step) of (\\Rightarrow), respectively.\n\nContext-free language \n\nThe language of a grammar G = (V, \\Sigma, R, S) is the set\nL(G) = \\{ w\\in\\Sigma^{*} : S\\stackrel{*}{\\Rightarrow} w\\}\n\nA language  is said to be a context-free language (CFL), if there exists a CFG , such that L\\,=\\,L(G).\n\nProper CFGs \n\nA context-free grammar is said to be proper,. if it has\n* no unreachable symbols: \\forall N \\in V: \\exists \\alpha,\\beta \\in (V\\cup\\Sigma)^*: S \\stackrel{*}{\\Rightarrow} \\alpha{N}\\beta\n* no unproductive symbols: \\forall N \\in V: \\exists w \\in \\Sigma^*: N \\stackrel{*}{\\Rightarrow} w\n* no ε-productions: \\neg\\exists N \\in V: (N, \\varepsilon) \\in R\n* no cycles: \\neg\\exists N \\in V: N \\stackrel{+}{\\Rightarrow} N\nEvery context-free grammar can be effectively transformed into a weakly equivalent one without unreachable symbols,Hopcroft & Ullman (1979), p.88, Lemma 4.1 a weakly equivalent one without unproductive symbols,Hopcroft & Ullman (1979), p.89, Lemma 4.2 and a weakly equivalent one without cycles.This is a consequence of the unit-production elimination theorem in Hopcroft & Ullman (1979), p.91, Theorem 4.4\nEvery context-free grammar not producing ε can be effectively transformed into a weakly equivalent one without ε-productions;Hopcroft & Ullman (1979), p.91, Theorem 4.4 altogether, every such grammar can be effectively transformed into a weakly equivalent proper CFG.\n\nExample \n\nThe grammar G = (\\{S\\}, \\{a, b\\}, P, S), with productions\n \n,\n,\n,\n \nis context-free. It is not proper since it includes an ε-production. A typical derivation in this grammar is\n.\nThis makes it clear that \nL(G) = \\{ww^R:w\\in\\{a,b\\}^*\\}. \nThe language is context-free, however, it can be proved that it is not regular.\n\nExamples \n\nWell-formed parentheses \n\nThe canonical example of a context-free grammar is parenthesis matching, which is representative of the general case. There are two terminal symbols \"(\" and \")\" and one nonterminal symbol S. The production rules are\n\nS → SS\nS → (S)\nS → ()\n\nThe first rule allows the S symbol to multiply; the second rule allows the S symbol to become enclosed by matching parentheses; and the third rule terminates the recursion.\n\nWell-formed nested parentheses and square brackets \n\nA second canonical example is two different kinds of matching nested parentheses, described by the productions:\n\nS → SS\nS → ()\nS → (S)\nS → []\nS → [S]\n\nwith terminal symbols [ ] ( ) and nonterminal S.\n\nThe following sequence can be derived in that grammar:\n([ [ [ ()() [ ][ ] ] ]([ ]) ])\n\nHowever, there is no context-free grammar for generating all sequences of two different types of parentheses, each separately balanced disregarding the other, but where the two types need not nest inside one another, for example:\n\n[ ( ] )\n\nor\n\n[ [ [ [(((( ] ] ] ]))))(([ ))(([ ))([ )( ])( ])( ])\n\nA regular grammar \n\nEvery regular grammar is context-free, but not all context-free grammars are regular. The following context-free grammar, however, is also regular.\n\nS → a\nS → aS\nS → bS\n\nThe terminals here are a and b, while the only nonterminal is S.\nThe language described is all nonempty strings of as and bs that end in a.\n\nThis grammar is regular: no rule has more than one nonterminal in its right-hand side, and each of these nonterminals is at the same end of the right-hand side.\n\nEvery regular grammar corresponds directly to a nondeterministic finite automaton, so we know that this is a regular language.\n\nUsing pipe symbols, the grammar above can be described more tersely as follows:\n\nS → a | aS | bS\n\nMatching pairs \n\nIn a context-free grammar, we can pair up characters the way we do with brackets. The simplest example:\n\nS → aSb\nS → ab\n\nThis grammar generates the language  \\{ a^n b^n : n \\ge 1 \\} , which is not regular (according to the pumping lemma for regular languages).\n\nThe special character ε stands for the empty string. By changing the above grammar to\nS → aSb | ε\nwe obtain a grammar generating the language  \\{ a^n b^n : n \\ge 0 \\}  instead. This differs only in that it contains the empty string while the original grammar did not.\n\nAlgebraic expressions \n\nHere is a context-free grammar for syntactically correct infix algebraic expressions in the variables x, y and z:\n# S → x\n# S → y\n# S → z\n# S → S + S\n# S → S - S\n# S → S * S\n# S → S / S\n# S → ( S )\n\nThis grammar can, for example, generate the string\n\n( x + y ) * x - z * y / ( x + x )\n\nas follows:\n\nS (the start symbol)\n→ S - S (by rule 5)\n→ S * S - S (by rule 6, applied to the leftmost S)\n→ S * S - S / S (by rule 7, applied to the rightmost S)\n→ ( S ) * S - S / S (by rule 8, applied to the leftmost S)\n→ ( S ) * S - S / ( S ) (by rule 8, applied to the rightmost S)\n→ ( S + S ) * S - S / ( S ) (etc.)\n→ ( S + S ) * S - S * S / ( S )\n→ ( S + S ) * S - S * S / ( S + S )\n→ ( x + S ) * S - S * S / ( S + S )\n→ ( x + y ) * S - S * S / ( S + S )\n→ ( x + y ) * x - S * y / ( S + S )\n→ ( x + y ) * x - S * y / ( x + S )\n→ ( x + y ) * x - z * y / ( x + S )\n→ ( x + y ) * x - z * y / ( x + x )\n\nNote that many choices were made underway as to which rewrite was going to be performed next.\nThese choices look quite arbitrary. As a matter of fact, they are, in the sense that the string finally generated is always the same.  For example, the second and third rewrites\n\n→ S * S - S (by rule 6, applied to the leftmost S)\n→ S * S - S / S (by rule 7, applied to the rightmost S)\n\ncould be done in the opposite order:\n\n→ S - S / S (by rule 7, applied to the rightmost S)\n→ S * S - S / S (by rule 6, applied to the leftmost S)\n\nAlso, many choices were made on which rule to apply to each selected S.\nChanging the choices made and not only the order they were made in usually affects which terminal string comes out at the end.\n\nLet's look at this in more detail.  Consider the parse tree of this derivation:\n \n\nStarting at the top, step by step, an S in the tree is expanded, until no more unexpanded Ses (nonterminals) remain.\nPicking a different order of expansion will produce a different derivation, but the same parse tree.\nThe parse tree will only change if we pick a different rule to apply at some position in the tree.\n\nBut can a different parse tree still produce the same terminal string,\nwhich is ( x + y ) * x - z * y / ( x + x ) in this case?\nYes, for this particular grammar, this is possible.\nGrammars with this property are called ambiguous.\n\nFor example, x + y * z can be produced with these two different parse trees:\n \n\nHowever, the language described by this grammar is not inherently ambiguous:\nan alternative, unambiguous grammar can be given for the language, for example:\n\nT → x\nT → y\nT → z\nS → S + T\nS → S - T\nS → S * T\nS → S / T\nT → ( S )\nS → T\n\n(once again picking S as the start symbol). This alternative grammar will produce x + y * z with a parse tree similar to the left one above, i.e. implicitly assuming the association (x + y) * z, which is not according to standard operator precedence. More elaborate, unambiguous and context-free grammars can be constructed that produce parse trees that obey all desired operator precedence and associativity rules.\n\nFurther examples \n\nExample 1 \n\nA context-free grammar for the language consisting of all strings over {a,b} containing an unequal number of a's and b's:\nS → U | V\nU → TaU | TaT | UaT\nV → TbV | TbT | VbT\nT → aTbT | bTaT | ε \nHere, the nonterminal T can generate all strings with the same number of a's as b's, the nonterminal U generates all strings with more a's than b's and the nonterminal V generates all strings with fewer a's than b's. Omitting the third alternative in the rule for U and V doesn't restrict the grammar's language.\n\nExample 2 \n\nAnother example of a non-regular language is  \\{ b^n a^m b^{2n} : n \\ge 0, m \\ge 0 \\} . It is context-free as it can be generated by the following context-free grammar:\nS → bSbb | A\nA → aA | ε\n\nOther examples \n\nThe formation rules for the terms and formulas of formal logic fit the definition of context-free grammar, except that the set of symbols may be infinite and there may be more than one start symbol.\n\nDerivations and syntax trees \n\nA derivation of a string for a grammar is a sequence of grammar rule applications that transform the start symbol into the string.\nA derivation proves that the string belongs to the grammar's language.\n\nA derivation is fully determined by giving, for each step:\n* the rule applied in that step\n* the occurrence of its left-hand side to which it is applied\nFor clarity, the intermediate string is usually given as well.\n\nFor instance, with the grammar:\n\n  (1)  S → S + S\n  (2)  S → 1\n  (3)  S → a\n\nthe string\n\n 1 + 1 + a\n\ncan be derived with the derivation:\n\n S\n     → (rule 1 on the first S)\n  S+S\n     → (rule 1 on the second S)\n  S+S+S\n     → (rule 2 on the second S)\n  S+1+S\n     → (rule 3 on the third S)\n  S+1+a\n     → (rule 2 on the first S)\n  1+1+a\n\nOften, a strategy is followed that deterministically determines the next nonterminal to rewrite:\n* in a leftmost derivation, it is always the leftmost nonterminal;\n* in a rightmost derivation, it is always the rightmost nonterminal.\nGiven such a strategy, a derivation is completely determined by the sequence of rules applied.  For instance, the leftmost derivation\n\n S\n     → (rule 1 on the first S)\n  S+S\n     → (rule 2 on the first S)\n  1+S\n     → (rule 1 on the first S)\n  1+S+S\n     → (rule 2 on the first S)\n  1+1+S\n     → (rule 3 on the first S)\n  1+1+a\n\ncan be summarized as\n\n rule 1, rule 2, rule 1, rule 2, rule 3\n\nThe distinction between leftmost derivation and rightmost derivation is important because in most parsers the transformation of the input is defined by giving a piece of code for every grammar rule that is executed whenever the rule is applied. Therefore, it is important to know whether the parser determines a leftmost or a rightmost derivation because this determines the order in which the pieces of code will be executed. See for an example LL parsers and LR parsers.\n\nA derivation also imposes in some sense a hierarchical structure on the string that is derived. For example, if the string \"1 + 1 + a\" is derived according to the leftmost derivation:\n\nS → S + S (1)\n   → 1 + S (2)\n   → 1 + S + S (1)\n   → 1 + 1 + S (2)\n   → 1 + 1 + a (3)\n\nthe structure of the string would be:\n\n{ { 1 }S + { { 1 }S + { a }S }S }S\nwhere { ... }S indicates a substring recognized as belonging to S. This hierarchy can also be seen as a tree:\n\nThis tree is called a parse tree or \"concrete syntax tree\" of the string, by contrast with the abstract syntax tree. In this case the presented leftmost and the rightmost derivations define the same parse tree; however, there is another (rightmost) derivation of the same string\n\nS → S + S (1)\n   → S + a (3)\n   → S + S + a (1)\n   → S + 1 + a (2)\n   → 1 + 1 + a (2)\n\nand this defines the following parse tree:\n\nNote however that both parse trees can be obtained by both leftmost and rightmost derivations.  For example, the last tree can be obtained with the leftmost derivation as follows:\n\nS → S + S (1)\n   → S + S + S (1)\n   → 1 + S + S (2)\n   → 1 + 1 + S (2)\n   → 1 + 1 + a (3)\n\nIf a string in the language of the grammar has more than one parsing tree, then the grammar is said to be an ambiguous grammar. Such grammars are usually hard to parse because the parser cannot always decide which grammar rule it has to apply. Usually, ambiguity is a feature of the grammar, not the language, and an unambiguous grammar can be found that generates the same context-free language. However, there are certain languages that can only be generated by ambiguous grammars; such languages are called inherently ambiguous languages.\n\nNormal forms \n\nEvery context-free grammar that does not generate the empty string can be transformed into one in which there is no ε-production (that is, a rule that has the empty string as a product). If a grammar does generate the empty string, it will be necessary to include the rule S \\rarr \\epsilon, but there need be no other ε-rule. Every context-free grammar with no ε-production has an equivalent grammar in Chomsky normal form, and a grammar in Greibach normal form. \"Equivalent\" here means that the two grammars generate the same language.\n\nThe especially simple form of production rules in Chomsky normal form grammars has both theoretical and practical implications. For instance, given a context-free grammar, one can use the Chomsky normal form to construct a polynomial-time algorithm that decides whether a given string is in the language represented by that grammar or not (the CYK algorithm).\n\nClosure properties\n\nContext-free languages are closed under the various operations, that is, if the languages K and L are \ncontext-free, so is the result of the following operations:\n* union K ∪ L; concatenation K ∘ L; Kleene star L*Hopcroft & Ullman (1979), p.131, Theorem 6.1\n* substitution (in particular homomorphism)Hopcroft & Ullman (1979), p.131-132, Theorem 6.2\n* inverse homomorphismHopcroft & Ullman (1979), p.132-134, Theorem 6.3\n* intersection with a regular languageHopcroft & Ullman (1979), p.135-136, Theorem 6.5\n\nThey are not closed under general intersection (hence neither under complementation) and set difference.Hopcroft & Ullman (1979), p.134-135, Theorem 6.4\n\nDecidable problems\n\nThere are algorithms to decide whether a context-free language is empty and whether it is finite.Hopcroft & Ullman (1979), p.137-138, Theorem 6.6\n\nUndecidable problems \n\nSome questions that are undecidable for wider classes of grammars become decidable for context-free grammars; e.g. the emptiness problem (whether the grammar generates any terminal strings at all), is undecidable for context-sensitive grammars, but decidable for context-free grammars.\n\nHowever, many problems are undecidable even for context-free grammars. Examples are:\n\nUniversality \n\nGiven a CFG, does it generate the language of all strings over the alphabet of terminal symbols used in its rules?, Theorem 5.10, p. 181.\n\nA reduction can be demonstrated to this problem from the well-known undecidable problem of determining whether a Turing machine accepts a particular input (the halting problem). The reduction uses the concept of a computation history, a string describing an entire computation of a Turing machine. A CFG can be constructed that generates all strings that are not accepting computation histories for a particular Turing machine on a particular input, and thus it will accept all strings only if the machine doesn't accept that input.\n\nLanguage equality \n\nGiven two CFGs, do they generate the same language?, p. 281.\n\nThe undecidability of this problem is a direct consequence of the previous: it is impossible to even decide whether a CFG is equivalent to the trivial CFG defining the language of all strings.\n\nLanguage inclusion \n\nGiven two CFGs, can the first one generate all strings that the second one can generate?\n\nIf this problem was decidable, then language equality could be decided too: two CFGs G1 and G2 generate the same language if L(G1) is a subset of L(G2) and L(G2) is a subset of L(G1).\n\nBeing in a lower or higher level of the Chomsky hierarchy \n\nUsing Greibach's theorem, it can be shown that the two following problems are undecidable:\n\n* Given a context-sensitive grammar, does it describe a context-free language?\n* Given a context-free grammar, does it describe a regular language?.\n\nGrammar ambiguity \n\nGiven a CFG, is it ambiguous?\n\nThe undecidability of this problem follows from the fact that if an algorithm to determine ambiguity existed, the Post correspondence problem could be decided, which is known to be undecidable.\n\nLanguage disjointness \n\nGiven two CFGs, is there any string derivable from both grammars?\n\nIf this problem was decidable, the undecidable Post correspondence problem could be decided, too: given strings \\alpha_1, \\ldots, \\alpha_N, \\beta_1, \\ldots, \\beta_N over some alphabet \\{a_1, \\ldots, a_k\\}, let the grammar  consist of the rule\nS \\to \\alpha_1 S \\beta_1^{rev} | \\cdots | \\alpha_N S \\beta_N^{rev} | b;\nwhere \\beta_i^{rev} denotes the reversed string \\beta_i and b doesn't occur among the a_i; and let grammar  consist of  the rule\nT \\to a_1 T a_1 | \\cdots | a_k T a_k | b;\nThen the Post problem given by \\alpha_1, \\ldots, \\alpha_N, \\beta_1, \\ldots, \\beta_N has a solution if and only if  and  share a derivable string.\n\nExtensions \n\nAn obvious way to extend the context-free grammar formalism is to allow nonterminals to have arguments, the values of which are passed along within the rules. This allows natural language features such as agreement and reference, and programming language analogs such as the correct use and definition of identifiers, to be expressed in a natural way. E.g. we can now easily express that in English sentences, the subject and verb must agree in number. In computer science, examples of this approach include affix grammars, attribute grammars, indexed grammars, and Van Wijngaarden two-level grammars. Similar extensions exist in linguistics.\n\nAn extended context-free grammar (or regular right part grammar) is one in which the right-hand side of the production rules is allowed to be a regular expression over the grammar's terminals and nonterminals. Extended context-free grammars describe exactly the context-free languages.\n\nAnother extension is to allow additional terminal symbols to appear at the left-hand side of rules, constraining their application. This produces the formalism of context-sensitive grammars.\n\nSubclasses \n\nThere are a number of important subclasses of the context-free grammars:\n\n* LR(k) grammars (also known as deterministic context-free grammars) allow parsing (string recognition) with deterministic pushdown automata (PDA), but they can only describe deterministic context-free languages.\n* Simple LR, Look-Ahead LR grammars are subclasses that allow further simplification of parsing. SLR and LALR are recognized using the same PDA as LR, but with simpler tables, in most cases.\n* LL(k) and LL(*) grammars allow parsing by direct construction of a leftmost derivation as described above, and describe even fewer languages.\n* Simple grammars are a subclass of the LL(1) grammars mostly interesting for its theoretical property that language equality of simple grammars is decidable, while language inclusion is not.\n* Bracketed grammars have the property that the terminal symbols are divided into left and right bracket pairs that always match up in rules.\n* Linear grammars have no rules with more than one nonterminal on the right-hand side.\n* Regular grammars are a subclass of the linear grammars and describe the regular languages, i.e. they correspond to finite automata and regular expressions.\n\nLR parsing extends LL parsing to support a larger range of grammars; in turn, generalized LR parsing extends LR parsing to support arbitrary context-free grammars.  On LL grammars and LR grammars, it essentially performs LL parsing and LR parsing, respectively, while on nondeterministic grammars, it is as efficient as can be expected.  Although GLR parsing was developed in the 1980s, many new language definitions and parser generators continue to be based on LL, LALR or LR parsing up to the present day.\n\nLinguistic applications\n\nChomsky initially hoped to overcome the limitations of context-free grammars by adding transformation rules.\n\nSuch rules are another standard device in traditional linguistics; e.g. passivization in English. Much of generative grammar has been devoted to finding ways of refining the descriptive mechanisms of phrase-structure grammar and transformation rules such that exactly the kinds of things can be expressed that natural language actually allows. Allowing arbitrary transformations doesn't meet that goal: they are much too powerful, being Turing complete unless significant restrictions are added (e.g. no transformations that introduce and then rewrite symbols in a context-free fashion).\n\nChomsky's general position regarding the non-context-freeness of natural language has held up since then,. although his specific examples regarding the inadequacy of context-free grammars in terms of their weak generative capacity were later disproved..\nGerald Gazdar and Geoffrey Pullum have argued that despite a few non-context-free constructions in natural language (such as cross-serial dependencies in Swiss German and reduplication in Bambara.), the vast majority of forms in natural language are indeed context-free. Context-free grammar. http://en.wikipedia.org/?curid=6759."
  }
}
