{
  "datasourceIdentifier" : "awesome wiki export",
  "backlink" : "http://en.wikipedia.org/?curid=19384",
  "eid" : "3a8d3870-52b2-11e8-ad1f-273b2f3b71fa",
  "loadTime" : 1525778607479,
  "textBody" : "Multivariate statistics is a subdivision of statistics encompassing the simultaneous observation and analysis of more than one outcome variable. The application of multivariate statistics is multivariate analysis.\n\nMultivariate statistics concerns understanding the different aims and background of each of the different forms of multivariate analysis, and how they relate to each other. The practical application of multivariate statistics to a particular problem may involve several types of univariate and multivariate analyses in order to understand the relationships between variables and their relevance to the problem being studied.\n\nIn addition, multivariate statistics is concerned with multivariate probability distributions, in terms of both\n*how these can be used to represent the distributions of observed data;\n*how they can be used as part of statistical inference, particularly where several different quantities are of interest to the same analysis.\n\nCertain types of problems involving multivariate data, for example simple linear regression and multiple regression, are not usually considered to be special cases of multivariate statistics because the analysis is dealt with by considering the (univariate) conditional distribution of a single outcome variable given the other variables.\n\nTypes of analysis\n\nThere are many different models, each with its own type of analysis:\n# Multivariate analysis of variance (MANOVA) extends the analysis of variance to cover cases where there is more than one dependent variable to be analyzed simultaneously; see also MANCOVA.\n#Multivariate regression attempts to determine a formula that can describe how elements in a vector of variables respond simultaneously to changes in others. For linear relations, regression analyses here are based on forms of the general linear model. Some suggest that that multivariate regression is distinct from multivariable regression, however, that is debated and not consistently true across scientific fields.\n# Principal components analysis (PCA) creates a new set of orthogonal variables that contain the same information as the original set. It rotates the axes of variation to give a new set of orthogonal axes, ordered so that they summarize decreasing proportions of the variation.\n# Factor analysis is similar to PCA but allows the user to extract a specified number of synthetic variables, fewer than the original set, leaving the remaining unexplained variation as error. The extracted variables are known as latent variables or factors; each one may be supposed to account for covariation in a group of observed variables.\n# Canonical correlation analysis finds linear relationships among two sets of variables; it is the generalised (i.e. canonical) version of bivariateUnsophisticated analysts of bivariate Gaussian problems may find useful a crude but accurate [http://www.dioi.org/sta.htm#sdsx method] of accurately gauging probability by simply taking the sum S of the N residuals' squares, subtracting the sum Sm at minimum, dividing this difference by Sm, multiplying the result by (N - 2) and taking the inverse anti-ln of half that product. correlation.\n# Redundancy analysis (RDA) is similar to canonical correlation analysis but allows the user to derive a specified number of synthetic variables from one set of (independent) variables that explain as much variance as possible in another (independent) set. It is a multivariate analogue of regression.\n# Correspondence analysis (CA), or reciprocal averaging, finds (like PCA) a set of synthetic variables that summarise the original set. The underlying model assumes chi-squared dissimilarities among records (cases).\n# Canonical (or \"constrained\") correspondence analysis (CCA) for summarising the joint variation in two sets of variables (like redundancy analysis); combination of correspondence analysis and multivariate regression analysis. The underlying model assumes chi-squared dissimilarities among records (cases).\n# Multidimensional scaling comprises various algorithms to determine a set of synthetic variables that best represent the pairwise distances between records. The original method is principal coordinates analysis (PCoA; based on PCA).\n# Discriminant analysis, or canonical variate analysis, attempts to establish whether a set of variables can be used to distinguish between two or more groups of cases.\n# Linear discriminant analysis (LDA) computes a linear predictor from two sets of normally distributed data to allow for classification of new observations.\n# Clustering systems assign objects into groups (called clusters) so that objects (cases) from the same cluster are more similar to each other than objects from different clusters.\n# Recursive partitioning creates a decision tree that attempts to correctly classify members of the population based on a dichotomous dependent variable.\n# Artificial neural networks extend regression and clustering methods to non-linear multivariate models.\n# Statistical graphics such as tours, parallel coordinate plots, scatterplot matrices can be used to explore multivariate data.\n# Simultaneous equations models involve more than one regression equation, with different dependent variables, estimated together.\n# Vector autoregression involves simultaneous regressions of various time series variables on their own and each other's lagged values.\n# Principal response curves analysis (PRC) is a method based on RDA that allows the user to focus on treatment effects over time by correcting for changes in control treatments over time.ter Braak, Cajo J.F. & Šmilauer, Petr (2012). Canoco reference manual and user´s guide: software for ordination (version 5.0), p292. Microcomputer Power, Ithaca, NY.\n\nImportant probability distributions\n\nThere is a set of probability distributions used in multivariate analyses that play a similar role to the corresponding set of distributions that are used in univariate analysis when the normal distribution is appropriate to a dataset. These multivariate distributions are:\n*Multivariate normal distribution\n*Wishart distribution\n*Multivariate Student-t distribution.\nThe Inverse-Wishart distribution is important in Bayesian inference, for example in Bayesian multivariate linear regression. Additionally, Hotelling's T-squared distribution is a multivariate distribution, generalising Student's t-distribution, that is used in multivariate hypothesis testing.\n\nHistory\n\nAnderson's 1958 textbook, An Introduction to Multivariate Analysis,T.W. Anderson (1958)  An Introduction to Multivariate Analysis, New York: Wiley ; 2e (1984) ; 3e (2003)  educated a generation of theorists and applied  statisticians; Anderson's book emphasizes hypothesis testing via likelihood ratio tests and the properties of power functions: admissibility, unbiasedness and monotonicity.(Pages 560–561)\n\nSoftware and tools\n\nThere are an enormous number of software packages and other tools for multivariate analysis, including:\n* High-D\n* JMP (statistical software)\n* MiniTab\n* Calc\n* PSPP\n* R[https://cran.r-project.org/web/views/Multivariate.html CRAN] has details on the packages available for multivariate data analysis\n* SAS (software)\n* SciPy for Python\n* SPSS\n* Stata\n* STATISTICA\n* TMVA - Toolkit for Multivariate Data Analysis in ROOT\n* The Unscrambler\n* SmartPLS\n* MATLAB\n* Eviews\n* Prosensus ProMV\n* Umetrics SIMCA\n* Multibase\n* Canoco",
  "entityProperties" : [ {
    "name" : "title",
    "type" : "String",
    "values" : [ "Multivariate statistics" ],
    "synthetic" : false
  }, {
    "name" : "url",
    "type" : "String",
    "values" : [ "http://en.wikipedia.org/?curid=19384" ],
    "synthetic" : false
  } ],
  "classifications" : [ "xml-export" ],
  "technicalAttributes" : {
    "technicalAttributes" : null,
    "aggregatedText" : "Multivariate statistics is a subdivision of statistics encompassing the simultaneous observation and analysis of more than one outcome variable. The application of multivariate statistics is multivariate analysis.\n\nMultivariate statistics concerns understanding the different aims and background of each of the different forms of multivariate analysis, and how they relate to each other. The practical application of multivariate statistics to a particular problem may involve several types of univariate and multivariate analyses in order to understand the relationships between variables and their relevance to the problem being studied.\n\nIn addition, multivariate statistics is concerned with multivariate probability distributions, in terms of both\n*how these can be used to represent the distributions of observed data;\n*how they can be used as part of statistical inference, particularly where several different quantities are of interest to the same analysis.\n\nCertain types of problems involving multivariate data, for example simple linear regression and multiple regression, are not usually considered to be special cases of multivariate statistics because the analysis is dealt with by considering the (univariate) conditional distribution of a single outcome variable given the other variables.\n\nTypes of analysis\n\nThere are many different models, each with its own type of analysis:\n# Multivariate analysis of variance (MANOVA) extends the analysis of variance to cover cases where there is more than one dependent variable to be analyzed simultaneously; see also MANCOVA.\n#Multivariate regression attempts to determine a formula that can describe how elements in a vector of variables respond simultaneously to changes in others. For linear relations, regression analyses here are based on forms of the general linear model. Some suggest that that multivariate regression is distinct from multivariable regression, however, that is debated and not consistently true across scientific fields.\n# Principal components analysis (PCA) creates a new set of orthogonal variables that contain the same information as the original set. It rotates the axes of variation to give a new set of orthogonal axes, ordered so that they summarize decreasing proportions of the variation.\n# Factor analysis is similar to PCA but allows the user to extract a specified number of synthetic variables, fewer than the original set, leaving the remaining unexplained variation as error. The extracted variables are known as latent variables or factors; each one may be supposed to account for covariation in a group of observed variables.\n# Canonical correlation analysis finds linear relationships among two sets of variables; it is the generalised (i.e. canonical) version of bivariateUnsophisticated analysts of bivariate Gaussian problems may find useful a crude but accurate [http://www.dioi.org/sta.htm#sdsx method] of accurately gauging probability by simply taking the sum S of the N residuals' squares, subtracting the sum Sm at minimum, dividing this difference by Sm, multiplying the result by (N - 2) and taking the inverse anti-ln of half that product. correlation.\n# Redundancy analysis (RDA) is similar to canonical correlation analysis but allows the user to derive a specified number of synthetic variables from one set of (independent) variables that explain as much variance as possible in another (independent) set. It is a multivariate analogue of regression.\n# Correspondence analysis (CA), or reciprocal averaging, finds (like PCA) a set of synthetic variables that summarise the original set. The underlying model assumes chi-squared dissimilarities among records (cases).\n# Canonical (or \"constrained\") correspondence analysis (CCA) for summarising the joint variation in two sets of variables (like redundancy analysis); combination of correspondence analysis and multivariate regression analysis. The underlying model assumes chi-squared dissimilarities among records (cases).\n# Multidimensional scaling comprises various algorithms to determine a set of synthetic variables that best represent the pairwise distances between records. The original method is principal coordinates analysis (PCoA; based on PCA).\n# Discriminant analysis, or canonical variate analysis, attempts to establish whether a set of variables can be used to distinguish between two or more groups of cases.\n# Linear discriminant analysis (LDA) computes a linear predictor from two sets of normally distributed data to allow for classification of new observations.\n# Clustering systems assign objects into groups (called clusters) so that objects (cases) from the same cluster are more similar to each other than objects from different clusters.\n# Recursive partitioning creates a decision tree that attempts to correctly classify members of the population based on a dichotomous dependent variable.\n# Artificial neural networks extend regression and clustering methods to non-linear multivariate models.\n# Statistical graphics such as tours, parallel coordinate plots, scatterplot matrices can be used to explore multivariate data.\n# Simultaneous equations models involve more than one regression equation, with different dependent variables, estimated together.\n# Vector autoregression involves simultaneous regressions of various time series variables on their own and each other's lagged values.\n# Principal response curves analysis (PRC) is a method based on RDA that allows the user to focus on treatment effects over time by correcting for changes in control treatments over time.ter Braak, Cajo J.F. & Šmilauer, Petr (2012). Canoco reference manual and user´s guide: software for ordination (version 5.0), p292. Microcomputer Power, Ithaca, NY.\n\nImportant probability distributions\n\nThere is a set of probability distributions used in multivariate analyses that play a similar role to the corresponding set of distributions that are used in univariate analysis when the normal distribution is appropriate to a dataset. These multivariate distributions are:\n*Multivariate normal distribution\n*Wishart distribution\n*Multivariate Student-t distribution.\nThe Inverse-Wishart distribution is important in Bayesian inference, for example in Bayesian multivariate linear regression. Additionally, Hotelling's T-squared distribution is a multivariate distribution, generalising Student's t-distribution, that is used in multivariate hypothesis testing.\n\nHistory\n\nAnderson's 1958 textbook, An Introduction to Multivariate Analysis,T.W. Anderson (1958)  An Introduction to Multivariate Analysis, New York: Wiley ; 2e (1984) ; 3e (2003)  educated a generation of theorists and applied  statisticians; Anderson's book emphasizes hypothesis testing via likelihood ratio tests and the properties of power functions: admissibility, unbiasedness and monotonicity.(Pages 560–561)\n\nSoftware and tools\n\nThere are an enormous number of software packages and other tools for multivariate analysis, including:\n* High-D\n* JMP (statistical software)\n* MiniTab\n* Calc\n* PSPP\n* R[https://cran.r-project.org/web/views/Multivariate.html CRAN] has details on the packages available for multivariate data analysis\n* SAS (software)\n* SciPy for Python\n* SPSS\n* Stata\n* STATISTICA\n* TMVA - Toolkit for Multivariate Data Analysis in ROOT\n* The Unscrambler\n* SmartPLS\n* MATLAB\n* Eviews\n* Prosensus ProMV\n* Umetrics SIMCA\n* Multibase\n* Canoco. Multivariate statistics. http://en.wikipedia.org/?curid=19384."
  }
}
