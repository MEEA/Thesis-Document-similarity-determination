{
  "datasourceIdentifier" : "awesome wiki export",
  "backlink" : "http://en.wikipedia.org/?curid=6115",
  "eid" : "d9f39b30-52b1-11e8-ad1f-273b2f3b71fa",
  "loadTime" : 1525778445411,
  "textBody" : "The P versus NP problem is a major unsolved problem in computer science. It asks whether every problem whose solution can be quickly verified (technically, verified in polynomial time) can also be solved quickly (again, in polynomial time).\n\nThe underlying issues were first discussed in the 1950s, in letters from John Forbes Nash Jr. to the National Security Agency, and from Kurt Gödel to John von Neumann. The precise statement of the P versus NP problem was introduced in 1971 by Stephen Cook in his seminal paper \"The complexity of theorem proving procedures\" and is considered by many to be the most important open problem in the field. It is one of the seven Millennium Prize Problems selected by the Clay Mathematics Institute to carry a US$1,000,000 prize for the first correct solution.\n\nThe informal term quickly, used above, means the existence of an algorithm solving the task that runs in polynomial time, such that the time to complete the task varies as a polynomial function on the size of the input to the algorithm (as opposed to, say, exponential time). The general class of questions for which some algorithm can provide an answer in polynomial time is called \"class P\" or just \"P\". For some questions, there is no known way to find an answer quickly, but if one is provided with information showing what the answer is, it is possible to verify the answer quickly. The class of questions for which an answer can be verified in polynomial time is called NP, which stands for \"nondeterministic polynomial time\".A nondeterministic Turing machine can move to a state that is not determined by the previous state.  Such a machine could solve a NP problem in polynomial time by falling into the correct answer state (by luck), then conventionally verifying it.  Such machines are not practical for solving realistic problems but can be used as theoretical models.\n\nConsider Sudoku, an example of a problem that is easy to verify, but whose answer may be difficult to compute. Given a partially filled-in Sudoku grid, of any size, is there at least one legal solution?  A proposed solution is easily verified, and the time to check a solution grows slowly (polynomially) as the grid gets bigger.  However, all known algorithms for finding solutions take, for difficult examples, time that grows exponentially as the grid gets bigger.  So Sudoku is in NP (quickly checkable) but does not seem to be in P (quickly solvable).  Thousands of other problems seem similar, fast to check but slow to solve.  Researchers have shown that a fast solution to any one of these problems could be used to build a quick solution to all the others, a property called NP-completeness.  Decades of searching have not yielded a fast solution to any of these problems, so most scientists suspect that none of these problems can be solved quickly.  However, this has never been proven.\n\nAn answer to the P = NP question would determine whether problems that can be verified in polynomial time, like Sudoku, can also be solved in polynomial time. If it turned out that P ≠ NP, it would mean that there are problems in NP  that are harder to compute than to verify: they could not be solved in polynomial time, but the answer could be verified in polynomial time.\n\nAside from being an important problem in computational theory, a proof either way would have profound implications for mathematics, cryptography, algorithm research, artificial intelligence, game theory, multimedia processing, philosophy, economics and many other fields.\n\nHistory\n\nAlthough the P versus NP problem was formally defined in 1971, there were previous inklings of the problems involved, the difficulty of proof, and the potential consequences.  In 1955, mathematician John Nash wrote a letter to the NSA, where he speculated that cracking a sufficiently complex code would require time exponential in the length of the key.  If proved (and Nash was suitably skeptical) this would imply what we today would call P ≠ NP, since a proposed key can easily be verified in polynomial time. Another mention of the underlying problem occurred in a 1956 letter written by Kurt Gödel to John von Neumann.  Gödel asked whether theorem-proving (now known to be co-NP-complete) could be solved in quadratic or linear time, and pointed out one of the most important consequences—that if so, then the discovery of mathematical proofs could be automated.\n\nContext\n\nThe relation between the complexity classes P and NP is studied in computational complexity theory, the part of the theory of computation dealing with the resources required during computation to solve a given problem. The most common resources are time (how many steps it takes to solve a problem) and space (how much memory it takes to solve a problem).\n\nIn such analysis, a model of the computer for which time must be analyzed is required. Typically such models assume that the computer is deterministic (given the computer's present state and any inputs, there is only one possible action that the computer might take) and sequential (it performs actions one after the other).\n\nIn this theory, the class P consists of all those decision problems (defined below) that can be solved on a deterministic sequential machine in an amount of time that is polynomial in the size of the input; the class NP consists of all those decision problems whose positive solutions can be verified in polynomial time given the right information, or equivalently, whose solution can be found in polynomial time on a non-deterministic machine.Sipser, Michael: Introduction to the Theory of Computation, Second Edition, International Edition, page 270. Thomson Course Technology, 2006. Definition 7.19 and Theorem 7.20. Clearly, P ⊆ NP. Arguably the biggest open question in theoretical computer science concerns the relationship between those two classes:\nIs P equal to NP?\nIn a 2002 poll of 100 researchers, 61 believed the answer to be no, 9 believed the answer is yes, and 22 were unsure; 8 believed the question may be independent of the currently accepted axioms and therefore impossible to prove or disprove.\n\nIn 2012, 10 years later, the same poll was repeated. The number of researchers who answered was 151: 126 (83%) believed the answer to be no, 12 (9%) believed the answer is yes, 5 (3%) believed the question may be independent of the currently accepted axioms and therefore impossible to prove or disprove, 8 (5%) said either don't know or don't care or don't want the answer to be yes nor the problem to be resolved.\n\nNP-completeness\n\nTo attack the P = NP question, the concept of NP-completeness is very useful. NP-complete problems are a set of problems to each of which any other NP-problem can be reduced in polynomial time, and whose solution may still be verified in polynomial time. That is, any NP problem can be transformed into any of the NP-complete problems. Informally, an NP-complete problem is an NP problem that is at least as \"tough\" as any other problem in NP.\n\nNP-hard problems are those at least as hard as NP problems, i.e., all NP problems can be reduced (in polynomial time) to them. NP-hard problems need not be in NP, i.e., they need not have solutions verifiable in polynomial time.\n\nFor instance, the Boolean satisfiability problem is NP-complete by the Cook–Levin theorem, so any instance of any problem in NP can be transformed mechanically into an instance of the Boolean satisfiability problem in polynomial time. The Boolean satisfiability problem is one of many such NP-complete problems. If any NP-complete problem is in P, then it would follow that P = NP. However, many important problems have been shown to be NP-complete, and no fast algorithm for any of them is known.\n\nBased on the definition alone it is not obvious that NP-complete problems exist; however, a trivial and contrived NP-complete problem can be formulated as follows: given a description of a Turing machine M guaranteed to halt in polynomial time, does there exist a polynomial-size input that M will accept? It is in NP because (given an input) it is simple to check whether M accepts the input by simulating M; it is NP-complete because the verifier for any particular instance of a problem in NP can be encoded as a polynomial-time machine M that takes the solution to be verified as input. Then the question of whether the instance is a yes or no instance is determined by whether a valid input exists.\n\nThe first natural problem proven to be NP-complete was the Boolean satisfiability problem. As noted above, this is the Cook–Levin theorem; its proof that satisfiability is NP-complete contains technical details about Turing machines as they relate to the definition of NP. However, after this problem was proved to be NP-complete, proof by reduction provided a simpler way to show that many other problems are also NP-complete, including the Sudoku discussed earlier. In this case, the proof shows that  a solution of Sudoku in polynomial time, could also be used to complete Latin squares in polynomial time.  This in turn gives a solution to the problem of partitioning tri-partitite graphs into triangles, which could then be used to find solutions for 3-sat, which then provides a solution for general boolean satisfiability.  So a polynomial time solution to Sudoku leads, by a series of mechanical transformations, to a polynomial time solution of satisfiability, which in turn can be used to solve any other NP-complete problem in polynomial time.  Using transformations like this, a vast class of seemingly unrelated problems are all reducible to one another, and are in a sense \"the same problem\".\n\nHarder problems\n\nAlthough it is unknown whether P NP, problems outside of P are known. A number of succinct problems (problems that operate not on normal input, but on a computational description of the input) are known to be EXPTIME-complete. Because it can be shown that P ≠ EXPTIME, these problems are outside P, and so require more than polynomial time. In fact, by the time hierarchy theorem, they cannot be solved in significantly less than exponential time. Examples include finding a perfect strategy for chess (on an N × N board) and some other board games.\n\nThe problem of deciding the truth of a statement in Presburger arithmetic requires even more time. Fischer and Rabin proved in 1974 that every algorithm that decides the truth of Presburger statements of length n has a runtime of at least 2^{2^{cn}} for some constant c. Hence, the problem is known to need more than exponential run time. Even more difficult are the undecidable problems, such as the halting problem. They cannot be completely solved by any algorithm, in the sense that for any particular algorithm there is at least one input for which that algorithm will not produce the right answer; it will either produce the wrong answer, finish without giving a conclusive answer, or otherwise run forever without producing any answer at all.\n\nIt is also possible to consider questions other than decision problems.  One such class, consisting of counting problems, is called #P: whereas an NP problem asks \"Are there any solutions?\", the corresponding #P problem asks \"How many solutions are there?\"  Clearly, a #P problem must be at least as hard as the corresponding NP problem, since a count of possible solutions immediately tells if at least one solution exists, if the count is greater than zero. Surprisingly, some #P problems that are believed to be difficult correspond to easy (for example linear-time) P problems.  For these problems, it's very easy to tell whether solutions exist, but thought to be very hard to tell how many.  Many of these problems are #P-complete, and hence among the hardest problems in #P, since a polynomial time solution to any of them would allow a polynomial time solution to all other #P problems.\n\nProblems in NP not known to be in P or NP-complete\n\nIt was shown by Ladner that if P ≠ NP then there exist problems in NP that are neither in P nor NP-complete. Such problems are called NP-intermediate problems. The graph isomorphism problem, the discrete logarithm problem and the integer factorization problem are examples of problems believed to be NP-intermediate. They are some of the very few NP problems not known to be in P or to be NP-complete.\n\nThe graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic. An important unsolved problem in complexity theory is whether the graph isomorphism problem is in P, NP-complete, or NP-intermediate. The answer is not known, but it is believed that the problem is at least not NP-complete. If graph isomorphism is NP-complete, the polynomial time hierarchy collapses to its second level. Since it is widely believed that the polynomial hierarchy does not collapse to any finite level, it is believed that graph isomorphism is not NP-complete. The best algorithm for this problem, due to László Babai and Eugene Luks, has run time 2O(√nlog(n)) for graphs with n vertices.\n\nThe integer factorization problem is the computational problem of determining the prime factorization of a given integer. Phrased as a decision problem, it is the problem of deciding whether the input has a factor less than k. No efficient integer factorization algorithm is known, and this fact forms the basis of several modern cryptographic systems, such as the RSA algorithm. The integer factorization problem is in NP and in co-NP (and even in UP and co-UPLance Fortnow. Computational Complexity Blog: [http://weblog.fortnow.com/2002/09/complexity-class-of-week-factoring.html Complexity Class of the Week: Factoring]. 13 September 2002.). If the problem is NP-complete, the polynomial time hierarchy will collapse to its first level (i.e., NP = co-NP). The best known algorithm for integer factorization is the general number field sieve, which takes expected time\n\nO\\left (\\exp \\left ( \\left (\\tfrac{64n}{9} \\log(2) \\right )^{\\frac{1}{3}} \\left ( \\log(n\\log(2)) \\right )^{\\frac{2}{3}} \\right) \\right )\n\nto factor an n-bit integer. However, the best known quantum algorithm for this problem, Shor's algorithm, does run in polynomial time, although this does not indicate where the problem lies with respect to non-quantum complexity classes.\n\nDoes P mean \"easy\"?\n\nAll of the above discussion has assumed that P means \"easy\" and \"not in P\" means \"hard\", an assumption known as Cobham's thesis. It is a common and reasonably accurate assumption in complexity theory; however, it has some caveats.\n\nFirst, it is not always true in practice. A theoretical polynomial algorithm may have extremely large constant factors or exponents thus rendering it impractical. On the other hand, even if a problem is shown to be NP-complete, and even if P ≠ NP, there may still be effective approaches to tackling the problem in practice. There are algorithms for many NP-complete problems, such as the knapsack problem, the traveling salesman problem and the Boolean satisfiability problem, that can solve to optimality many real-world instances in reasonable time. The empirical average-case complexity (time vs. problem size) of such algorithms can be surprisingly low.  An example is the simplex algorithm in linear programming, which works surprisingly well in practice; despite having exponential worst-case time complexity it runs on par with the best known polynomial-time algorithms.\n\nSecond, there are types of computations which do not conform to the Turing machine model on which P and NP are defined, such as quantum computation and randomized algorithms.\n\nReasons to believe P ≠ NP\n\nAccording to polls, most computer scientists believe that P ≠ NP. A key reason for this belief is that after decades of studying these problems no one has been able to find a polynomial-time algorithm for any of more than 3000 important known NP-complete problems (see List of NP-complete problems). These algorithms were sought long before the concept of NP-completeness was even defined (Karp's 21 NP-complete problems, among the first found, were all well-known existing problems at the time they were shown to be NP-complete). Furthermore, the result P \n NP would imply many other startling results that are currently believed to be false, such as NP co-NP and P \n PH.\n\nIt is also intuitively argued that the existence of problems that are hard to solve but for which the solutions are easy to verify matches real-world experience., point 9.\n\nOn the other hand, some researchers believe that there is overconfidence in believing P ≠ NP and that researchers should explore proofs of P NP as well. For example, in 2002 these statements were made:\n\nConsequences of solution\n\nOne of the reasons the problem attracts so much attention is the consequences of the answer.  Either direction of resolution would advance theory enormously, and perhaps have huge practical consequences as well.\n\nP \n NPA proof that P \n NP could have stunning practical consequences if the proof leads to efficient methods for solving some of the important problems in NP. It is also possible that a proof would not lead directly to efficient methods, perhaps if the proof is non-constructive, or the size of the bounding polynomial is too big to be efficient in practice. The consequences, both positive and negative, arise since various NP-complete problems are fundamental in many fields.\n\nCryptography, for example, relies on certain problems being difficult. A constructive and efficient solutionExactly how efficient a solution must be to pose a threat to cryptography depends on the details.  A solution of O(N^2) or better and a reasonable constant term would be disastrous.  On the other hand, a solution that is \\Omega(N^4) or worse in almost all cases would not pose an immediate practical danger. to an NP-complete problem such as 3-SAT would break most existing cryptosystems including:\n* public-key cryptography,See  for a reduction of factoring to SAT.  A 512 bit factoring problem (8400 MIPS-years when factored) translates to a SAT problem of 63,652 variables and 406,860 clauses. a foundation for many modern security applications such as secure financial transactions over the Internet; and\n* symmetric ciphers such as AES or 3DES,See, for example,  in which an instance of DES is encoded as a SAT problem with 10336 variables and 61935 clauses.  A 3DES problem instance would be about 3 times this size. used for the encryption of communications data.\n* cryptographic hashing as the problem of finding a pre-image that hashes to a given valueFind a messageM that when hashed by the function H() gives a digest h, or H(M)h must be difficult to be useful, and ideally should require exponential time. However, if P\nNP, then finding a pre-image M can be done in polynomial time, through reduction to SAT.\nThese would need to be modified or replaced by information-theoretically secure solutions not inherently based on P-NP equivalence.\n\nOn the other hand, there are enormous positive consequences that would follow from rendering tractable many currently mathematically intractable problems. For instance, many problems in operations research are NP-complete, such as some types of integer programming and the travelling salesman problem. Efficient solutions to these problems would have enormous implications for logistics. Many other important problems, such as some problems in protein structure prediction, are also NP-complete; if these problems were efficiently solvable it could spur considerable advances in life sciences and biotechnology.\n\nBut such changes may pale in significance compared to the revolution an efficient method for solving NP-complete problems would cause in mathematics itself. Gödel, in his early thoughts on computational complexity, noted that a mechanical method that could solve any problem would revolutionize mathematics:History of this letter and its translation from  From pages 359–376 of Optimization Stories, M. Grötschel (editor), a special issue of ¨ Documenta Mathematica, published in August 2012 and distributed to attendees at the 21st International Symposium on Mathematical Programming in Berlin.\n\nSimilarly, Stephen Cook says\n\nResearch mathematicians spend their careers trying to prove theorems, and some proofs have taken decades or even centuries to find after problems have been stated—for instance, Fermat's Last Theorem took over three centuries to prove. A method that is guaranteed to find proofs to theorems, should one exist of a \"reasonable\" size, would essentially end this struggle.\n\nDonald Knuth has stated that he has come to believe that P = NP, but is reserved about the impact of a possible proof:\n\nP ≠ NP\n\nA proof that showed that P ≠ NP would lack the practical computational benefits of a proof that P = NP, but would nevertheless represent a very significant advance in computational complexity theory and provide guidance for future research. It would allow one to show in a formal way that many common problems cannot be solved efficiently, so that the attention of researchers can be focused on partial solutions or solutions to other problems. Due to widespread belief in P ≠ NP, much of this focusing of research has already taken place.\n\nAlso P ≠ NP still leaves open the average-case complexity of hard problems in NP.  For example, it is possible that SAT requires exponential time in the worst case, but that almost all randomly selected instances of it are efficiently solvable. Russell Impagliazzo has described five hypothetical \"worlds\" that could result from different possible resolutions to the average-case complexity question.R. Impagliazzo, [http://cseweb.ucsd.edu/~russell/average.ps \"A personal view of average-case complexity,\"] sct, pp.134, 10th Annual Structure in Complexity Theory Conference (SCT'95), 1995  These range from \"Algorithmica\", where P = NP and problems like SAT can be solved efficiently in all instances, to \"Cryptomania\", where P ≠ NP and generating hard instances of problems outside P is easy, with three intermediate possibilities reflecting different possible distributions of difficulty over instances of NP-hard problems.  The \"world\" where P ≠ NP but all problems in NP are tractable in the average case is called \"Heuristica\" in the paper. A Princeton University workshop in 2009 studied the status of the five worlds.\n\nResults about difficulty of proof\n\nAlthough the P NP problem itself remains open despite a million-dollar prize and a huge amount of dedicated research, efforts to solve the problem have led to several new techniques.  In particular, some of the most fruitful research related to the P \n NP problem has been in showing that existing proof techniques are not powerful enough to answer the question, thus suggesting that novel technical approaches are required.\n\nAs additional evidence for the difficulty of the problem, essentially all known proof techniques in computational complexity theory fall into one of the following classifications, each of which is known to be insufficient to prove that P ≠ NP:\n\nThese barriers are another reason why NP-complete problems are useful: if a polynomial-time algorithm can be demonstrated for an NP-complete problem, this would solve the P = NP problem in a way not excluded by the above results.\n\nThese barriers have also led some computer scientists to suggest that the P versus NP problem may be independent of standard axiom systems like ZFC (cannot be proved or disproved within them). The interpretation of an independence result could be that either no polynomial-time algorithm exists for any NP-complete problem, and such a proof cannot be constructed in (e.g.) ZFC, or that polynomial-time algorithms for NP-complete problems may exist, but it is impossible to prove in ZFC that such algorithms are correct.. However, if it can be shown, using techniques of the sort that are currently known to be applicable, that the problem cannot be decided even with much weaker assumptions extending the Peano axioms (PA) for integer arithmetic, then there would necessarily exist nearly-polynomial-time algorithms for every problem in NP.. Therefore, if one believes (as most complexity theorists do) that not all problems in NP have efficient algorithms, it would follow that proofs of independence using those techniques cannot be possible. Additionally, this result implies that proving independence from PA or ZFC using currently known techniques is no easier than proving the existence of efficient algorithms for all problems in NP.\n\nClaimed solutions While the P versus NP problem is generally considered unsolved, many amateur and some professional researchers have claimed solutions. Gerhard J. Woeginger has a comprehensive list. As of 2018, this list contained 62 purported proofs of P \n NP, 50 of P ≠ NP, 2 proofs the problem is unprovable, and one proof that it's undecidable.  An August 2010 claim of proof that P ≠ NP, by Vinay Deolalikar, a researcher at HP Labs, received heavy Internet and press attention after being initially described as \" to be a relatively serious attempt\" by two leading specialists. The proof has been reviewed publicly by academics,Science News, [http://www.sciencenews.org/index/generic/activity/view/id/63252/title/Crowdsourcing_peer_review \"Crowdsourcing peer review\"] and Neil Immerman, an expert in the field, has pointed out two possibly fatal errors in the proof.\nIn September 2010, Deolalikar was reported to be working on a detailed expansion of his attempted proof. However, opinions expressed by several notable theoretical computer scientists indicate that the attempted proof is neither correct nor a significant advancement in the understanding of the problem.Gödel’s Lost Letter and P=NP, [http://rjlipton.wordpress.com/2010/08/10/update-on-deolalikars-proof-that-p%E2%89%A0np/#comment-4885 Update on Deolalikar’s Proof that P≠NP]  This assessment prompted a May 2013 The New Yorker article to call the proof attempt \"thoroughly discredited\".\n\nLogical characterizations\n\nThe P = NP problem can be restated in terms of expressible certain classes of logical statements, as a result of work in descriptive complexity.\n\nConsider all languages of finite structures with a fixed signature including a linear order relation. Then, all such languages in P can be expressed in first-order logic with the addition of a suitable least fixed-point combinator. Effectively, this, in combination with the order, allows the definition of recursive functions. As long as the signature contains at least one predicate or function in addition to the distinguished order relation, so that the amount of space taken to store such finite structures is actually polynomial in the number of elements in the structure, this precisely characterizes P.\n\nSimilarly, NP is the set of languages expressible in existential second-order logic—that is, second-order logic restricted to exclude universal quantification over relations, functions, and subsets. The languages in the polynomial hierarchy, PH, correspond to all of second-order logic. Thus, the question \"is P a proper subset of NP\" can be reformulated as \"is existential second-order logic able to describe languages (of finite linearly ordered structures with nontrivial signature) that first-order logic with least fixed point cannot?\".Elvira Mayordomo. [http://www.unizar.es/acz/05Publicaciones/Monografias/MonografiasPublicadas/Monografia26/057Mayordomo.pdf \"P versus NP\"] Monografías de la Real Academia de Ciencias de Zaragoza 26: 57–68 (2004). The word \"existential\" can even be dropped from the previous characterization, since P NP if and only if P \n PH (as the former would establish that NP co-NP, which in turn implies that NP \n PH).\n\nPolynomial-time algorithms\n\nNo algorithm for any NP-complete problem is known to run in polynomial time. However, there are algorithms known for NP-complete problems with the property that if P NP, then the algorithm runs in polynomial time on accepting instances (although with enormous constants, making the algorithm impractical). However, these algorithms do not qualify as polynomial time because their running time on rejecting instances are not polynomial. The following algorithm, due to Levin (without any citation), is such an example below. It correctly accepts the NP-complete language SUBSET-SUM. It runs in polynomial time on inputs that are in SUBSET-SUM if and only if P \n NP:\n\n // Algorithm that accepts the NP-complete language SUBSET-SUM.\n //\n // this is a polynomial-time algorithm if and only if P = NP.\n //\n // \"Polynomial-time\" means it returns \"yes\" in polynomial time when\n // the answer should be \"yes\", and runs forever when it is \"no\".\n //\n // Input: S = a finite set of integers\n // Output: \"yes\" if any subset of S adds up to 0.\n // Runs forever with no output otherwise.\n // Note: \"Program number P\" is the program obtained by\n // writing the integer P in binary, then\n // considering that string of bits to be a\n // program. Every possible program can be\n // generated this way, though most do nothing\n // because of syntax errors.\n FOR N = 1...∞\n   FOR P = 1...N\n     Run program number P for N steps with input S\n     IF the program outputs a list of distinct integers\n       AND the integers are all in S\n       AND the integers sum to 0\n     THEN\n       OUTPUT \"yes\" and HALT\n\nIf, and only if, P = NP, then this is a polynomial-time algorithm accepting an NP-complete language. \"Accepting\" means it gives \"yes\" answers in polynomial time, but is allowed to run forever when the answer is \"no\" (also known as a semi-algorithm).\n\nThis algorithm is enormously impractical, even if P = NP. If the shortest program that can solve SUBSET-SUM in polynomial time is b bits long, the above algorithm will try at least 2b−1 other programs first.\n\nFormal definitions\n\nP and NP\n\nConceptually speaking, a decision problem is a problem that takes as input some string w over an alphabet Σ, and outputs \"yes\" or \"no\". If there is an algorithm (say a Turing machine, or a computer program with unbounded memory) that can produce the correct answer for any input string of length n in at most cnk steps, where k and c are constants independent of the input string, then we say that the problem can be solved in polynomial time and we place it in the class P. Formally, P is defined as the set of all languages that can be decided by a deterministic polynomial-time Turing machine. That is,\n\\mathbf{P} \\{ L : L\nL(M) \\text{ for some deterministic polynomial-time Turing machine } M \\}\nwhere\nL(M) = \\{ w\\in\\Sigma^{*}: M \\text{ accepts } w \\}\nand a deterministic polynomial-time Turing machine is a deterministic Turing machine M that satisfies the following two conditions:\n\n# M halts on all input w and\n# there exists k \\in N such that T_M(n)\\in O(n^k), where O refers to the big O notation and\n:T_M(n) \\max\\{ t_M(w) : w\\in\\Sigma^{*}, |w| \n n \\}\n:t_M(w) = \\text{ number of steps }M\\text{ takes to halt on input }w.\n\nNP can be defined similarly using nondeterministic Turing machines (the traditional way). However, a modern approach to define NP is to use the concept of certificate and verifier. Formally, NP is defined as the set of languages over a finite alphabet that have a verifier that runs in polynomial time, where the notion of \"verifier\" is defined as follows.\n\nLet L be a language over a finite alphabet, Σ.\n\nL ∈ NP if, and only if, there exists a binary relation R\\subset\\Sigma^{*}\\times\\Sigma^{*} and a positive integer k such that the following two conditions are satisfied:\n\n# For all x\\in\\Sigma^{*}, x\\in L \\Leftrightarrow\\exists y\\in\\Sigma^{*} such that (x, y) ∈ R and |y|\\in O(|x|^k); and\n# the language L_{R} = \\{ x\\# y:(x,y)\\in R\\} over \\Sigma\\cup\\{\\#\\} is decidable by a deterministic Turing machine in polynomial time.\n\nA Turing machine that decides LR is called a verifier for L and a y such that (x, y) ∈ R is called a certificate of membership of x in L.\n\nIn general, a verifier does not have to be polynomial-time. However, for L to be in NP, there must be a verifier that runs in polynomial time.\n\nExample\n\nLet\n\\mathrm{COMPOSITE} \\left \\{x\\in\\mathbb{N} \\mid x\npq \\text{ for integers } p, q > 1 \\right \\}\nR = \\left \\{(x,y)\\in\\mathbb{N} \\times\\mathbb{N} \\mid 1\nClearly, the question of whether a given x is a composite is equivalent to the question of whether x is a member of COMPOSITE. It can be shown that COMPOSITE ∈ NP by verifying that it satisfies the above definition (if we identify natural numbers with their binary representations).\n\nCOMPOSITE also happens to be in P.AKS primality test\n\nNP-completeness\n\nThere are many equivalent ways of describing NP-completeness.\n\nLet L be a language over a finite alphabet Σ.\n\nL is NP-complete if, and only if, the following two conditions are satisfied:\n\n# L ∈ NP; and\n# any L′ in NP is polynomial-time-reducible to L (written as L' \\leq_{p} L), where L' \\leq_{p} L if, and only if, the following two conditions are satisfied:\n## There exists f : Σ* → Σ* such that for all w in Σ* we have: (w\\in L' \\Leftrightarrow f(w)\\in L); and\n## there exists a polynomial-time Turing machine that halts with f(w) on its tape on any input w.\n\nAlternatively, if L ∈ NP, and there is another NP-complete problem that can be polynomial-time reduced to L, then L is NP-complete. This is a common way of proving some new problem is NP-complete.\n\nPopular culture\n\nThe film Travelling Salesman, by director Timothy Lanzone, is the story of four mathematicians hired by the US government to solve the P vs. NP problem.\n\nIn the sixth episode of The Simpson's seventh season Treehouse of Horror VI, P=NP is seen shortly after Homer accidentally stumbles into the 'third dimension'.\n\nThe Elementary episode Solve for X revolves around the murders of mathematicians, who were involved in solving P versus NP.",
  "entityProperties" : [ {
    "name" : "title",
    "type" : "String",
    "values" : [ "P versus NP problem" ],
    "synthetic" : false
  }, {
    "name" : "url",
    "type" : "String",
    "values" : [ "http://en.wikipedia.org/?curid=6115" ],
    "synthetic" : false
  } ],
  "classifications" : [ "xml-export" ],
  "technicalAttributes" : {
    "technicalAttributes" : null,
    "aggregatedText" : "The P versus NP problem is a major unsolved problem in computer science. It asks whether every problem whose solution can be quickly verified (technically, verified in polynomial time) can also be solved quickly (again, in polynomial time).\n\nThe underlying issues were first discussed in the 1950s, in letters from John Forbes Nash Jr. to the National Security Agency, and from Kurt Gödel to John von Neumann. The precise statement of the P versus NP problem was introduced in 1971 by Stephen Cook in his seminal paper \"The complexity of theorem proving procedures\" and is considered by many to be the most important open problem in the field. It is one of the seven Millennium Prize Problems selected by the Clay Mathematics Institute to carry a US$1,000,000 prize for the first correct solution.\n\nThe informal term quickly, used above, means the existence of an algorithm solving the task that runs in polynomial time, such that the time to complete the task varies as a polynomial function on the size of the input to the algorithm (as opposed to, say, exponential time). The general class of questions for which some algorithm can provide an answer in polynomial time is called \"class P\" or just \"P\". For some questions, there is no known way to find an answer quickly, but if one is provided with information showing what the answer is, it is possible to verify the answer quickly. The class of questions for which an answer can be verified in polynomial time is called NP, which stands for \"nondeterministic polynomial time\".A nondeterministic Turing machine can move to a state that is not determined by the previous state.  Such a machine could solve a NP problem in polynomial time by falling into the correct answer state (by luck), then conventionally verifying it.  Such machines are not practical for solving realistic problems but can be used as theoretical models.\n\nConsider Sudoku, an example of a problem that is easy to verify, but whose answer may be difficult to compute. Given a partially filled-in Sudoku grid, of any size, is there at least one legal solution?  A proposed solution is easily verified, and the time to check a solution grows slowly (polynomially) as the grid gets bigger.  However, all known algorithms for finding solutions take, for difficult examples, time that grows exponentially as the grid gets bigger.  So Sudoku is in NP (quickly checkable) but does not seem to be in P (quickly solvable).  Thousands of other problems seem similar, fast to check but slow to solve.  Researchers have shown that a fast solution to any one of these problems could be used to build a quick solution to all the others, a property called NP-completeness.  Decades of searching have not yielded a fast solution to any of these problems, so most scientists suspect that none of these problems can be solved quickly.  However, this has never been proven.\n\nAn answer to the P = NP question would determine whether problems that can be verified in polynomial time, like Sudoku, can also be solved in polynomial time. If it turned out that P ≠ NP, it would mean that there are problems in NP  that are harder to compute than to verify: they could not be solved in polynomial time, but the answer could be verified in polynomial time.\n\nAside from being an important problem in computational theory, a proof either way would have profound implications for mathematics, cryptography, algorithm research, artificial intelligence, game theory, multimedia processing, philosophy, economics and many other fields.\n\nHistory\n\nAlthough the P versus NP problem was formally defined in 1971, there were previous inklings of the problems involved, the difficulty of proof, and the potential consequences.  In 1955, mathematician John Nash wrote a letter to the NSA, where he speculated that cracking a sufficiently complex code would require time exponential in the length of the key.  If proved (and Nash was suitably skeptical) this would imply what we today would call P ≠ NP, since a proposed key can easily be verified in polynomial time. Another mention of the underlying problem occurred in a 1956 letter written by Kurt Gödel to John von Neumann.  Gödel asked whether theorem-proving (now known to be co-NP-complete) could be solved in quadratic or linear time, and pointed out one of the most important consequences—that if so, then the discovery of mathematical proofs could be automated.\n\nContext\n\nThe relation between the complexity classes P and NP is studied in computational complexity theory, the part of the theory of computation dealing with the resources required during computation to solve a given problem. The most common resources are time (how many steps it takes to solve a problem) and space (how much memory it takes to solve a problem).\n\nIn such analysis, a model of the computer for which time must be analyzed is required. Typically such models assume that the computer is deterministic (given the computer's present state and any inputs, there is only one possible action that the computer might take) and sequential (it performs actions one after the other).\n\nIn this theory, the class P consists of all those decision problems (defined below) that can be solved on a deterministic sequential machine in an amount of time that is polynomial in the size of the input; the class NP consists of all those decision problems whose positive solutions can be verified in polynomial time given the right information, or equivalently, whose solution can be found in polynomial time on a non-deterministic machine.Sipser, Michael: Introduction to the Theory of Computation, Second Edition, International Edition, page 270. Thomson Course Technology, 2006. Definition 7.19 and Theorem 7.20. Clearly, P ⊆ NP. Arguably the biggest open question in theoretical computer science concerns the relationship between those two classes:\nIs P equal to NP?\nIn a 2002 poll of 100 researchers, 61 believed the answer to be no, 9 believed the answer is yes, and 22 were unsure; 8 believed the question may be independent of the currently accepted axioms and therefore impossible to prove or disprove.\n\nIn 2012, 10 years later, the same poll was repeated. The number of researchers who answered was 151: 126 (83%) believed the answer to be no, 12 (9%) believed the answer is yes, 5 (3%) believed the question may be independent of the currently accepted axioms and therefore impossible to prove or disprove, 8 (5%) said either don't know or don't care or don't want the answer to be yes nor the problem to be resolved.\n\nNP-completeness\n\nTo attack the P = NP question, the concept of NP-completeness is very useful. NP-complete problems are a set of problems to each of which any other NP-problem can be reduced in polynomial time, and whose solution may still be verified in polynomial time. That is, any NP problem can be transformed into any of the NP-complete problems. Informally, an NP-complete problem is an NP problem that is at least as \"tough\" as any other problem in NP.\n\nNP-hard problems are those at least as hard as NP problems, i.e., all NP problems can be reduced (in polynomial time) to them. NP-hard problems need not be in NP, i.e., they need not have solutions verifiable in polynomial time.\n\nFor instance, the Boolean satisfiability problem is NP-complete by the Cook–Levin theorem, so any instance of any problem in NP can be transformed mechanically into an instance of the Boolean satisfiability problem in polynomial time. The Boolean satisfiability problem is one of many such NP-complete problems. If any NP-complete problem is in P, then it would follow that P = NP. However, many important problems have been shown to be NP-complete, and no fast algorithm for any of them is known.\n\nBased on the definition alone it is not obvious that NP-complete problems exist; however, a trivial and contrived NP-complete problem can be formulated as follows: given a description of a Turing machine M guaranteed to halt in polynomial time, does there exist a polynomial-size input that M will accept? It is in NP because (given an input) it is simple to check whether M accepts the input by simulating M; it is NP-complete because the verifier for any particular instance of a problem in NP can be encoded as a polynomial-time machine M that takes the solution to be verified as input. Then the question of whether the instance is a yes or no instance is determined by whether a valid input exists.\n\nThe first natural problem proven to be NP-complete was the Boolean satisfiability problem. As noted above, this is the Cook–Levin theorem; its proof that satisfiability is NP-complete contains technical details about Turing machines as they relate to the definition of NP. However, after this problem was proved to be NP-complete, proof by reduction provided a simpler way to show that many other problems are also NP-complete, including the Sudoku discussed earlier. In this case, the proof shows that  a solution of Sudoku in polynomial time, could also be used to complete Latin squares in polynomial time.  This in turn gives a solution to the problem of partitioning tri-partitite graphs into triangles, which could then be used to find solutions for 3-sat, which then provides a solution for general boolean satisfiability.  So a polynomial time solution to Sudoku leads, by a series of mechanical transformations, to a polynomial time solution of satisfiability, which in turn can be used to solve any other NP-complete problem in polynomial time.  Using transformations like this, a vast class of seemingly unrelated problems are all reducible to one another, and are in a sense \"the same problem\".\n\nHarder problems\n\nAlthough it is unknown whether P NP, problems outside of P are known. A number of succinct problems (problems that operate not on normal input, but on a computational description of the input) are known to be EXPTIME-complete. Because it can be shown that P ≠ EXPTIME, these problems are outside P, and so require more than polynomial time. In fact, by the time hierarchy theorem, they cannot be solved in significantly less than exponential time. Examples include finding a perfect strategy for chess (on an N × N board) and some other board games.\n\nThe problem of deciding the truth of a statement in Presburger arithmetic requires even more time. Fischer and Rabin proved in 1974 that every algorithm that decides the truth of Presburger statements of length n has a runtime of at least 2^{2^{cn}} for some constant c. Hence, the problem is known to need more than exponential run time. Even more difficult are the undecidable problems, such as the halting problem. They cannot be completely solved by any algorithm, in the sense that for any particular algorithm there is at least one input for which that algorithm will not produce the right answer; it will either produce the wrong answer, finish without giving a conclusive answer, or otherwise run forever without producing any answer at all.\n\nIt is also possible to consider questions other than decision problems.  One such class, consisting of counting problems, is called #P: whereas an NP problem asks \"Are there any solutions?\", the corresponding #P problem asks \"How many solutions are there?\"  Clearly, a #P problem must be at least as hard as the corresponding NP problem, since a count of possible solutions immediately tells if at least one solution exists, if the count is greater than zero. Surprisingly, some #P problems that are believed to be difficult correspond to easy (for example linear-time) P problems.  For these problems, it's very easy to tell whether solutions exist, but thought to be very hard to tell how many.  Many of these problems are #P-complete, and hence among the hardest problems in #P, since a polynomial time solution to any of them would allow a polynomial time solution to all other #P problems.\n\nProblems in NP not known to be in P or NP-complete\n\nIt was shown by Ladner that if P ≠ NP then there exist problems in NP that are neither in P nor NP-complete. Such problems are called NP-intermediate problems. The graph isomorphism problem, the discrete logarithm problem and the integer factorization problem are examples of problems believed to be NP-intermediate. They are some of the very few NP problems not known to be in P or to be NP-complete.\n\nThe graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic. An important unsolved problem in complexity theory is whether the graph isomorphism problem is in P, NP-complete, or NP-intermediate. The answer is not known, but it is believed that the problem is at least not NP-complete. If graph isomorphism is NP-complete, the polynomial time hierarchy collapses to its second level. Since it is widely believed that the polynomial hierarchy does not collapse to any finite level, it is believed that graph isomorphism is not NP-complete. The best algorithm for this problem, due to László Babai and Eugene Luks, has run time 2O(√nlog(n)) for graphs with n vertices.\n\nThe integer factorization problem is the computational problem of determining the prime factorization of a given integer. Phrased as a decision problem, it is the problem of deciding whether the input has a factor less than k. No efficient integer factorization algorithm is known, and this fact forms the basis of several modern cryptographic systems, such as the RSA algorithm. The integer factorization problem is in NP and in co-NP (and even in UP and co-UPLance Fortnow. Computational Complexity Blog: [http://weblog.fortnow.com/2002/09/complexity-class-of-week-factoring.html Complexity Class of the Week: Factoring]. 13 September 2002.). If the problem is NP-complete, the polynomial time hierarchy will collapse to its first level (i.e., NP = co-NP). The best known algorithm for integer factorization is the general number field sieve, which takes expected time\n\nO\\left (\\exp \\left ( \\left (\\tfrac{64n}{9} \\log(2) \\right )^{\\frac{1}{3}} \\left ( \\log(n\\log(2)) \\right )^{\\frac{2}{3}} \\right) \\right )\n\nto factor an n-bit integer. However, the best known quantum algorithm for this problem, Shor's algorithm, does run in polynomial time, although this does not indicate where the problem lies with respect to non-quantum complexity classes.\n\nDoes P mean \"easy\"?\n\nAll of the above discussion has assumed that P means \"easy\" and \"not in P\" means \"hard\", an assumption known as Cobham's thesis. It is a common and reasonably accurate assumption in complexity theory; however, it has some caveats.\n\nFirst, it is not always true in practice. A theoretical polynomial algorithm may have extremely large constant factors or exponents thus rendering it impractical. On the other hand, even if a problem is shown to be NP-complete, and even if P ≠ NP, there may still be effective approaches to tackling the problem in practice. There are algorithms for many NP-complete problems, such as the knapsack problem, the traveling salesman problem and the Boolean satisfiability problem, that can solve to optimality many real-world instances in reasonable time. The empirical average-case complexity (time vs. problem size) of such algorithms can be surprisingly low.  An example is the simplex algorithm in linear programming, which works surprisingly well in practice; despite having exponential worst-case time complexity it runs on par with the best known polynomial-time algorithms.\n\nSecond, there are types of computations which do not conform to the Turing machine model on which P and NP are defined, such as quantum computation and randomized algorithms.\n\nReasons to believe P ≠ NP\n\nAccording to polls, most computer scientists believe that P ≠ NP. A key reason for this belief is that after decades of studying these problems no one has been able to find a polynomial-time algorithm for any of more than 3000 important known NP-complete problems (see List of NP-complete problems). These algorithms were sought long before the concept of NP-completeness was even defined (Karp's 21 NP-complete problems, among the first found, were all well-known existing problems at the time they were shown to be NP-complete). Furthermore, the result P \n NP would imply many other startling results that are currently believed to be false, such as NP co-NP and P \n PH.\n\nIt is also intuitively argued that the existence of problems that are hard to solve but for which the solutions are easy to verify matches real-world experience., point 9.\n\nOn the other hand, some researchers believe that there is overconfidence in believing P ≠ NP and that researchers should explore proofs of P NP as well. For example, in 2002 these statements were made:\n\nConsequences of solution\n\nOne of the reasons the problem attracts so much attention is the consequences of the answer.  Either direction of resolution would advance theory enormously, and perhaps have huge practical consequences as well.\n\nP \n NPA proof that P \n NP could have stunning practical consequences if the proof leads to efficient methods for solving some of the important problems in NP. It is also possible that a proof would not lead directly to efficient methods, perhaps if the proof is non-constructive, or the size of the bounding polynomial is too big to be efficient in practice. The consequences, both positive and negative, arise since various NP-complete problems are fundamental in many fields.\n\nCryptography, for example, relies on certain problems being difficult. A constructive and efficient solutionExactly how efficient a solution must be to pose a threat to cryptography depends on the details.  A solution of O(N^2) or better and a reasonable constant term would be disastrous.  On the other hand, a solution that is \\Omega(N^4) or worse in almost all cases would not pose an immediate practical danger. to an NP-complete problem such as 3-SAT would break most existing cryptosystems including:\n* public-key cryptography,See  for a reduction of factoring to SAT.  A 512 bit factoring problem (8400 MIPS-years when factored) translates to a SAT problem of 63,652 variables and 406,860 clauses. a foundation for many modern security applications such as secure financial transactions over the Internet; and\n* symmetric ciphers such as AES or 3DES,See, for example,  in which an instance of DES is encoded as a SAT problem with 10336 variables and 61935 clauses.  A 3DES problem instance would be about 3 times this size. used for the encryption of communications data.\n* cryptographic hashing as the problem of finding a pre-image that hashes to a given valueFind a messageM that when hashed by the function H() gives a digest h, or H(M)h must be difficult to be useful, and ideally should require exponential time. However, if P\nNP, then finding a pre-image M can be done in polynomial time, through reduction to SAT.\nThese would need to be modified or replaced by information-theoretically secure solutions not inherently based on P-NP equivalence.\n\nOn the other hand, there are enormous positive consequences that would follow from rendering tractable many currently mathematically intractable problems. For instance, many problems in operations research are NP-complete, such as some types of integer programming and the travelling salesman problem. Efficient solutions to these problems would have enormous implications for logistics. Many other important problems, such as some problems in protein structure prediction, are also NP-complete; if these problems were efficiently solvable it could spur considerable advances in life sciences and biotechnology.\n\nBut such changes may pale in significance compared to the revolution an efficient method for solving NP-complete problems would cause in mathematics itself. Gödel, in his early thoughts on computational complexity, noted that a mechanical method that could solve any problem would revolutionize mathematics:History of this letter and its translation from  From pages 359–376 of Optimization Stories, M. Grötschel (editor), a special issue of ¨ Documenta Mathematica, published in August 2012 and distributed to attendees at the 21st International Symposium on Mathematical Programming in Berlin.\n\nSimilarly, Stephen Cook says\n\nResearch mathematicians spend their careers trying to prove theorems, and some proofs have taken decades or even centuries to find after problems have been stated—for instance, Fermat's Last Theorem took over three centuries to prove. A method that is guaranteed to find proofs to theorems, should one exist of a \"reasonable\" size, would essentially end this struggle.\n\nDonald Knuth has stated that he has come to believe that P = NP, but is reserved about the impact of a possible proof:\n\nP ≠ NP\n\nA proof that showed that P ≠ NP would lack the practical computational benefits of a proof that P = NP, but would nevertheless represent a very significant advance in computational complexity theory and provide guidance for future research. It would allow one to show in a formal way that many common problems cannot be solved efficiently, so that the attention of researchers can be focused on partial solutions or solutions to other problems. Due to widespread belief in P ≠ NP, much of this focusing of research has already taken place.\n\nAlso P ≠ NP still leaves open the average-case complexity of hard problems in NP.  For example, it is possible that SAT requires exponential time in the worst case, but that almost all randomly selected instances of it are efficiently solvable. Russell Impagliazzo has described five hypothetical \"worlds\" that could result from different possible resolutions to the average-case complexity question.R. Impagliazzo, [http://cseweb.ucsd.edu/~russell/average.ps \"A personal view of average-case complexity,\"] sct, pp.134, 10th Annual Structure in Complexity Theory Conference (SCT'95), 1995  These range from \"Algorithmica\", where P = NP and problems like SAT can be solved efficiently in all instances, to \"Cryptomania\", where P ≠ NP and generating hard instances of problems outside P is easy, with three intermediate possibilities reflecting different possible distributions of difficulty over instances of NP-hard problems.  The \"world\" where P ≠ NP but all problems in NP are tractable in the average case is called \"Heuristica\" in the paper. A Princeton University workshop in 2009 studied the status of the five worlds.\n\nResults about difficulty of proof\n\nAlthough the P NP problem itself remains open despite a million-dollar prize and a huge amount of dedicated research, efforts to solve the problem have led to several new techniques.  In particular, some of the most fruitful research related to the P \n NP problem has been in showing that existing proof techniques are not powerful enough to answer the question, thus suggesting that novel technical approaches are required.\n\nAs additional evidence for the difficulty of the problem, essentially all known proof techniques in computational complexity theory fall into one of the following classifications, each of which is known to be insufficient to prove that P ≠ NP:\n\nThese barriers are another reason why NP-complete problems are useful: if a polynomial-time algorithm can be demonstrated for an NP-complete problem, this would solve the P = NP problem in a way not excluded by the above results.\n\nThese barriers have also led some computer scientists to suggest that the P versus NP problem may be independent of standard axiom systems like ZFC (cannot be proved or disproved within them). The interpretation of an independence result could be that either no polynomial-time algorithm exists for any NP-complete problem, and such a proof cannot be constructed in (e.g.) ZFC, or that polynomial-time algorithms for NP-complete problems may exist, but it is impossible to prove in ZFC that such algorithms are correct.. However, if it can be shown, using techniques of the sort that are currently known to be applicable, that the problem cannot be decided even with much weaker assumptions extending the Peano axioms (PA) for integer arithmetic, then there would necessarily exist nearly-polynomial-time algorithms for every problem in NP.. Therefore, if one believes (as most complexity theorists do) that not all problems in NP have efficient algorithms, it would follow that proofs of independence using those techniques cannot be possible. Additionally, this result implies that proving independence from PA or ZFC using currently known techniques is no easier than proving the existence of efficient algorithms for all problems in NP.\n\nClaimed solutions While the P versus NP problem is generally considered unsolved, many amateur and some professional researchers have claimed solutions. Gerhard J. Woeginger has a comprehensive list. As of 2018, this list contained 62 purported proofs of P \n NP, 50 of P ≠ NP, 2 proofs the problem is unprovable, and one proof that it's undecidable.  An August 2010 claim of proof that P ≠ NP, by Vinay Deolalikar, a researcher at HP Labs, received heavy Internet and press attention after being initially described as \" to be a relatively serious attempt\" by two leading specialists. The proof has been reviewed publicly by academics,Science News, [http://www.sciencenews.org/index/generic/activity/view/id/63252/title/Crowdsourcing_peer_review \"Crowdsourcing peer review\"] and Neil Immerman, an expert in the field, has pointed out two possibly fatal errors in the proof.\nIn September 2010, Deolalikar was reported to be working on a detailed expansion of his attempted proof. However, opinions expressed by several notable theoretical computer scientists indicate that the attempted proof is neither correct nor a significant advancement in the understanding of the problem.Gödel’s Lost Letter and P=NP, [http://rjlipton.wordpress.com/2010/08/10/update-on-deolalikars-proof-that-p%E2%89%A0np/#comment-4885 Update on Deolalikar’s Proof that P≠NP]  This assessment prompted a May 2013 The New Yorker article to call the proof attempt \"thoroughly discredited\".\n\nLogical characterizations\n\nThe P = NP problem can be restated in terms of expressible certain classes of logical statements, as a result of work in descriptive complexity.\n\nConsider all languages of finite structures with a fixed signature including a linear order relation. Then, all such languages in P can be expressed in first-order logic with the addition of a suitable least fixed-point combinator. Effectively, this, in combination with the order, allows the definition of recursive functions. As long as the signature contains at least one predicate or function in addition to the distinguished order relation, so that the amount of space taken to store such finite structures is actually polynomial in the number of elements in the structure, this precisely characterizes P.\n\nSimilarly, NP is the set of languages expressible in existential second-order logic—that is, second-order logic restricted to exclude universal quantification over relations, functions, and subsets. The languages in the polynomial hierarchy, PH, correspond to all of second-order logic. Thus, the question \"is P a proper subset of NP\" can be reformulated as \"is existential second-order logic able to describe languages (of finite linearly ordered structures with nontrivial signature) that first-order logic with least fixed point cannot?\".Elvira Mayordomo. [http://www.unizar.es/acz/05Publicaciones/Monografias/MonografiasPublicadas/Monografia26/057Mayordomo.pdf \"P versus NP\"] Monografías de la Real Academia de Ciencias de Zaragoza 26: 57–68 (2004). The word \"existential\" can even be dropped from the previous characterization, since P NP if and only if P \n PH (as the former would establish that NP co-NP, which in turn implies that NP \n PH).\n\nPolynomial-time algorithms\n\nNo algorithm for any NP-complete problem is known to run in polynomial time. However, there are algorithms known for NP-complete problems with the property that if P NP, then the algorithm runs in polynomial time on accepting instances (although with enormous constants, making the algorithm impractical). However, these algorithms do not qualify as polynomial time because their running time on rejecting instances are not polynomial. The following algorithm, due to Levin (without any citation), is such an example below. It correctly accepts the NP-complete language SUBSET-SUM. It runs in polynomial time on inputs that are in SUBSET-SUM if and only if P \n NP:\n\n // Algorithm that accepts the NP-complete language SUBSET-SUM.\n //\n // this is a polynomial-time algorithm if and only if P = NP.\n //\n // \"Polynomial-time\" means it returns \"yes\" in polynomial time when\n // the answer should be \"yes\", and runs forever when it is \"no\".\n //\n // Input: S = a finite set of integers\n // Output: \"yes\" if any subset of S adds up to 0.\n // Runs forever with no output otherwise.\n // Note: \"Program number P\" is the program obtained by\n // writing the integer P in binary, then\n // considering that string of bits to be a\n // program. Every possible program can be\n // generated this way, though most do nothing\n // because of syntax errors.\n FOR N = 1...∞\n   FOR P = 1...N\n     Run program number P for N steps with input S\n     IF the program outputs a list of distinct integers\n       AND the integers are all in S\n       AND the integers sum to 0\n     THEN\n       OUTPUT \"yes\" and HALT\n\nIf, and only if, P = NP, then this is a polynomial-time algorithm accepting an NP-complete language. \"Accepting\" means it gives \"yes\" answers in polynomial time, but is allowed to run forever when the answer is \"no\" (also known as a semi-algorithm).\n\nThis algorithm is enormously impractical, even if P = NP. If the shortest program that can solve SUBSET-SUM in polynomial time is b bits long, the above algorithm will try at least 2b−1 other programs first.\n\nFormal definitions\n\nP and NP\n\nConceptually speaking, a decision problem is a problem that takes as input some string w over an alphabet Σ, and outputs \"yes\" or \"no\". If there is an algorithm (say a Turing machine, or a computer program with unbounded memory) that can produce the correct answer for any input string of length n in at most cnk steps, where k and c are constants independent of the input string, then we say that the problem can be solved in polynomial time and we place it in the class P. Formally, P is defined as the set of all languages that can be decided by a deterministic polynomial-time Turing machine. That is,\n\\mathbf{P} \\{ L : L\nL(M) \\text{ for some deterministic polynomial-time Turing machine } M \\}\nwhere\nL(M) = \\{ w\\in\\Sigma^{*}: M \\text{ accepts } w \\}\nand a deterministic polynomial-time Turing machine is a deterministic Turing machine M that satisfies the following two conditions:\n\n# M halts on all input w and\n# there exists k \\in N such that T_M(n)\\in O(n^k), where O refers to the big O notation and\n:T_M(n) \\max\\{ t_M(w) : w\\in\\Sigma^{*}, |w| \n n \\}\n:t_M(w) = \\text{ number of steps }M\\text{ takes to halt on input }w.\n\nNP can be defined similarly using nondeterministic Turing machines (the traditional way). However, a modern approach to define NP is to use the concept of certificate and verifier. Formally, NP is defined as the set of languages over a finite alphabet that have a verifier that runs in polynomial time, where the notion of \"verifier\" is defined as follows.\n\nLet L be a language over a finite alphabet, Σ.\n\nL ∈ NP if, and only if, there exists a binary relation R\\subset\\Sigma^{*}\\times\\Sigma^{*} and a positive integer k such that the following two conditions are satisfied:\n\n# For all x\\in\\Sigma^{*}, x\\in L \\Leftrightarrow\\exists y\\in\\Sigma^{*} such that (x, y) ∈ R and |y|\\in O(|x|^k); and\n# the language L_{R} = \\{ x\\# y:(x,y)\\in R\\} over \\Sigma\\cup\\{\\#\\} is decidable by a deterministic Turing machine in polynomial time.\n\nA Turing machine that decides LR is called a verifier for L and a y such that (x, y) ∈ R is called a certificate of membership of x in L.\n\nIn general, a verifier does not have to be polynomial-time. However, for L to be in NP, there must be a verifier that runs in polynomial time.\n\nExample\n\nLet\n\\mathrm{COMPOSITE} \\left \\{x\\in\\mathbb{N} \\mid x\npq \\text{ for integers } p, q > 1 \\right \\}\nR = \\left \\{(x,y)\\in\\mathbb{N} \\times\\mathbb{N} \\mid 1\nClearly, the question of whether a given x is a composite is equivalent to the question of whether x is a member of COMPOSITE. It can be shown that COMPOSITE ∈ NP by verifying that it satisfies the above definition (if we identify natural numbers with their binary representations).\n\nCOMPOSITE also happens to be in P.AKS primality test\n\nNP-completeness\n\nThere are many equivalent ways of describing NP-completeness.\n\nLet L be a language over a finite alphabet Σ.\n\nL is NP-complete if, and only if, the following two conditions are satisfied:\n\n# L ∈ NP; and\n# any L′ in NP is polynomial-time-reducible to L (written as L' \\leq_{p} L), where L' \\leq_{p} L if, and only if, the following two conditions are satisfied:\n## There exists f : Σ* → Σ* such that for all w in Σ* we have: (w\\in L' \\Leftrightarrow f(w)\\in L); and\n## there exists a polynomial-time Turing machine that halts with f(w) on its tape on any input w.\n\nAlternatively, if L ∈ NP, and there is another NP-complete problem that can be polynomial-time reduced to L, then L is NP-complete. This is a common way of proving some new problem is NP-complete.\n\nPopular culture\n\nThe film Travelling Salesman, by director Timothy Lanzone, is the story of four mathematicians hired by the US government to solve the P vs. NP problem.\n\nIn the sixth episode of The Simpson's seventh season Treehouse of Horror VI, P=NP is seen shortly after Homer accidentally stumbles into the 'third dimension'.\n\nThe Elementary episode Solve for X revolves around the murders of mathematicians, who were involved in solving P versus NP. P versus NP problem. http://en.wikipedia.org/?curid=6115."
  }
}
