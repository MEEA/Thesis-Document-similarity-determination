{
  "datasourceIdentifier" : "awesome wiki export",
  "backlink" : "http://en.wikipedia.org/?curid=15237",
  "eid" : "1acb5670-52b2-11e8-ad1f-273b2f3b71fa",
  "loadTime" : 1525778554199,
  "textBody" : "In computational mathematics, an iterative method is a mathematical procedure that uses an initial guess to generate a sequence of improving approximate solutions for a class of problems, in which the n-th approximation is derived from the previous ones. A specific implementation of an iterative method, including the termination criteria, is an algorithm of the iterative method. An iterative method is called convergent if the corresponding sequence converges for given initial approximations. A mathematically rigorous convergence analysis of an iterative method is usually performed; however, heuristic-based iterative methods are also common. \n\nIn contrast, direct methods attempt to solve the problem by a finite  sequence of operations. In the absence of rounding errors, direct methods would deliver an exact solution (like solving a linear system of equations A\\mathbf{x}=\\mathbf{b} by Gaussian elimination). Iterative methods are often the only choice for nonlinear equations. However, iterative methods are often useful even for linear problems involving a large number of variables (sometimes of the order of millions), where direct methods would be prohibitively expensive (and in some cases impossible) even with the best available computing power.\n\nAttractive fixed points\n\nIf an equation can be put into the form f(x) x, and a solution x is an attractive fixed point of the function f, then one may begin with a point x1 in the basin of attraction of x, and let xn+1 \n f(xn) for n ≥ 1, and the sequence {xn}n ≥ 1 will converge to the solution x. Here xn is the nth approximation or iteration of x and xn+1 is the next or n + 1 iteration of x.  Alternately, superscripts in parentheses are often used in numerical methods, so as not to interfere with subscripts with other meanings.  (For example, x(n+1) = f(x(n)).) If the function f is continuously differentiable, a sufficient condition for convergence is that the spectral radius of the derivative is strictly bounded by one in a neighborhood of the fixed point.  If this condition holds at the fixed point, then a sufficiently small neighborhood (basin of attraction) must exist.\n\nLinear systems\n\nIn the case of a system of linear equations, the two main classes of iterative methods are the stationary iterative methods, and the more general Krylov subspace methods.\n\nStationary iterative methods\n\nIntroduction \n\nStationary iterative methods solve a linear system with an operator approximating the original one; and based on a measurement of the error in the result (the residual), form a \"correction equation\" for which this process is repeated. While these methods are simple to derive, implement, and analyze, convergence is only guaranteed for a limited class of matrices. \n\nDefinition\n\nAn iterative method is defined by\n\n  \\mathbf{x}^{k+1} := \\Psi (  \\mathbf{x}^k ) \\,, \\quad k\\geq0\n\nand for a given linear system  A\\mathbf x= \\mathbf b  with exact solution  \\mathbf{x}^*  the error by\n\n  \\mathbf{e}^k := \\mathbf{x}^k - \\mathbf{x}^* \\,, \\quad k\\geq0\\,.\n\nAn iterative method is called linear if there exists a matrix  C \\in \\R^{n\\times x}  s.t.\n\n  \\mathbf{e}^{k+1} = C  \\mathbf{e}^k   \\quad \\forall \\, k\\geq0\n\nand this matrix is called iteration matrix.\nAn iterative method with a given iteration matrix  C  is called convergent if the following holds\n\n  \\lim_{k\\rightarrow \\infty} C^k=0\\,.\n\nAn important theorem states that for a given iterative method and its iteration matrix  C  it is convergent if and only if its spectral radius  \\rho(C)   is smaller than unity, i.e.\n\n  \\rho(C) \n\nThe basic iterative methods work by a splitting up the matrix  A  into\n\n  A = M - N\n\nand here the matrix  M  should be easily invertible.\nThe iterative methods are now defined as\n\n  M \\mathbf{x}^{k+1} = N \\mathbf{x}^k + b \\,, \\quad k\\geq0\\,.\n\nFrom this follows that the iteration matrix is given by\n\n  C I - M^{-1}A \n M^{-1}N\\,.\n\nExamples\n\nBasic examples of stationary iterative methods use a splitting of the matrix  A  such as\n\n  A M-N\\,,\\quad M :\n D \\text{diag}( (a_{ii})_i)\\,,\\quad N:\nL+U\n\nwhere  D  is only the diagonal part of  A , and  L  is the strict lower triangular part of  A .\nRespectively,  U  is the upper triangular part of  A .\n* Richardson method:  M:\\frac{1}{\\omega} I\\,,  \\quad C \n I-\\omega A  \\quad \\text{for} \\quad  \\omega\\in\\R, \\omega\\neq0 \n* Jacobi method\n* Gauss–Seidel method\n* Successive over-relaxation method\nLinear stationary iterative methods are also called relaxation methods.\n\nKrylov subspace methods\n\nKrylov subspace methods work by forming a basis of the sequence of successive matrix powers times the initial residual (the Krylov sequence). The approximations to the solution are then formed by minimizing the residual over the subspace formed. The prototypical method in this class is the conjugate gradient method (CG). Other methods are the generalized minimal residual method (GMRES) and the biconjugate gradient method (BiCG).\n\nConvergence of Krylov subspace methods\n\nSince these methods form a basis, it is evident that the method converges in N iterations, where N is the system size. However, in the presence of rounding errors this statement does not hold; moreover, in practice N can be very large, and the iterative process reaches sufficient accuracy already far earlier. The analysis of these methods is hard, depending on a complicated function of the spectrum of the operator.\n\nPreconditioners\n\nThe approximating operator that appears in stationary iterative methods can also be incorporated in Krylov subspace methods such as GMRES (alternatively, preconditioned Krylov methods can be considered as accelerations of stationary iterative methods), where they become transformations of the original operator to a presumably better conditioned one. The construction of preconditioners is a large research area.\n\nHistory\n\nProbably the first iterative method for solving a linear system appeared in a letter of Gauss to a student of his.  He proposed solving a 4-by-4 system of equations by repeatedly solving the component in which the residual was the largest. \n\nThe theory of stationary iterative methods was solidly established with the work of D.M. Young starting in the 1950s. The Conjugate Gradient method was also invented in the 1950s, with independent developments by Cornelius Lanczos, Magnus Hestenes and Eduard Stiefel, but its nature and applicability were misunderstood at the time. Only in the 1970s was it realized that conjugacy based methods work very well for partial differential equations, especially the elliptic type.",
  "entityProperties" : [ {
    "name" : "title",
    "type" : "String",
    "values" : [ "Iterative method" ],
    "synthetic" : false
  }, {
    "name" : "url",
    "type" : "String",
    "values" : [ "http://en.wikipedia.org/?curid=15237" ],
    "synthetic" : false
  } ],
  "classifications" : [ "xml-export" ],
  "technicalAttributes" : {
    "technicalAttributes" : null,
    "aggregatedText" : "In computational mathematics, an iterative method is a mathematical procedure that uses an initial guess to generate a sequence of improving approximate solutions for a class of problems, in which the n-th approximation is derived from the previous ones. A specific implementation of an iterative method, including the termination criteria, is an algorithm of the iterative method. An iterative method is called convergent if the corresponding sequence converges for given initial approximations. A mathematically rigorous convergence analysis of an iterative method is usually performed; however, heuristic-based iterative methods are also common. \n\nIn contrast, direct methods attempt to solve the problem by a finite  sequence of operations. In the absence of rounding errors, direct methods would deliver an exact solution (like solving a linear system of equations A\\mathbf{x}=\\mathbf{b} by Gaussian elimination). Iterative methods are often the only choice for nonlinear equations. However, iterative methods are often useful even for linear problems involving a large number of variables (sometimes of the order of millions), where direct methods would be prohibitively expensive (and in some cases impossible) even with the best available computing power.\n\nAttractive fixed points\n\nIf an equation can be put into the form f(x) x, and a solution x is an attractive fixed point of the function f, then one may begin with a point x1 in the basin of attraction of x, and let xn+1 \n f(xn) for n ≥ 1, and the sequence {xn}n ≥ 1 will converge to the solution x. Here xn is the nth approximation or iteration of x and xn+1 is the next or n + 1 iteration of x.  Alternately, superscripts in parentheses are often used in numerical methods, so as not to interfere with subscripts with other meanings.  (For example, x(n+1) = f(x(n)).) If the function f is continuously differentiable, a sufficient condition for convergence is that the spectral radius of the derivative is strictly bounded by one in a neighborhood of the fixed point.  If this condition holds at the fixed point, then a sufficiently small neighborhood (basin of attraction) must exist.\n\nLinear systems\n\nIn the case of a system of linear equations, the two main classes of iterative methods are the stationary iterative methods, and the more general Krylov subspace methods.\n\nStationary iterative methods\n\nIntroduction \n\nStationary iterative methods solve a linear system with an operator approximating the original one; and based on a measurement of the error in the result (the residual), form a \"correction equation\" for which this process is repeated. While these methods are simple to derive, implement, and analyze, convergence is only guaranteed for a limited class of matrices. \n\nDefinition\n\nAn iterative method is defined by\n\n  \\mathbf{x}^{k+1} := \\Psi (  \\mathbf{x}^k ) \\,, \\quad k\\geq0\n\nand for a given linear system  A\\mathbf x= \\mathbf b  with exact solution  \\mathbf{x}^*  the error by\n\n  \\mathbf{e}^k := \\mathbf{x}^k - \\mathbf{x}^* \\,, \\quad k\\geq0\\,.\n\nAn iterative method is called linear if there exists a matrix  C \\in \\R^{n\\times x}  s.t.\n\n  \\mathbf{e}^{k+1} = C  \\mathbf{e}^k   \\quad \\forall \\, k\\geq0\n\nand this matrix is called iteration matrix.\nAn iterative method with a given iteration matrix  C  is called convergent if the following holds\n\n  \\lim_{k\\rightarrow \\infty} C^k=0\\,.\n\nAn important theorem states that for a given iterative method and its iteration matrix  C  it is convergent if and only if its spectral radius  \\rho(C)   is smaller than unity, i.e.\n\n  \\rho(C) \n\nThe basic iterative methods work by a splitting up the matrix  A  into\n\n  A = M - N\n\nand here the matrix  M  should be easily invertible.\nThe iterative methods are now defined as\n\n  M \\mathbf{x}^{k+1} = N \\mathbf{x}^k + b \\,, \\quad k\\geq0\\,.\n\nFrom this follows that the iteration matrix is given by\n\n  C I - M^{-1}A \n M^{-1}N\\,.\n\nExamples\n\nBasic examples of stationary iterative methods use a splitting of the matrix  A  such as\n\n  A M-N\\,,\\quad M :\n D \\text{diag}( (a_{ii})_i)\\,,\\quad N:\nL+U\n\nwhere  D  is only the diagonal part of  A , and  L  is the strict lower triangular part of  A .\nRespectively,  U  is the upper triangular part of  A .\n* Richardson method:  M:\\frac{1}{\\omega} I\\,,  \\quad C \n I-\\omega A  \\quad \\text{for} \\quad  \\omega\\in\\R, \\omega\\neq0 \n* Jacobi method\n* Gauss–Seidel method\n* Successive over-relaxation method\nLinear stationary iterative methods are also called relaxation methods.\n\nKrylov subspace methods\n\nKrylov subspace methods work by forming a basis of the sequence of successive matrix powers times the initial residual (the Krylov sequence). The approximations to the solution are then formed by minimizing the residual over the subspace formed. The prototypical method in this class is the conjugate gradient method (CG). Other methods are the generalized minimal residual method (GMRES) and the biconjugate gradient method (BiCG).\n\nConvergence of Krylov subspace methods\n\nSince these methods form a basis, it is evident that the method converges in N iterations, where N is the system size. However, in the presence of rounding errors this statement does not hold; moreover, in practice N can be very large, and the iterative process reaches sufficient accuracy already far earlier. The analysis of these methods is hard, depending on a complicated function of the spectrum of the operator.\n\nPreconditioners\n\nThe approximating operator that appears in stationary iterative methods can also be incorporated in Krylov subspace methods such as GMRES (alternatively, preconditioned Krylov methods can be considered as accelerations of stationary iterative methods), where they become transformations of the original operator to a presumably better conditioned one. The construction of preconditioners is a large research area.\n\nHistory\n\nProbably the first iterative method for solving a linear system appeared in a letter of Gauss to a student of his.  He proposed solving a 4-by-4 system of equations by repeatedly solving the component in which the residual was the largest. \n\nThe theory of stationary iterative methods was solidly established with the work of D.M. Young starting in the 1950s. The Conjugate Gradient method was also invented in the 1950s, with independent developments by Cornelius Lanczos, Magnus Hestenes and Eduard Stiefel, but its nature and applicability were misunderstood at the time. Only in the 1970s was it realized that conjugacy based methods work very well for partial differential equations, especially the elliptic type. Iterative method. http://en.wikipedia.org/?curid=15237."
  }
}
