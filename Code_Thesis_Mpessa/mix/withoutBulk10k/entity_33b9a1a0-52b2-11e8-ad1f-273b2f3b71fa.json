{
  "datasourceIdentifier" : "awesome wiki export",
  "backlink" : "http://en.wikipedia.org/?curid=18572",
  "eid" : "33b9a1a0-52b2-11e8-ad1f-273b2f3b71fa",
  "loadTime" : 1525778596026,
  "textBody" : "Linear prediction is a mathematical operation where future values of a discrete-time signal are estimated as a linear function of previous samples.\n\nIn digital signal processing, linear prediction is often called linear predictive coding (LPC) and can thus be viewed as a subset of filter theory. In system analysis (a subfield of mathematics), linear prediction can be viewed as a part of mathematical modelling or optimization.\n\nThe prediction model \n\nThe most common representation is\n\n\\widehat{x}(n) \\sum_{i\n1}^p a_i x(n-i)\\,\n\nwhere \\widehat{x}(n) is the predicted signal value, x(n-i) the previous observed values, and a_i the predictor coefficients. The error generated by this estimate is\n\ne(n) = x(n) - \\widehat{x}(n)\\,\n\nwhere x(n) is the true signal value.\n\nThese equations are valid for all types of (one-dimensional) linear prediction. The differences are found in the way the predictor coefficients a_i are chosen.\n\nFor multi-dimensional signals the error metric is often defined as\n\ne(n) = \\|x(n) - \\widehat{x}(n)\\|\\,\n\nwhere \\|\\cdot\\| is a suitable chosen vector norm. Predictions such as \\widehat{x}(n)  are routinely used within Kalman filters and smoothers to estimate current and past signal values, respectively.\n\nEstimating the parameters \n\nThe most common choice in optimization of parameters a_i is the root mean square criterion which is also called the autocorrelation criterion. In this method we minimize the expected value of the squared error  E[e^2(n)], which yields the equation\n\n\\sum_{i1}^p a_i R(j-i) \n -R(j),\n\nfor 1 ≤ j ≤ p, where R is the autocorrelation of signal xn, defined as\n\n\\ R(i) = E\\{x(n)x(n-i)\\}\\,,\n\nand E is the expected value.  In the multi-dimensional case this corresponds to minimizing the L2 norm.\n\nThe above equations are called the normal equations or Yule-Walker equations. In matrix form the equations can be equivalently written as\n\nRa = -r,\\,\n\nwhere the autocorrelation matrix  R  is a symmetric, p \\times p Toeplitz matrix with elements  r_{ij} R(i-j), 0 \\leq i, j, the vector  r  is the autocorrelation vector  r_j \n R(j), 0, and the vector a is the parameter vector.\n\nAnother, more general, approach is to minimize the sum of squares of the errors defined in the form\n\ne(n) x(n) - \\widehat{x}(n) \n x(n) - \\sum_{i1}^p a_i x(n-i) \n - \\sum_{i=0}^p a_i x(n-i)\n\nwhere the optimisation problem searching over all a_i must now be constrained with a_0=-1.\n\nOn the other hand, if the mean square prediction error is constrained to be unity and the prediction error equation is included on top of the normal equations, the augmented set of equations is obtained as\n\n\\ Ra = [1, 0, ... , 0]^{\\mathrm{T}}\n\nwhere the index i ranges from 0 to p, and R is a (p + 1) × (p + 1) matrix.\n\nSpecification of the parameters of the linear predictor is a wide topic and a large number of other approaches have been proposed. In fact, the autocorrelation method is the most common and it is used, for example, for speech coding in the GSM standard.\n\nSolution of the matrix equation Ra = r is computationally a relatively expensive process. The Gaussian elimination for matrix inversion is probably the oldest solution but this approach does not efficiently use the symmetry of R and r. A faster algorithm is the Levinson recursion proposed by Norman Levinson in 1947, which recursively calculates the solution. In particular, the autocorrelation equations above may be more efficiently solved by the Durbin algorithm.\n\nIn 1986, Philippe Delsarte and Y.V. Genin proposed an improvement to this algorithm called the split Levinson recursion, which requires about half the number of multiplications and divisions.Delsarte, P. and Genin, Y. V. (1986), The split Levinson algorithm, IEEE Transactions on Acoustics, Speech, and Signal Processing, v. ASSP-34(3), pp. 470–478 It uses a special symmetrical property of parameter vectors on subsequent recursion levels. That is, calculations for the optimal predictor containing p terms make use of similar calculations for the optimal predictor containing p − 1 terms.\n\nAnother way of identifying model parameters is to iteratively calculate state estimates using Kalman filters and obtaining maximum likelihood estimates within expectation–maximization algorithms.",
  "entityProperties" : [ {
    "name" : "title",
    "type" : "String",
    "values" : [ "Linear prediction" ],
    "synthetic" : false
  }, {
    "name" : "url",
    "type" : "String",
    "values" : [ "http://en.wikipedia.org/?curid=18572" ],
    "synthetic" : false
  } ],
  "classifications" : [ "xml-export" ],
  "technicalAttributes" : {
    "technicalAttributes" : null,
    "aggregatedText" : "Linear prediction is a mathematical operation where future values of a discrete-time signal are estimated as a linear function of previous samples.\n\nIn digital signal processing, linear prediction is often called linear predictive coding (LPC) and can thus be viewed as a subset of filter theory. In system analysis (a subfield of mathematics), linear prediction can be viewed as a part of mathematical modelling or optimization.\n\nThe prediction model \n\nThe most common representation is\n\n\\widehat{x}(n) \\sum_{i\n1}^p a_i x(n-i)\\,\n\nwhere \\widehat{x}(n) is the predicted signal value, x(n-i) the previous observed values, and a_i the predictor coefficients. The error generated by this estimate is\n\ne(n) = x(n) - \\widehat{x}(n)\\,\n\nwhere x(n) is the true signal value.\n\nThese equations are valid for all types of (one-dimensional) linear prediction. The differences are found in the way the predictor coefficients a_i are chosen.\n\nFor multi-dimensional signals the error metric is often defined as\n\ne(n) = \\|x(n) - \\widehat{x}(n)\\|\\,\n\nwhere \\|\\cdot\\| is a suitable chosen vector norm. Predictions such as \\widehat{x}(n)  are routinely used within Kalman filters and smoothers to estimate current and past signal values, respectively.\n\nEstimating the parameters \n\nThe most common choice in optimization of parameters a_i is the root mean square criterion which is also called the autocorrelation criterion. In this method we minimize the expected value of the squared error  E[e^2(n)], which yields the equation\n\n\\sum_{i1}^p a_i R(j-i) \n -R(j),\n\nfor 1 ≤ j ≤ p, where R is the autocorrelation of signal xn, defined as\n\n\\ R(i) = E\\{x(n)x(n-i)\\}\\,,\n\nand E is the expected value.  In the multi-dimensional case this corresponds to minimizing the L2 norm.\n\nThe above equations are called the normal equations or Yule-Walker equations. In matrix form the equations can be equivalently written as\n\nRa = -r,\\,\n\nwhere the autocorrelation matrix  R  is a symmetric, p \\times p Toeplitz matrix with elements  r_{ij} R(i-j), 0 \\leq i, j, the vector  r  is the autocorrelation vector  r_j \n R(j), 0, and the vector a is the parameter vector.\n\nAnother, more general, approach is to minimize the sum of squares of the errors defined in the form\n\ne(n) x(n) - \\widehat{x}(n) \n x(n) - \\sum_{i1}^p a_i x(n-i) \n - \\sum_{i=0}^p a_i x(n-i)\n\nwhere the optimisation problem searching over all a_i must now be constrained with a_0=-1.\n\nOn the other hand, if the mean square prediction error is constrained to be unity and the prediction error equation is included on top of the normal equations, the augmented set of equations is obtained as\n\n\\ Ra = [1, 0, ... , 0]^{\\mathrm{T}}\n\nwhere the index i ranges from 0 to p, and R is a (p + 1) × (p + 1) matrix.\n\nSpecification of the parameters of the linear predictor is a wide topic and a large number of other approaches have been proposed. In fact, the autocorrelation method is the most common and it is used, for example, for speech coding in the GSM standard.\n\nSolution of the matrix equation Ra = r is computationally a relatively expensive process. The Gaussian elimination for matrix inversion is probably the oldest solution but this approach does not efficiently use the symmetry of R and r. A faster algorithm is the Levinson recursion proposed by Norman Levinson in 1947, which recursively calculates the solution. In particular, the autocorrelation equations above may be more efficiently solved by the Durbin algorithm.\n\nIn 1986, Philippe Delsarte and Y.V. Genin proposed an improvement to this algorithm called the split Levinson recursion, which requires about half the number of multiplications and divisions.Delsarte, P. and Genin, Y. V. (1986), The split Levinson algorithm, IEEE Transactions on Acoustics, Speech, and Signal Processing, v. ASSP-34(3), pp. 470–478 It uses a special symmetrical property of parameter vectors on subsequent recursion levels. That is, calculations for the optimal predictor containing p terms make use of similar calculations for the optimal predictor containing p − 1 terms.\n\nAnother way of identifying model parameters is to iteratively calculate state estimates using Kalman filters and obtaining maximum likelihood estimates within expectation–maximization algorithms. Linear prediction. http://en.wikipedia.org/?curid=18572."
  }
}
